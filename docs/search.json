[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Apothecary’s Notes",
    "section": "",
    "text": "Github\n  \n\n  \n  \nHello, welcome!\nThis is the note repository for modern data science skills with a focus on real-world data (observational studies) and clinical trials.\nContent will be gradually added while I learn the topics, and mostly for personal use. Therefore, it is by no means a complete guide by the time you read it!\nI try to organize the content in a modular way: these should cover the important aspects in which a data scientist / modern statistician should know.\n\nstudy design\ninference\nmodels\nreporting\nprogramming\n\nAny feedback / mistake corrections are welcome, you can find me here. Thank you!"
  },
  {
    "objectID": "documentation/understand_data.html",
    "href": "documentation/understand_data.html",
    "title": "Part 1: Understand the current diet",
    "section": "",
    "text": "Load the aggregated food groups and their attributes. We have 28 food groups.\n\nfoods <- read.csv('data/foods.csv', sep = ',')\ndata.table::setDT(foods) # use data.table format\nfoods[1:10,] # show the first 10\n\n                   food intake energy protein   fat carbs sugar alcohol  ghge\n 1:               Bread  175.4 10.696   0.091 0.030 0.441 0.002       0 0.001\n 2:        Other grains   45.0 14.022   0.100 0.042 0.607 0.011       0 0.002\n 3:               Cakes   35.6 14.185   0.067 0.152 0.424 0.185       0 0.002\n 4:            Potatoes   67.8  3.791   0.021 0.007 0.178 0.000       0 0.000\n 5:          Vegetables  154.6  1.565   0.015 0.008 0.050 0.005       0 0.001\n 6:             Legumes    3.5  8.571   0.143 0.029 0.286 0.000       0 0.001\n 7:      Fruit, berries  171.5  2.729   0.008 0.004 0.134 0.029       0 0.001\n 8:               Juice  111.0  1.928   0.005 0.002 0.103 0.000       0 0.001\n 9:                Nuts    4.3 25.581   0.209 0.535 0.116 0.000       0 0.005\n10: Vegetarian products    0.7  4.286   0.143 0.000 0.000 0.000       0 0.003\n\n\nDefine the constraints on\n\nenergy\nprotein, fat, carbs, sugar, alcohol\nGHGE (greenhouse gas equivalent)\n\n\nconstraints <- data.frame(\n  constraint = c('lower', 'upper'), \n  energy = c(9000, 10000), \n  protein = c(55, 111.5), \n  fat = c(61.8, 98.8), \n  carbs = c(250, 334.6), \n  sugar = c(0, 54.8), \n  alcohol = c(0, 10),\n  ghge = c(0, 4.7)\n)\nconstraints\n\n  constraint energy protein  fat carbs sugar alcohol ghge\n1      lower   9000    55.0 61.8 250.0   0.0       0  0.0\n2      upper  10000   111.5 98.8 334.6  54.8      10  4.7"
  },
  {
    "objectID": "documentation/index.html",
    "href": "documentation/index.html",
    "title": "Documentation",
    "section": "",
    "text": "It is important to document while you go.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nPart 1: Understand the current diet\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "What is Data Apothecary’s Notes?",
    "section": "",
    "text": "Hello, welcome!\nThis is the note repository for some documentation and demonstration on modern data science skills related to drug development.\nI created this repo so that I can find notes with code and rendered results easily. Therefore, the target audience is the future-me; if it helps you in some way as well, I’d be more than happy!\nIf you see some errors or have any suggestions, please feel free to reach out.\nFind out who I am here!"
  },
  {
    "objectID": "reporting/index.html",
    "href": "reporting/index.html",
    "title": "Reporting",
    "section": "",
    "text": "It is important to document while you go.\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nClinical trial: reporting results\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "reporting/understand_data.html",
    "href": "reporting/understand_data.html",
    "title": "Part 1: Understand the current diet",
    "section": "",
    "text": "Load the aggregated food groups and their attributes. We have 28 food groups.\n\nfoods <- read.csv('data/foods.csv', sep = ',')\ndata.table::setDT(foods) # use data.table format\nfoods[1:10,] # show the first 10\n\n                   food intake energy protein   fat carbs sugar alcohol  ghge\n 1:               Bread  175.4 10.696   0.091 0.030 0.441 0.002       0 0.001\n 2:        Other grains   45.0 14.022   0.100 0.042 0.607 0.011       0 0.002\n 3:               Cakes   35.6 14.185   0.067 0.152 0.424 0.185       0 0.002\n 4:            Potatoes   67.8  3.791   0.021 0.007 0.178 0.000       0 0.000\n 5:          Vegetables  154.6  1.565   0.015 0.008 0.050 0.005       0 0.001\n 6:             Legumes    3.5  8.571   0.143 0.029 0.286 0.000       0 0.001\n 7:      Fruit, berries  171.5  2.729   0.008 0.004 0.134 0.029       0 0.001\n 8:               Juice  111.0  1.928   0.005 0.002 0.103 0.000       0 0.001\n 9:                Nuts    4.3 25.581   0.209 0.535 0.116 0.000       0 0.005\n10: Vegetarian products    0.7  4.286   0.143 0.000 0.000 0.000       0 0.003\n\n\nDefine the constraints on\n\nenergy\nprotein, fat, carbs, sugar, alcohol\nGHGE (greenhouse gas equivalent)\n\n\nconstraints <- data.frame(\n  constraint = c('lower', 'upper'), \n  energy = c(9000, 10000), \n  protein = c(55, 111.5), \n  fat = c(61.8, 98.8), \n  carbs = c(250, 334.6), \n  sugar = c(0, 54.8), \n  alcohol = c(0, 10),\n  ghge = c(0, 4.7)\n)\nconstraints\n\n  constraint energy protein  fat carbs sugar alcohol ghge\n1      lower   9000    55.0 61.8 250.0   0.0       0  0.0\n2      upper  10000   111.5 98.8 334.6  54.8      10  4.7"
  },
  {
    "objectID": "models/index.html",
    "href": "models/index.html",
    "title": "Models",
    "section": "",
    "text": "Models for repeated measurement, longitudinal data and related matters.\nOr more precisely: different endpoints\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nMixed models for repeted measurements\n\n\n\n\nSurvival\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "models/understand_data.html",
    "href": "models/understand_data.html",
    "title": "Mixed models",
    "section": "",
    "text": "Yet to be filled in"
  },
  {
    "objectID": "study_design/index.html",
    "href": "study_design/index.html",
    "title": "Study design",
    "section": "",
    "text": "Topics (ongoing)\n\nClinical trial design\n\nPhase I, II, III\nadaptive design\n\nSample size calculation\n\ncomparing a few groups (visualization TBD)\nregression (LR, GLM)\nmore advanced model (e.g. GLMM)\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nAdaptive design: overview\n\n\nIntro to adaptive design \n\n\n\n\nSample size (part I)\n\n\nOverview, mean and proportion comparison \n\n\n\n\nSample size (part II)\n\n\nRegression \n\n\n\n\nClinical trial design: overview\n\n\nNotes related to clinical trial design. \n\n\n\n\n\nNo matching items\n\n\n\nCHECK\nhttps://stats.stackexchange.com/questions/48374/sample-size-calculation-for-mixed-models\nICH Guidelines (EMA)\nhttps://www.ema.europa.eu/en/ich-e10-choice-control-group-clinical-trials-scientific-guideline#current-effective-version-section\nhttps://www.ema.europa.eu/en/documents/scientific-guideline/ich-e-10-choice-control-group-clinical-trials-step-5_en.pdf\nhttps://clinicaltrials.gov"
  },
  {
    "objectID": "study_design/understand_data.html",
    "href": "study_design/understand_data.html",
    "title": "Part 1: Understand the current diet",
    "section": "",
    "text": "Load the aggregated food groups and their attributes. We have 28 food groups.\n\nfoods <- read.csv('data/foods.csv', sep = ',')\ndata.table::setDT(foods) # use data.table format\nfoods[1:10,] # show the first 10\n\n                   food intake energy protein   fat carbs sugar alcohol  ghge\n 1:               Bread  175.4 10.696   0.091 0.030 0.441 0.002       0 0.001\n 2:        Other grains   45.0 14.022   0.100 0.042 0.607 0.011       0 0.002\n 3:               Cakes   35.6 14.185   0.067 0.152 0.424 0.185       0 0.002\n 4:            Potatoes   67.8  3.791   0.021 0.007 0.178 0.000       0 0.000\n 5:          Vegetables  154.6  1.565   0.015 0.008 0.050 0.005       0 0.001\n 6:             Legumes    3.5  8.571   0.143 0.029 0.286 0.000       0 0.001\n 7:      Fruit, berries  171.5  2.729   0.008 0.004 0.134 0.029       0 0.001\n 8:               Juice  111.0  1.928   0.005 0.002 0.103 0.000       0 0.001\n 9:                Nuts    4.3 25.581   0.209 0.535 0.116 0.000       0 0.005\n10: Vegetarian products    0.7  4.286   0.143 0.000 0.000 0.000       0 0.003\n\n\nDefine the constraints on\n\nenergy\nprotein, fat, carbs, sugar, alcohol\nGHGE (greenhouse gas equivalent)\n\n\nconstraints <- data.frame(\n  constraint = c('lower', 'upper'), \n  energy = c(9000, 10000), \n  protein = c(55, 111.5), \n  fat = c(61.8, 98.8), \n  carbs = c(250, 334.6), \n  sugar = c(0, 54.8), \n  alcohol = c(0, 10),\n  ghge = c(0, 4.7)\n)\nconstraints\n\n  constraint energy protein  fat carbs sugar alcohol ghge\n1      lower   9000    55.0 61.8 250.0   0.0       0  0.0\n2      upper  10000   111.5 98.8 334.6  54.8      10  4.7"
  },
  {
    "objectID": "programming/index.html",
    "href": "programming/index.html",
    "title": "Programming",
    "section": "",
    "text": "This page hosts study notes on programming, with a strong flavour of R.\n\nR programming\n\n\n\n\n\nTitle\n\n\n\n\n\n\nData manipulation with data.table\n\n\n\n\nError handling in R\n\n\n\n\nFunctions\n\n\n\n\nOOP in R: S3\n\n\n\n\nProcessing text and characters\n\n\n\n\n\nNo matching items\n\n\n\n\nR package\n\n\n\n\n\nTitle\n\n\n\n\n\n\nDependencies\n\n\n\n\nMy workflow in working with functions in a package\n\n\n\n\nR package Engineering Workflow\n\n\n\n\nR package: tests\n\n\n\n\n\nNo matching items\n\n\n\n\nWeb and quarto\n\n\n\n\n\nTitle\n\n\n\n\n\n\nMake slides with reveal.js\n\n\n\n\nWeb basics\n\n\n\n\nWebR: Use with an existing quarto website\n\n\n\n\n\nNo matching items\n\n\n\n\nOther topics\n\n\n\n\n\nTitle\n\n\n\n\n\n\nSet up version control\n\n\n\n\nSoftware papers\n\n\n\n\nTopic coverage: R developer\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "programming/understand_data.html",
    "href": "programming/understand_data.html",
    "title": "Part 1: Understand the current diet",
    "section": "",
    "text": "Load the aggregated food groups and their attributes. We have 28 food groups.\n\nfoods <- read.csv('data/foods.csv', sep = ',')\ndata.table::setDT(foods) # use data.table format\nfoods[1:10,] # show the first 10\n\n                   food intake energy protein   fat carbs sugar alcohol  ghge\n 1:               Bread  175.4 10.696   0.091 0.030 0.441 0.002       0 0.001\n 2:        Other grains   45.0 14.022   0.100 0.042 0.607 0.011       0 0.002\n 3:               Cakes   35.6 14.185   0.067 0.152 0.424 0.185       0 0.002\n 4:            Potatoes   67.8  3.791   0.021 0.007 0.178 0.000       0 0.000\n 5:          Vegetables  154.6  1.565   0.015 0.008 0.050 0.005       0 0.001\n 6:             Legumes    3.5  8.571   0.143 0.029 0.286 0.000       0 0.001\n 7:      Fruit, berries  171.5  2.729   0.008 0.004 0.134 0.029       0 0.001\n 8:               Juice  111.0  1.928   0.005 0.002 0.103 0.000       0 0.001\n 9:                Nuts    4.3 25.581   0.209 0.535 0.116 0.000       0 0.005\n10: Vegetarian products    0.7  4.286   0.143 0.000 0.000 0.000       0 0.003\n\n\nDefine the constraints on\n\nenergy\nprotein, fat, carbs, sugar, alcohol\nGHGE (greenhouse gas equivalent)\n\n\nconstraints <- data.frame(\n  constraint = c('lower', 'upper'), \n  energy = c(9000, 10000), \n  protein = c(55, 111.5), \n  fat = c(61.8, 98.8), \n  carbs = c(250, 334.6), \n  sugar = c(0, 54.8), \n  alcohol = c(0, 10),\n  ghge = c(0, 4.7)\n)\nconstraints\n\n  constraint energy protein  fat carbs sugar alcohol ghge\n1      lower   9000    55.0 61.8 250.0   0.0       0  0.0\n2      upper  10000   111.5 98.8 334.6  54.8      10  4.7"
  },
  {
    "objectID": "inference/index.html",
    "href": "inference/index.html",
    "title": "Inference and models",
    "section": "",
    "text": "Different settings to apply methods.\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nReal-world Data, Real-world Evidence\n\n\nRWD, RWE \n\n\n\n\nGenomics in Drug Discovery\n\n\nUse of machine learning techniques \n\n\n\n\nAntibiotics\n\n\nBackground of antimicrobial drugs and resistance \n\n\n\n\nRWD EHR Vendor Engagement\n\n\nOverview of vendor engagement \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "inference/understand_data.html",
    "href": "inference/understand_data.html",
    "title": "(Placeholder)",
    "section": "",
    "text": "Define the constraints on\n\nenergy\nprotein, fat, carbs, sugar, alcohol\nGHGE (greenhouse gas equivalent)\n\n\nconstraints <- data.frame(\n  constraint = c('lower', 'upper'), \n  energy = c(9000, 10000), \n  protein = c(55, 111.5), \n  fat = c(61.8, 98.8), \n  carbs = c(250, 334.6), \n  sugar = c(0, 54.8), \n  alcohol = c(0, 10),\n  ghge = c(0, 4.7)\n)\nconstraints\n\n  constraint energy protein  fat carbs sugar alcohol ghge\n1      lower   9000    55.0 61.8 250.0   0.0       0  0.0\n2      upper  10000   111.5 98.8 334.6  54.8      10  4.7"
  },
  {
    "objectID": "study_design/methodnotes_20230601_trialdesign/index.html",
    "href": "study_design/methodnotes_20230601_trialdesign/index.html",
    "title": "Clinical trial design",
    "section": "",
    "text": "Coursera course Design and interpretation of clinical trials by Johns Hopkins University"
  },
  {
    "objectID": "study_design/methodnotes_20230601_trialdesign/index.html#types-of-trials-designs",
    "href": "study_design/methodnotes_20230601_trialdesign/index.html#types-of-trials-designs",
    "title": "Clinical trial design",
    "section": "Types of trials designs",
    "text": "Types of trials designs\nPhase 1: 10-30, identify tolerable dose, information on drug metabolism, extretion and toxicity. Often not controlled\nPhase 2: 30-100, efficacy, safety and side effects,\nPhase 3: 100+, often randomized\nPhase 4: demonstration\n\nTypes of design\nPopulation have the disease outcome of interest; not healthy voluteers vs diseased.\nRandomisation unit: persons, two eyes of a person, or groups of persons\nComparison structure: parallel, crossover, group allocation\n\nParallel: simultaneous treatment and control groups, subjects randomly assigned to one group.\nCrossover: randomize of order in which treatments are received; TC or CT. Each patient is his/her own control. Washout period: time between two treatments.\n\nVariability reduced because less variability within patient than between patients. Fewer patients needed.\nDisadvantages: only certain treatments can use crossover design, treatment can’t have permanent effects. Carry-over effects from first period; washout needs to be long enough. Dropouts more significant, analysis may be more difficult: correlated outcomes.\nConstant intensity of underlying disease: chronic diseases (e.g. asthma, hypertension, arthritis) + short-term treatment effects (relief of signs or symptoms)\ne.g. morning dose vs evening dose\n\nGroup allocation: a group of subjects (community, school, clinic).\n\nExtensions of the parallel design: factorial, large simple\n\nFactorial: two interventions tested simultaneously. Can be presented in a 2 by 2 table (treatment A +-, treatment B +-); or 3 by 2 etc.\n\nInterested in main effect (if no interaction expected). A vs no A; B vs no B. The other treatment doesn’t matter.\n\nLarge simple: large number of patients, possibly from many study sites.\n\n\nTests other than superiority\n\nEquivalency: intervention response is close to control group response\nNon-inferiority: Treatment A (new) is at least as good as B (established). One-sided test, if A is worse than B, one can be rejected. Does not require as big sample size.\n\n\n\nAdaptive design\nPossible adaptations\n\nrandomization probabilities\nsample size (e.g. group sequential methods)\nvisit schedule: shorten/lengthen follow-up time, change number of timing of visits, treatments (dose/duration, concomitant meds)\nhypothesis tested"
  },
  {
    "objectID": "study_design/methodnotes_20230601_trialdesign/index.html#randomisation-and-masking",
    "href": "study_design/methodnotes_20230601_trialdesign/index.html#randomisation-and-masking",
    "title": "Clinical trial design",
    "section": "Randomisation and masking",
    "text": "Randomisation and masking\nRationale:\n\navoid selection bias: prognostic factors related to treatment assignment\ntends to produce comparable treatment groups\n\n\nSchemes\nSimple randomization, restricted randomization, adaptive randomization\n\nSimple rz\nEach assignment is unpredictable, number of patients in each group should be equal in the long run.\nRisks: imbalances in number assigned to treatment groups, or confounding factors (gender, disease severity) -> reduced power\n\n\nRestricted rz\nSchemes with constraints to produce expected assignment ratio\n\nblocking\nstratification\n\nBlocking. Block of size 2 with treatment allocation ratio 1:1: A,B. Size 4: 2As, 2Bs. Need to be permuted: AABB, ABAB, … in total 6 combinations. Then choose one of the permutations.\nStratification. Ensure balance in treatment assignments with subgroups defined before rz. Limit to a few variables (highly related to outcome and/or logistical): e.g. clinic in a multicenter trial, surgeon (skills, procedures), stage of disease, demographic such as gender and age.\nUse these two together.\n\n\nAdaptive rz\nProbability of assignment does not remain constant, but determined by the current balance and composition of the groups.\n\nminimization: choose the design that gives the smallest imbalance.\nplay the winner: change allocation ratio or favor the better treatment based on the primary outcome. Need to evaluate outcomes relatively quickly.\n\n\n\n\nMasking (blinding)\nTreatment assignment is not known after rz.\n\npatient, clinical personnel, evaluators, data processors, …\nsingle (only participant), double (+ investigator), triple (+ data processors, …), quadruple …\n\nPurpose: remove bias related to treatment effects.\nDifferent levels of masking protects to different extent against bias in different aspects\n\ndata reporting\ndata collection / follow-up\ntesting, behaviors\noutcome assessment\n\nDecision to mask treatments\n\nethical?\npossible? can you make the treatment seem identical so the participants do not know?\ntrial design features: more important to mask subjective ones (e.g. alive or dead is the least subjective, hence wouldn’t benefit much; however if participants need to report effects that are not objectively measureable, they might report that treatment is better in contrast to placebo group)\nfeasible? cost-benefit, practicality (adherence)\n\nSometimes investigators in a double blind study might know which treatment is being assigned to participants, if the effect of drug is very obvious (both good or bad).\nUnmasking\n\nPlanned: inform participants once the trial finished\nUnplanned (discouraged): in the event of adverse event"
  },
  {
    "objectID": "study_design/methodnotes_20230601_trialdesign/index.html#outcomes-and-analysis",
    "href": "study_design/methodnotes_20230601_trialdesign/index.html#outcomes-and-analysis",
    "title": "Clinical trial design",
    "section": "Outcomes and analysis",
    "text": "Outcomes and analysis\nOutcome: endpoint. It is a quantitaive measure.\nObjectives of the trial\n\nefficacy / effectiveness\nsafety\nprocess\ncosts\n\nExample: evaluate treatment for asthma\nOutcomes: exhaled nitrous oxide, lung function (spirometry measures), asthma symptoms (wheezing, night awakenings), …\nExample: evaluate a procedure to reduce perioperative morbidity\nOutcome considerations: time window (what is postoperative), specific events to be considered an outcome, procedures to establish outcomes, …\n\nMetrics for events as outcomes\n\ndichotomous: 1/0 for presence absense, normal abnormal; clinical state or cut-off value\ntime-to-event: in addition to dichotomous, add time dimension; allow for censoring. More powerful than dichotomous.\nrates: 1/0 but allow for repeats, analyze count or rate. Events within a person are usually not independent, need to account for it.\ncontinuous variables: value or change from baseline; standard units (lab values, scores). Need to define an important difference. Distributional assumptions more important.\nordinal scale: ranked categories (e.g. adverse event grading, 1-5). Difference between categories is usually qualitative.\n\nPatients opinions are subjective\n\nhealth status / change in status, e.g. pain relief, quality of life\nmasking is more important\nhawthorne / placebo effect: effect of being studies, usually positive\nquantify with standardized scales\n\n\n\nInfluence of outcomes on design\nEfficacy vs effectiveness:\nIn a vaccine trial, efficacy is the clinical case with lab confirmation; effectivenenss is the clinical case of influenza in a larger population, may or may not be confirmed.\nIn asthma, efficacy is FEV1, effectiveness is the decrease of the hospitalizations/steroid courses.\nConsiderations (3Bs)\n\nbiology: does outcome reflect a clinically relevant fact/change\nbiostatistics: detectable difference between groups is plausible and practical\nbudget: afford total N and can measure it reliably in every participant\n\nExample: HIV trial outcomes\n\nsurvival (deaths; AIDS status)\nimmunologic response\nvirologic response\nchange in patient status (e.g QoL)\nspecified toxicity\nother side effects\n\nChoice of primary outcome depends on the objectives or stage of research\n\nphase 1, emphasis on safety\nphase 2, short-term efficacy\nphase 3, long-term efficacy\nphase 4, long-term effectiveness\n\n\n\nIntention to treat ITT\nCross-overs after rz: some patients might have a treatment (yes or no) beyond what they were assigned to, e.g. refuse surgery or medical treatment\nNon-adherence during followup: some in treatment group refuse of can not tolerate certain treatment; while some in placebo group require medication or take on their own\n\n\nSubgroup analysis\nStratified analysis: estimate treatment effect separately in subgroups. Does not tell difference across different subgroups\nTest for interaction: use of main effect and interaction.\nIssue of multiple testing when doing a series of analyses\n\ninflate sample size to plan for subgroup analysis\nreport number of subgroup analyses performed\npossibly adjust for multiple comparisons\nreport CI instead of just p-values"
  },
  {
    "objectID": "study_design/methodnotes_20230601_trialdesign/index.html#reporting-results",
    "href": "study_design/methodnotes_20230601_trialdesign/index.html#reporting-results",
    "title": "Clinical trial design",
    "section": "Reporting results",
    "text": "Reporting results\n\nCONSORT\nConsolidated Standards of Reporting Trials\n25 item checklist. Not a guideline of how to do the trial; but a guideline for writers on how to report clearly.\nTitle\n\nkey design items\ntreatment(s) evaluated\ndisease or population studied\n\nAbstract\n\ndesign, method, results, conclusions\n\nIntroduction\n\nbackgrounds, rationale, establish equipoise (balance), systematic review, objectives / hypothesis\n\nMethod\n\nIRB review and approvals\ntrial design, allocation ratio\neligibility criteria\nsetting and location of the trial\nintervention - detailed enough to allow replication\noutcomes - primary, secondary, how assessed and defined\nsample size - how determined, interim analyses\nimportant changes during trial\nrandomization, allocation concealment, implementation\nmasking - who, how\nstatistical methods - primary and secondary; subgroup analyses\n\nConvenient to use a flow-chart\nBaseline characteristics by treatment group\np-values (common to not report in rct?)\n\n\nEvaluate literature\n\nLegitimacy, is it a fair comparison\nTrustworthy investigators? conflict of interest\nAdequate protections against bias\n\nrandomization\nmasking\nfollow-up design and execution\n\nITT analysis\n\nhave all events (outcomes) observed been counted in the treatment group assigned?\nvariations in denominators explained and consistent with good practice?\n\nappropriate subgroup analysis interpretation\n\nad hoc or post hoc status"
  },
  {
    "objectID": "programming/r_roop.html",
    "href": "programming/r_roop.html",
    "title": "OOP in R: S3",
    "section": "",
    "text": "Useful references:"
  },
  {
    "objectID": "programming/r_roop.html#terminology",
    "href": "programming/r_roop.html#terminology",
    "title": "OOP in R: S3",
    "section": "Terminology",
    "text": "Terminology\n\nObject: individual instances of a class\nClass: type of an object, i.e. what an object is\nMethod: a function associated with a particular class, i.e. what the object can do\n\ngeneric method: mean() of a vector of numbers is a number, mean() of a vector of dates is a date\nInherit: a sub-class inherits all the attributes and methods from the super-class. E.g. generalized linear model inherits from a linear model.\nmethod dispatch: the process of finding the correct method given a class\n\n\nEncapsulated OOP:\n\nmethods belong to object or classes\nobject.method(arg1, arg2)\ncommon in most languages\nR6, RC (reference class) are examples of this type\n\nFunctional OOP:\n\nmethods belong to generic functions\ngeneric(object, arg2, arg3)\nS3 is an informal implementation of this type"
  },
  {
    "objectID": "programming/r_roop.html#base-types",
    "href": "programming/r_roop.html#base-types",
    "title": "OOP in R: S3",
    "section": "Base types",
    "text": "Base types\nCheck whether an object is object-oriented, or base object:\n\nis.object()\nsloop::otype(): returns base or S3/S4\nattr(obj_name, 'class'): OO objects has a class attribute, BO does not.\n\n\nx &lt;- 1:10 # a numeric vector\ny &lt;- factor(c('a', 'b'))  # a factor\n\nc(is.object(x), is.object(y))\n\n[1] FALSE  TRUE\n\nc(sloop::otype(x), sloop::otype(y))\n\n[1] \"base\" \"S3\"  \n\nattr(x, 'class') \n\nNULL\n\nattr(y, 'class')\n\n[1] \"factor\"\n\n\nAll objects have a base type; not all are OO objects.\n\ntypeof(1:10) returns ‘integer’\n25 base types in total\n\nvectors: e.g. NULL, logical, integer, double, complex, character, list, raw\nfunctions: e.g. closure, special, builtin\nenvironments: environment\nS4: S4\nlanguage components, symbol, language, pairlist the rest are less common."
  },
  {
    "objectID": "programming/r_roop.html#generic-or-method",
    "href": "programming/r_roop.html#generic-or-method",
    "title": "OOP in R: S3",
    "section": "Generic or method?",
    "text": "Generic or method?\n\ngeneric.class(), for example: print.factor()\ndo not call the method directly; use the generic (dispatch) to find it.\ngenerally has the . in the name; however it is not guaranteed * t.test() is a generic like print(), as t.test() can be used on multiple types of inputs\n\nas.factor() is not an OO object, hence not S3\n\n\n\nCheck function type with sloop::ftype()\n\nsloop::ftype(predict) # predict is a generic\n\n[1] \"S3\"      \"generic\"\n\nsloop::ftype(predict.glm)  # glm (class) method for predict() generic\n\n[1] \"S3\"     \"method\"\n\n\n\n\nCheck methods with methods()\nmethods() checks all the methods that either:\n\nbelongs to a generic (the function), such as plot, predict, t.test\nbelongs to a class (the type of input), such as lm, ar\n\n\nmethods('predict')  \n\n [1] predict.ar*                predict.Arima*            \n [3] predict.arima0*            predict.glm               \n [5] predict.HoltWinters*       predict.lm                \n [7] predict.loess*             predict.mlm*              \n [9] predict.nls*               predict.poly*             \n[11] predict.ppr*               predict.prcomp*           \n[13] predict.princomp*          predict.smooth.spline*    \n[15] predict.smooth.spline.fit* predict.StructTS*         \nsee '?methods' for accessing help and source code\n\nmethods(class = 'lm')\n\n [1] add1           alias          anova          case.names     coerce        \n [6] confint        cooks.distance deviance       dfbeta         dfbetas       \n[11] drop1          dummy.coef     effects        extractAIC     family        \n[16] formula        hatvalues      influence      initialize     kappa         \n[21] labels         logLik         model.frame    model.matrix   nobs          \n[26] plot           predict        print          proj           qr            \n[31] residuals      rstandard      rstudent       show           simulate      \n[36] slotsFromS3    summary        variable.names vcov          \nsee '?methods' for accessing help and source code\n\n\nEquivalently, use sloop::s3_methods_*(), as it gives more information in the output.\n\nsloop::s3_methods_generic('predict') \n\n# A tibble: 16 × 4\n   generic class             visible source             \n   &lt;chr&gt;   &lt;chr&gt;             &lt;lgl&gt;   &lt;chr&gt;              \n 1 predict ar                FALSE   registered S3method\n 2 predict Arima             FALSE   registered S3method\n 3 predict arima0            FALSE   registered S3method\n 4 predict glm               TRUE    stats              \n 5 predict HoltWinters       FALSE   registered S3method\n 6 predict lm                TRUE    stats              \n 7 predict loess             FALSE   registered S3method\n 8 predict mlm               FALSE   registered S3method\n 9 predict nls               FALSE   registered S3method\n10 predict poly              FALSE   registered S3method\n11 predict ppr               FALSE   registered S3method\n12 predict prcomp            FALSE   registered S3method\n13 predict princomp          FALSE   registered S3method\n14 predict smooth.spline     FALSE   registered S3method\n15 predict smooth.spline.fit FALSE   registered S3method\n16 predict StructTS          FALSE   registered S3method\n\nsloop::s3_methods_class('lm')\n\n# A tibble: 35 × 4\n   generic        class visible source             \n   &lt;chr&gt;          &lt;chr&gt; &lt;lgl&gt;   &lt;chr&gt;              \n 1 add1           lm    FALSE   registered S3method\n 2 alias          lm    FALSE   registered S3method\n 3 anova          lm    FALSE   registered S3method\n 4 case.names     lm    FALSE   registered S3method\n 5 confint        lm    TRUE    stats              \n 6 cooks.distance lm    FALSE   registered S3method\n 7 deviance       lm    FALSE   registered S3method\n 8 dfbeta         lm    FALSE   registered S3method\n 9 dfbetas        lm    FALSE   registered S3method\n10 drop1          lm    FALSE   registered S3method\n# ℹ 25 more rows"
  },
  {
    "objectID": "programming/r_roop.html#class-assignment",
    "href": "programming/r_roop.html#class-assignment",
    "title": "OOP in R: S3",
    "section": "Class assignment",
    "text": "Class assignment\nTwo options: structure(), or class(existing_obj)\n\nsimple_number &lt;- structure(1, class = 'simple')\nclass(simple_number)\n\n[1] \"simple\"\n\n\nOr, you can do it for an existing object by giving it a class\n\nsimple_char &lt;- 'your_name'\nclass(simple_char) &lt;- 'simple'\nclass(simple_char)\n\n[1] \"simple\""
  },
  {
    "objectID": "programming/r_roop.html#constructor",
    "href": "programming/r_roop.html#constructor",
    "title": "OOP in R: S3",
    "section": "Constructor",
    "text": "Constructor\n\nfruit &lt;- function(x){\n  stopifnot(is.character(x))\n  # checks if x is char\n  # better use a named list, easier to call\n  structure(list(fruit_name = x), class = 'fruit') \n}\n\nfruit1 &lt;- fruit('pineapple')\nfruit2 &lt;- fruit('apple')\n\nExamine what comes out\n\nfruit1\n\n$fruit_name\n[1] \"pineapple\"\n\nattr(,\"class\")\n[1] \"fruit\""
  },
  {
    "objectID": "programming/r_roop.html#define-new-generic-and-method",
    "href": "programming/r_roop.html#define-new-generic-and-method",
    "title": "OOP in R: S3",
    "section": "Define new generic and method",
    "text": "Define new generic and method\n[name of method] &lt;- functionn(x){UseMethod(\"[name of method]\")}\nNow we define one generic function f, and two methods. One for class plus2, and another for class plus10.\n\nf &lt;- function(x){UseMethod('f')} # define generic f\nf.plus2 &lt;- function(x) x+2 # f method for class plus2\nf.plus10 &lt;- function(x) x+10 # f method for class plus10\n\nNow we try to give the function some input. First use a numeric number, 1 (the class for a number is double and numeric).\n\nnumber &lt;- 1\nf(number) # returns error, class of number does not match!\n\nError in UseMethod(\"f\"): no applicable method for 'f' applied to an object of class \"c('double', 'numeric')\"\n\n\nThis returns an error, because the class of number is not defined for function f (plus2, plus10).\n\n# can check what f(number) tried \n# none of these exist \nsloop::s3_dispatch(f(number))\n\n   f.double\n   f.numeric\n   f.default\n\n\nWe need to match it. Assign the number with plus2 class, and evaluate it. You can check which method has been used (dispatched).\n\n# fix: assign a class to number\nclass(number) &lt;- 'plus2'\nf(number) # number+2, f.plus2 method\n\n[1] 3\nattr(,\"class\")\n[1] \"plus2\"\n\nsloop::s3_dispatch(f(number))\n\n=&gt; f.plus2\n   f.default\n\n\nNow we try another number, but let it be plus10 class.\n\nnumberx &lt;- 200\nclass(numberx) &lt;- 'plus10'\nf(numberx)\n\n[1] 210\nattr(,\"class\")\n[1] \"plus10\"\n\nsloop::s3_dispatch(f(numberx))\n\n=&gt; f.plus10\n   f.default"
  },
  {
    "objectID": "programming/r_roop.html#new-method-for-existing-generic-print",
    "href": "programming/r_roop.html#new-method-for-existing-generic-print",
    "title": "OOP in R: S3",
    "section": "New method for existing generic (print())",
    "text": "New method for existing generic (print())\nWe create the S3 object using the constructor defined above, fruit().\n\npineapple &lt;- fruit('pineapple') # create by the constructor\npineapple\n\n$fruit_name\n[1] \"pineapple\"\n\nattr(,\"class\")\n[1] \"fruit\"\n\n\nThe output does not look very nice, we can modify what prints out. Since print() is an exisiting generic function, we do not need to define a new one (i.e. UseMethod). We define the new method directly: generic.your_class.\n\n# we do not need to define print() as generic, bec it IS already\n# directly define print.fruit\nprint.fruit &lt;- function(x){\n  cat('I used constructor for my fruit:', x$fruit_name)\n}\n\nprint.fruit(pineapple)\n\nI used constructor for my fruit: pineapple"
  },
  {
    "objectID": "study_design/rct_design_overview.html",
    "href": "study_design/rct_design_overview.html",
    "title": "Clinical trial design: overview",
    "section": "",
    "text": "Considerations: choice of comparator and trial outcome measures (due to the grrowing number of treatmenet options, standard care would change over time), annd definition of target patient population (e.g. molecular profiling makes it possible to identify smaller subgroups of patient with defined tumor type)."
  },
  {
    "objectID": "study_design/rct_design_overview.html#types-of-trials-designs",
    "href": "study_design/rct_design_overview.html#types-of-trials-designs",
    "title": "Clinical trial design: overview",
    "section": "Types of trials designs",
    "text": "Types of trials designs\nPhase 1: 10-30, identify tolerable dose, information on drug metabolism, extretion and toxicity. Often not controlled\nPhase 2: 30-100, efficacy, safety and side effects,\nPhase 3: 100+, often randomized\nPhase 4: demonstration\n\nTypes of design\nPopulation have the disease outcome of interest; not healthy voluteers vs diseased.\nRandomisation unit: persons, two eyes of a person, or groups of persons\nComparison structure: parallel, crossover, group allocation\n\nParallel: simultaneous treatment and control groups, subjects randomly assigned to one group.\nCrossover: randomize of order in which treatments are received; TC or CT. Each patient is his/her own control. Washout period: time between two treatments.\n\nVariability reduced because less variability within patient than between patients. Fewer patients needed.\nDisadvantages: only certain treatments can use crossover design, treatment can’t have permanent effects. Carry-over effects from first period; washout needs to be long enough. Dropouts more significant, analysis may be more difficult: correlated outcomes.\nConstant intensity of underlying disease: chronic diseases (e.g. asthma, hypertension, arthritis) + short-term treatment effects (relief of signs or symptoms)\ne.g. morning dose vs evening dose\n\nGroup allocation: a group of subjects (community, school, clinic).\n\nExtensions of the parallel design: factorial, large simple\n\nFactorial: two interventions tested simultaneously. Can be presented in a 2 by 2 table (treatment A +-, treatment B +-); or 3 by 2 etc.\n\nInterested in main effect (if no interaction expected). A vs no A; B vs no B. The other treatment doesn’t matter.\n\nLarge simple: large number of patients, possibly from many study sites.\n\n\nTests other than superiority\n\nEquivalency: intervention response is close to control group response\nNon-inferiority: Treatment A (new) is at least as good as B (established). One-sided test, if A is worse than B, one can be rejected. Does not require as big sample size.\n\n\n\nAdaptive design\nPossible adaptations\n\nrandomization probabilities\nsample size (e.g. group sequential methods)\nvisit schedule: shorten/lengthen follow-up time, change number of timing of visits, treatments (dose/duration, concomitant meds)\nhypothesis tested"
  },
  {
    "objectID": "study_design/rct_design_overview.html#randomisation-and-masking",
    "href": "study_design/rct_design_overview.html#randomisation-and-masking",
    "title": "Clinical trial design: overview",
    "section": "Randomisation and masking",
    "text": "Randomisation and masking\nRationale:\n\navoid selection bias: prognostic factors related to treatment assignment\ntends to produce comparable treatment groups\n\n\nSchemes\nSimple randomization, restricted randomization, adaptive randomization\n\nSimple rz\nEach assignment is unpredictable, number of patients in each group should be equal in the long run.\nRisks: imbalances in number assigned to treatment groups, or confounding factors (gender, disease severity) -> reduced power\n\n\nRestricted rz\nSchemes with constraints to produce expected assignment ratio\n\nblocking\nstratification\n\nBlocking. Block of size 2 with treatment allocation ratio 1:1: A,B. Size 4: 2As, 2Bs. Need to be permuted: AABB, ABAB, … in total 6 combinations. Then choose one of the permutations.\nStratification. Ensure balance in treatment assignments with subgroups defined before rz. Limit to a few variables (highly related to outcome and/or logistical): e.g. clinic in a multicenter trial, surgeon (skills, procedures), stage of disease, demographic such as gender and age.\nUse these two together.\n\n\nAdaptive rz\nProbability of assignment does not remain constant, but determined by the current balance and composition of the groups.\n\nminimization: choose the design that gives the smallest imbalance.\nplay the winner: change allocation ratio or favor the better treatment based on the primary outcome. Need to evaluate outcomes relatively quickly.\n\n\n\n\nMasking (blinding)\nTreatment assignment is not known after rz.\n\npatient, clinical personnel, evaluators, data processors, …\nsingle (only participant), double (+ investigator), triple (+ data processors, …), quadruple …\n\nPurpose: remove bias related to treatment effects.\nDifferent levels of masking protects to different extent against bias in different aspects\n\ndata reporting\ndata collection / follow-up\ntesting, behaviors\noutcome assessment\n\nDecision to mask treatments\n\nethical?\npossible? can you make the treatment seem identical so the participants do not know?\ntrial design features: more important to mask subjective ones (e.g. alive or dead is the least subjective, hence wouldn’t benefit much; however if participants need to report effects that are not objectively measureable, they might report that treatment is better in contrast to placebo group)\nfeasible? cost-benefit, practicality (adherence)\n\nSometimes investigators in a double blind study might know which treatment is being assigned to participants, if the effect of drug is very obvious (both good or bad).\nUnmasking\n\nPlanned: inform participants once the trial finished\nUnplanned (discouraged): in the event of adverse event"
  },
  {
    "objectID": "study_design/rct_design_overview.html#outcomes-and-analysis",
    "href": "study_design/rct_design_overview.html#outcomes-and-analysis",
    "title": "Clinical trial design: overview",
    "section": "Outcomes and analysis",
    "text": "Outcomes and analysis\nOutcome: endpoint. It is a quantitaive measure.\nObjectives of the trial\n\nefficacy / effectiveness\nsafety\nprocess\ncosts\n\nExample: evaluate treatment for asthma\nOutcomes: exhaled nitrous oxide, lung function (spirometry measures), asthma symptoms (wheezing, night awakenings), …\nExample: evaluate a procedure to reduce perioperative morbidity\nOutcome considerations: time window (what is postoperative), specific events to be considered an outcome, procedures to establish outcomes, …\n\nMetrics for events as outcomes\n\ndichotomous: 1/0 for presence absense, normal abnormal; clinical state or cut-off value\ntime-to-event: in addition to dichotomous, add time dimension; allow for censoring. More powerful than dichotomous.\nrates: 1/0 but allow for repeats, analyze count or rate. Events within a person are usually not independent, need to account for it.\ncontinuous variables: value or change from baseline; standard units (lab values, scores). Need to define an important difference. Distributional assumptions more important.\nordinal scale: ranked categories (e.g. adverse event grading, 1-5). Difference between categories is usually qualitative.\n\nPatients opinions are subjective\n\nhealth status / change in status, e.g. pain relief, quality of life\nmasking is more important\nhawthorne / placebo effect: effect of being studies, usually positive\nquantify with standardized scales\n\n\n\nInfluence of outcomes on design\nEfficacy vs effectiveness:\nIn a vaccine trial, efficacy is the clinical case with lab confirmation; effectivenenss is the clinical case of influenza in a larger population, may or may not be confirmed.\nIn asthma, efficacy is FEV1, effectiveness is the decrease of the hospitalizations/steroid courses.\nConsiderations (3Bs)\n\nbiology: does outcome reflect a clinically relevant fact/change\nbiostatistics: detectable difference between groups is plausible and practical\nbudget: afford total N and can measure it reliably in every participant\n\nExample: HIV trial outcomes\n\nsurvival (deaths; AIDS status)\nimmunologic response\nvirologic response\nchange in patient status (e.g QoL)\nspecified toxicity\nother side effects\n\nChoice of primary outcome depends on the objectives or stage of research\n\nphase 1, emphasis on safety\nphase 2, short-term efficacy\nphase 3, long-term efficacy\nphase 4, long-term effectiveness\n\n\n\nIntention to treat ITT\nCross-overs after rz: some patients might have a treatment (yes or no) beyond what they were assigned to, e.g. refuse surgery or medical treatment\nNon-adherence during followup: some in treatment group refuse of can not tolerate certain treatment; while some in placebo group require medication or take on their own\n\n\nSubgroup analysis\nStratified analysis: estimate treatment effect separately in subgroups. Does not tell difference across different subgroups\nTest for interaction: use of main effect and interaction.\nIssue of multiple testing when doing a series of analyses\n\ninflate sample size to plan for subgroup analysis\nreport number of subgroup analyses performed\npossibly adjust for multiple comparisons\nreport CI instead of just p-values"
  },
  {
    "objectID": "study_design/rct_design_overview.html#reporting-results",
    "href": "study_design/rct_design_overview.html#reporting-results",
    "title": "Clinical trial design",
    "section": "Reporting results",
    "text": "Reporting results\n\nCONSORT\nConsolidated Standards of Reporting Trials\n25 item checklist. Not a guideline of how to do the trial; but a guideline for writers on how to report clearly.\nTitle\n\nkey design items\ntreatment(s) evaluated\ndisease or population studied\n\nAbstract\n\ndesign, method, results, conclusions\n\nIntroduction\n\nbackgrounds, rationale, establish equipoise (balance), systematic review, objectives / hypothesis\n\nMethod\n\nIRB review and approvals\ntrial design, allocation ratio\neligibility criteria\nsetting and location of the trial\nintervention - detailed enough to allow replication\noutcomes - primary, secondary, how assessed and defined\nsample size - how determined, interim analyses\nimportant changes during trial\nrandomization, allocation concealment, implementation\nmasking - who, how\nstatistical methods - primary and secondary; subgroup analyses\n\nConvenient to use a flow-chart\nBaseline characteristics by treatment group\np-values (common to not report in rct?)\n\n\nEvaluate literature\n\nLegitimacy, is it a fair comparison\nTrustworthy investigators? conflict of interest\nAdequate protections against bias\n\nrandomization\nmasking\nfollow-up design and execution\n\nITT analysis\n\nhave all events (outcomes) observed been counted in the treatment group assigned?\nvariations in denominators explained and consistent with good practice?\n\nappropriate subgroup analysis interpretation\n\nad hoc or post hoc status"
  },
  {
    "objectID": "reporting/reporting_overview.html",
    "href": "reporting/reporting_overview.html",
    "title": "Clinical trial: reporting results",
    "section": "",
    "text": "Coursera course Design and interpretation of clinical trials by Johns Hopkins University"
  },
  {
    "objectID": "reporting/reporting_overview.html#consort",
    "href": "reporting/reporting_overview.html#consort",
    "title": "Clinical trial: reporting results",
    "section": "CONSORT",
    "text": "CONSORT\nConsolidated Standards of Reporting Trials\n25 item checklist. Not a guideline of how to do the trial; but a guideline for writers on how to report clearly.\nTitle\n\nkey design items\ntreatment(s) evaluated\ndisease or population studied\n\nAbstract\n\ndesign, method, results, conclusions\n\nIntroduction\n\nbackgrounds, rationale, establish equipoise (balance), systematic review, objectives / hypothesis\n\nMethod\n\nIRB review and approvals\ntrial design, allocation ratio\neligibility criteria\nsetting and location of the trial\nintervention - detailed enough to allow replication\noutcomes - primary, secondary, how assessed and defined\nsample size - how determined, interim analyses\nimportant changes during trial\nrandomization, allocation concealment, implementation\nmasking - who, how\nstatistical methods - primary and secondary; subgroup analyses\n\nConvenient to use a flow-chart\nBaseline characteristics by treatment group\np-values (common to not report in rct?)\n\nEvaluate literature\n\nLegitimacy, is it a fair comparison\nTrustworthy investigators? conflict of interest\nAdequate protections against bias\n\nrandomization\nmasking\nfollow-up design and execution\n\nITT analysis\n\nhave all events (outcomes) observed been counted in the treatment group assigned?\nvariations in denominators explained and consistent with good practice?\n\nappropriate subgroup analysis interpretation\n\nad hoc or post hoc status"
  },
  {
    "objectID": "study_design/sample_size.html",
    "href": "study_design/sample_size.html",
    "title": "Sample size",
    "section": "",
    "text": "Sample size calculation is to determine the smallest number of subjects required, to detect a clinical meaningful effect. Why not recruiting as many as possible? Too expensive; or unethical (i.e. more people will be having potentially harmful or futile treatments)."
  },
  {
    "objectID": "dev/question_list.html",
    "href": "dev/question_list.html",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "In phase 2, difference in design (sample size) for single arm and randomizedf\n\n\nUnbiased assignment of treatment. Break the link between prognosis and prescription (allocation). Application of statistical methods based on random sampling\nE.g. avoid giving the same treatment to group A that have significantly worse health condition\n\n\n\n\n\n\nMight not observe an effect in RCT, but effect in OBS studies\n\npossible no effect in RCT: trial too short to see effect; late stage of disease among candidates to see any improvements of treatment; different treatment in OBS and RCT; comparing initiators vs non-initiators (hence shorter trial period)\npossible effect in OBS: residual confounding: social and environmental exposures; some already using treatment before enrolment\nsampling bias, in OBS the asymptomatic subjects might be missed (or under reported) while in RCT you can count every subject\nsurvival bias: survived long enough to be enrolled in OBS, and those dead are not; hence boosting effect to prolong survival\n\n\n\n\nNon-inferiority trials adopt one-sided test\nEquivalence: if the alternative is simpler, cheaper or have fewer side effects. Aim is to determine if difference is between +- delta\n\n\n\nWhy would these type of design reduce type 1 error rate by accounting for correlation?"
  },
  {
    "objectID": "dev/question_list.html#survival-analysis",
    "href": "dev/question_list.html#survival-analysis",
    "title": "Data Apothecary's Notes",
    "section": "Survival analysis",
    "text": "Survival analysis\nCan you analyse duration data with t-test? Why not?"
  },
  {
    "objectID": "study_design/sample_size.html#relevant-conncepts",
    "href": "study_design/sample_size.html#relevant-conncepts",
    "title": "Sample size",
    "section": "Relevant conncepts",
    "text": "Relevant conncepts\nStudy design\n\nparallel: group 1 TxA, group 2 TxB\ncrossover: requires fewer ssample than parallel; but requires wash-out period. Group 1 TxA -> TxB; group 2 TxB -> TxA\n\nTests\n\n\\(\\mu_T, \\mu_s\\): mean of new Tx or standard procedure\n\\(\\delta\\): minimum clinically important difference\n\\(\\delta_{NI}\\): non-inferiority margin\n\n\n\n\n\n\n\n\n\nTest for\nH0\nH1\n\n\n\n\nEquality\n\\(\\mu_T - \\mu_s = 0\\)\n\\(\\mu_T - \\mu_s \\neq 0\\)\n\n\nEquivalence\n\\(|\\mu_T - \\mu_s| \\geq 0\\)\n\\(|\\mu_T - \\mu_s| < 0\\)\n\n\nSuperiority\n\\(\\mu_T - \\mu_s \\geq 0\\)\n\\(\\mu_T - \\mu_s < 0\\)\n\n\nNon-inferiority\n\\(\\mu_T - \\mu_s \\leq -\\delta_{NI}\\)\n\\(\\mu_T - \\mu_s \\geq -\\delta_{NI}\\)\n\n\n\nErrors\n\nType I error, significance level \\(\\alpha\\). P(reject H0 |H0). Usually set to 0.05\nType II error \\(\\beta\\). P(not reject H0 |H1).\nPower, \\(1 - \\beta\\). P(reject H0 |H1). Usually set to 80% or 90%\n\nPrimary outcome\n\ncan be categorical or continuous\nMinimal meaningful detecable difference MD: the smallest difference to be considered as clinically meaningful in the primary outcome\n\nDropout rate: need to be adjusted.\nAllocation ratio: unequal sample size.\nEffect size (Cohen’s d, f) should be found in the literature. In general,\n\nvery small, d = 0.01\nsmall, d = 0.2\nmedium, d = 0.5\nlarge, d = 0.8\nvery large, d = 1.2\nhuge, d = 2"
  },
  {
    "objectID": "study_design/sample_size.html#proportions",
    "href": "study_design/sample_size.html#proportions",
    "title": "Sample size",
    "section": "Proportions",
    "text": "Proportions\nCohen’s h is used as the effect size, \\(h = 2arcsin(\\sqrt{p_1} - 2arcsin(\\sqrt{p_2}))\\). Use 0.2, 0.5, 0.8 for small, medium and large effect sizes.\n\n# one group\npwr::pwr.p.test(h = 0.5, sig.level = 0.05, power = 0.8, alternative = 'two.sided')\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.5\n              n = 31.39544\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n# two groups\npwr::pwr.2p.test(h = 0.5, sig.level = 0.05, power = 0.8, alternative = 'two.sided')\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.5\n              n = 62.79088\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes"
  },
  {
    "objectID": "study_design/sample_size.html#chi-square-test",
    "href": "study_design/sample_size.html#chi-square-test",
    "title": "Sample size",
    "section": "Chi-square test",
    "text": "Chi-square test\nCohen’s w. Use \\(l, k\\) to compute degrees of freedom.\n\n# k: number of groups; f: effect ssize\npwr::pwr.chisq.test(w = 0.3, df = (2-1)*(3-1), sig.level = 0.05, power = 0.8)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 107.0521\n             df = 2\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations"
  },
  {
    "objectID": "study_design/sample_size.html#exact-test",
    "href": "study_design/sample_size.html#exact-test",
    "title": "Sample size",
    "section": "Exact test",
    "text": "Exact test\nNeed to specify the proportion in each group (control, treatment). Allocation ratio 1:1\n\nexact2x2::ss2x2(p0 = 0.2, p1 = 0.8, n1.over.n0 = 1, sig.level = 0.05, power = 0.8, \n                approx = F, print.steps = T, paired = F)\n\n[1] \"starting calculation at n0= 11  n1= 11\"\n[1] \"n0=11 n1=11 power=0.734302912043505\"\n[1] \"n0=19 n1=19 power=0.962100966603327\"\n[1] \"n0=15 n1=15 power=0.872315260457242\"\n[1] \"n0=13 n1=13 power=0.868827534112504\"\n[1] \"n0=12 n1=12 power=0.811527612034704\"\n\n\n\n     Power for Fisher's Exact Test \n\n          power = 0.8115276\n             n0 = 12\n             n1 = 12\n             p0 = 0.2\n             p1 = 0.8\n      sig.level = 0.05\n    alternative = two.sided\n  nullOddsRatio = 1\n\nNOTE: errbound= 1e-06"
  },
  {
    "objectID": "study_design/sample_size_2.html",
    "href": "study_design/sample_size_2.html",
    "title": "Sample size (part II)",
    "section": "",
    "text": "(This is the part II on sample size calculation)\n\nCorrelation\nCorrelation coefficient is used for effect size measure. Usse 0.1, 0.3, 0.5 to represent small, medium and large sizes.\n\n# correlation coeff\npwr::pwr.r.test(r = 0.3, sig.level = 0.05, power = 0.8, alternative = 'two.sided')\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 84.07364\n              r = 0.3\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\nLinear regression (F-test)\nF-test for linear regression is testinng whether \\(R^2\\) is greater than zero (one-sided). \\(R^2\\) is the explained variance by using the predictors, \\(R^2 = 0.3\\) means that 30% of the variance are explained by the model.\nCohen’s f2, based on \\(R^2\\), goodness of fit (\\(f2 = R^2/(1-R^2)\\)). use 0.02, 0.15, 0.35 to represent small, medium and large effect sizes.\n\nu: number of predictors\nv: n-u-1\nas a result, sample size n = v+u+1\n\n\n# effect size f2 = 0.15; use u=3 predictors\npwr::pwr.f2.test(u = 3, f2 = 0.3, sig.level = 0.05, power = 0.8)\n\n\n     Multiple regression power calculation \n\n              u = 3\n              v = 36.47078\n             f2 = 0.3\n      sig.level = 0.05\n          power = 0.8\n\n\nHere v = 73, sample size is 73+3+1 = 77.\nAlternatively, can use pwrss::pwrss.f.reg(). The parameter is r2 rather than f2 (but can also use f2).\n\npwrss::pwrss.f.reg(r2 = 0.3, k = 0.3, power = 0.8, alpha = 0.05)\n\n Linear Regression (F test) \n R-squared Deviation from 0 (zero) \n H0: r2 = 0 \n HA: r2 &gt; 0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 14 \n ------------------------------ \n Numerator degrees of freedom = 0.3 \n Denominator degrees of freedom = 12.415 \n Non-centrality parameter = 5.878 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n\n# should be equivalent to\n# pwr::pwr.f2.test(u = 3, f2 = 0.3/0.7, sig.level = 0.05, power = 0.8)\n\n\n\nGLM\n\nResources\n\nSample size calculation in clinical trial using R.\nPark et al. 2023. https://doi.org/10.7602/jmis.2023.26.1.9\nBulus, M (2023) pwrss: Statistical Power and Sample Size Calculation Tools. R package version 0.3.1. https://CRAN.R-project.org/package=pwrss. Vignette documentation"
  },
  {
    "objectID": "study_design/sample_size_1.html",
    "href": "study_design/sample_size_1.html",
    "title": "Sample size (part I)",
    "section": "",
    "text": "(This is the part I on sample size calculation)\nSample size calculation is to determine the smallest number of subjects required, to detect a clinical meaningful effect. Why not recruiting as many as possible? Too expensive; or unethical (i.e. more people will be having potentially harmful or futile treatments)."
  },
  {
    "objectID": "study_design/sample_size_1.html#relevant-concepts",
    "href": "study_design/sample_size_1.html#relevant-concepts",
    "title": "Sample size (part I)",
    "section": "Relevant concepts",
    "text": "Relevant concepts\nStudy design\n\nparallel: group 1 TxA, group 2 TxB\ncrossover: requires fewer ssample than parallel; but requires wash-out period. Group 1 TxA -&gt; TxB; group 2 TxB -&gt; TxA\n\nTests\n\n\\(\\mu_T, \\mu_s\\): mean of new Tx or standard procedure\n\\(\\delta\\): minimum clinically important difference\n\\(\\delta_{NI}\\): non-inferiority margin\n\n\n\n\n\n\n\n\n\nTest for\nH0\nH1\n\n\n\n\nEquality\n\\(\\mu_T - \\mu_s = 0\\)\n\\(\\mu_T - \\mu_s \\neq 0\\)\n\n\nEquivalence\n\\(|\\mu_T - \\mu_s| \\geq 0\\)\n\\(|\\mu_T - \\mu_s| &lt; 0\\)\n\n\nSuperiority\n\\(\\mu_T - \\mu_s \\geq 0\\)\n\\(\\mu_T - \\mu_s &lt; 0\\)\n\n\nNon-inferiority\n\\(\\mu_T - \\mu_s \\leq -\\delta_{NI}\\)\n\\(\\mu_T - \\mu_s \\geq -\\delta_{NI}\\)\n\n\n\nErrors\n\nType I error, significance level \\(\\alpha\\). P(reject H0 |H0). Usually set to 0.05\nType II error \\(\\beta\\). P(not reject H0 |H1).\nPower, \\(1 - \\beta\\). P(reject H0 |H1). Usually set to 80% or 90%\n\nPrimary outcome\n\ncan be categorical or continuous\nMinimal meaningful detecable difference MD: the smallest difference to be considered as clinically meaningful in the primary outcome\n\nDropout rate: need to be adjusted.\nAllocation ratio: unequal sample size.\nEffect size (Cohen’s d, f) should be found in the literature. In general,\n\nvery small, d = 0.01\nsmall, d = 0.2\nmedium, d = 0.5\nlarge, d = 0.8\nvery large, d = 1.2\nhuge, d = 2"
  },
  {
    "objectID": "study_design/sample_size_1.html#two-sample-t-test",
    "href": "study_design/sample_size_1.html#two-sample-t-test",
    "title": "Sample size (part I)",
    "section": "Two sample t-test",
    "text": "Two sample t-test\nNote that this result is for one group: in total it’s times two.\n\n# effect size: 0.5\npwr::pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8, type = 'two.sample', alternative = 'two.sided')\n\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group"
  },
  {
    "objectID": "study_design/sample_size_1.html#anova",
    "href": "study_design/sample_size_1.html#anova",
    "title": "Sample size (part I)",
    "section": "ANOVA",
    "text": "ANOVA\nResult is for each group.\n\n# k: number of groups; f: effect ssize\npwr::pwr.anova.test(k = 3, f = 0.25, sig.level = 0.05, power = 0.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.3966\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group"
  },
  {
    "objectID": "study_design/sample_size_1.html#proportions",
    "href": "study_design/sample_size_1.html#proportions",
    "title": "Sample size (part I)",
    "section": "Proportions",
    "text": "Proportions\nCohen’s h is used as the effect size, \\(h = 2arcsin(\\sqrt{p_1} - 2arcsin(\\sqrt{p_2}))\\). Use 0.2, 0.5, 0.8 for small, medium and large effect sizes.\n\n# one group\npwr::pwr.p.test(h = 0.5, sig.level = 0.05, power = 0.8, alternative = 'two.sided')\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.5\n              n = 31.39544\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n# two groups\npwr::pwr.2p.test(h = 0.5, sig.level = 0.05, power = 0.8, alternative = 'two.sided')\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.5\n              n = 62.79088\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes"
  },
  {
    "objectID": "study_design/sample_size_1.html#chi-square-test",
    "href": "study_design/sample_size_1.html#chi-square-test",
    "title": "Sample size (part I)",
    "section": "Chi-square test",
    "text": "Chi-square test\nCohen’s w. Use \\(l, k\\) to compute degrees of freedom.\n\n# k: number of groups; f: effect ssize\npwr::pwr.chisq.test(w = 0.3, df = (2-1)*(3-1), sig.level = 0.05, power = 0.8)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 107.0521\n             df = 2\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations"
  },
  {
    "objectID": "study_design/sample_size_1.html#exact-test",
    "href": "study_design/sample_size_1.html#exact-test",
    "title": "Sample size (part I)",
    "section": "Exact test",
    "text": "Exact test\nNeed to specify the proportion in each group (control, treatment). Allocation ratio 1:1\n\nexact2x2::ss2x2(p0 = 0.2, p1 = 0.8, n1.over.n0 = 1, sig.level = 0.05, power = 0.8, \n                approx = F, print.steps = T, paired = F)\n\n[1] \"starting calculation at n0= 11  n1= 11\"\n[1] \"n0=11 n1=11 power=0.734302912043505\"\n[1] \"n0=19 n1=19 power=0.962100966603327\"\n[1] \"n0=15 n1=15 power=0.872315260457242\"\n[1] \"n0=13 n1=13 power=0.868827534112504\"\n[1] \"n0=12 n1=12 power=0.811527612034704\"\n\n\n\n     Power for Fisher's Exact Test \n\n          power = 0.8115276\n             n0 = 12\n             n1 = 12\n             p0 = 0.2\n             p1 = 0.8\n      sig.level = 0.05\n    alternative = two.sided\n  nullOddsRatio = 1\n\nNOTE: errbound= 1e-06"
  },
  {
    "objectID": "programming/web_basics.html",
    "href": "programming/web_basics.html",
    "title": "Web basics",
    "section": "",
    "text": "Resources\nhttps://jakobtures.github.io/web-scraping/rvest1.html"
  },
  {
    "objectID": "programming/web_basics.html#attributes",
    "href": "programming/web_basics.html#attributes",
    "title": "Web basics",
    "section": "Attributes",
    "text": "Attributes\nBasic syntax: &lt;tag attribute=\"value\"&gt;...&lt;/tag&gt;. No space between equal sign and value.\n\nWeb link\n&lt;a href=\"https://jakobtures.github.io/web-scraping/html.html\"&gt;This is a link&lt;/a&gt;\nThe code above creates an active link. The code below also points to this link, but opens in a new page using target=\"_blank\".\n&lt;a href=\"https://jakobtures.github.io/web-scraping/html.html\" target=\"_blank\"&gt;This is a link&lt;/a&gt;\n\n\nImages\nTwo images, one with adjusted size\n&lt;img src=\"https://jakobtures.github.io/web-scraping/Rlogo.png\"&gt;\n&lt;img src=\"https://jakobtures.github.io/web-scraping/Rlogo.png\" width=\"100\" height=\"100\"&gt;\nCan combine image with link, by puttinng the links within the anchor.\n&lt;a href=\"https://www.r-project.org/\" target=\"_blank\"&gt;&lt;img src=\"https://jakobtures.github.io/web-scraping/Rlogo.png\"&gt;&lt;/a&gt;"
  },
  {
    "objectID": "programming/web_basics.html#entities",
    "href": "programming/web_basics.html#entities",
    "title": "Web basics",
    "section": "Entities",
    "text": "Entities\nCoded representations of certain characters, &..;. For example,\n\n&lt; less than, &lt;\n&quot; ”\n&amp; &: ampersand\n&nbsp; non-breaking space"
  },
  {
    "objectID": "programming/web_basics.html#html-tags",
    "href": "programming/web_basics.html#html-tags",
    "title": "Web basics",
    "section": "HTML tags",
    "text": "HTML tags\n&lt;!DOCTYPE html&gt;\n\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Hello World!&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;b&gt;Hello World!&lt;/b&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nLine 1: declare which version of HTML. For now it is 5\nThe rest are different tags.\nImportant tags:\n\nheader, &lt;h1&gt;, &lt;h2&gt;, ..., &lt;h6&gt;\ndivision or span, &lt;div&gt;, &lt;span&gt;\nparagraph, &lt;p&gt;\nline break, &lt;br&gt;. This does not need to be closed with &lt;/br&gt;\nbold, italics &lt;b&gt;, &lt;i&gt;\nlists - ordered &lt;ol&gt;, unordered &lt;ul&gt;. Within the tags, use &lt;li&gt;\ntables. Lines are defined by &lt;tr&gt; (table row), table header &lt;th&gt; and &lt;td&gt;, table data.\nanchor, &lt;a&gt;, useful for url links: but it is different from &lt;link&gt; tag (which links to files such as JS or CSS)."
  },
  {
    "objectID": "study_design/rct_design_overview.html#intention-to-treat-itt",
    "href": "study_design/rct_design_overview.html#intention-to-treat-itt",
    "title": "Clinical trial design: overview",
    "section": "Intention to treat ITT",
    "text": "Intention to treat ITT\nCross-overs after rz: some patients might have a treatment (yes or no) beyond what they were assigned to, e.g. refuse surgery or medical treatment\nNon-adherence during followup: some in treatment group refuse of can not tolerate certain treatment; while some in placebo group require medication or take on their own"
  },
  {
    "objectID": "study_design/rct_design_overview.html#subgroup-analysis",
    "href": "study_design/rct_design_overview.html#subgroup-analysis",
    "title": "Clinical trial design: overview",
    "section": "Subgroup analysis",
    "text": "Subgroup analysis\nStratified analysis: estimate treatment effect separately in subgroups. Does not tell difference across different subgroups\nTest for interaction: use of main effect and interaction.\nIssue of multiple testing when doing a series of analyses\n\ninflate sample size to plan for subgroup analysis\nreport number of subgroup analyses performed\npossibly adjust for multiple comparisons\nreport CI instead of just p-values\n\n\nResources\n\nCoursera course Design and interpretation of clinical trials by Johns Hopkins University\nBook Fast Facts: Clinical trialss inn oncology: The fundamentals"
  },
  {
    "objectID": "study_design/adaptive_design.html",
    "href": "study_design/adaptive_design.html",
    "title": "Adaptive design: overview",
    "section": "",
    "text": "Resources\n\nR package rpact and tutorial\nR package gsDesign and tutorial\n\n\nSequential design\nNumber of patient isn’t set in advance. A good group sequential design can reduce the sample size needed, while keeping the desired statistical power and controlling the overall type I error.\nGSD includes pre-determined number of stages (interim, final). Each stage specified by\n\nsample size\ncritical values\nstopping criterion to support or reject null hypothesis\n\nTBC"
  },
  {
    "objectID": "inference/overview.html",
    "href": "inference/overview.html",
    "title": "Overview: causal inference",
    "section": "",
    "text": "\\(Y^{a=1}, Y^{a=0}\\) are the potential outcomes under treatment 1 and 0. They are random variables. Treatment A has causal effect if \\(Y^{a=1} \\neq Y^{a=0}\\).\nFor individual \\(i\\), \\(Y_i^{a=1}, Y_i^{a=0}\\) are deterministic.\nIn reality we do not observe both potential outcomes for an individual, since we only have ONE outcome. We observe \\(Y\\) and \\(A\\). For a population, average treatment effect (ATE) can be estimate"
  },
  {
    "objectID": "inference/estimation_gcomp_ipw.html",
    "href": "inference/estimation_gcomp_ipw.html",
    "title": "Estimation",
    "section": "",
    "text": "Introduction\nSmoking example\n\ndoes smoking cause lung cancer?\ndoes lung cancer cause people to smoke?\nis there a third factor that causes both smoking and lung cancer?\n\nPotential outcome framework\n\\(Y_i = Y_i(z)\\) if \\(Z_i = z\\), under treatment Z = z\nEstimand: a precise description of the treatment effect"
  },
  {
    "objectID": "models/survival.html",
    "href": "models/survival.html",
    "title": "Survival",
    "section": "",
    "text": "Links\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html\nhttps://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/\nhttps://www.danieldsjoberg.com/ggsurvfit/\nhttps://www.coursera.org/learn/survival-analysis-r-public-health\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html\nhttps://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/\njmpost: combines survival analysis, mixed effect model https://genentech.github.io/jmpost/main/"
  },
  {
    "objectID": "models/mixed_models.html",
    "href": "models/mixed_models.html",
    "title": "Mixed models for repeted measurements",
    "section": "",
    "text": "Resources:\n\nmmrm package vignette\nMixed models with R\n\nMMRM has one distinct feature compared to other linear mixed models: subject-specific random effects are considered as residual effects (part of error correlation matrix).\n\nMethodology\nBasic linear mixed-effects model for a single level of grouping\n\\[\ny_i = X_i \\beta + Z_i b_i + \\epsilon_i, i = 1, ..., n\n\\] \\[\nb_i \\sim N(0, \\Psi), \\epsilon \\sim N(0, \\sigma^2 I)\n\\]\n\n\\(\\beta\\) is p-dim vector of fixed effects\n\\(b\\) is q-dim vecor of random patient specific effects\n\\(X_i\\) of size \\(n_i \\times p\\) and \\(Z_i\\) of size \\(n_i \\times q\\) are regressor matrices relating observations to the fixed effects and random effects.\n\\(\\epsilon_i\\) is \\(n_i\\)-dimensional within-subject error"
  },
  {
    "objectID": "inference/rwd_rwe.html",
    "href": "inference/rwd_rwe.html",
    "title": "Real-world Data, Real-world Evidence",
    "section": "",
    "text": "RWD is any data collected outside clinical trial setting, can be combined with clinical trials.\n(The general benefits and disadvantages of RWD is coherent with EHR data)\nTherapheutic areas: oncology, rare diseases, infectious diseases among others\nTraditionally regarded as inferior evidence: lack of randomization, limited information on potential relevant prognostic factors.\nFDA: (2016) 21st Century Cures Act, evaluate the use of RWD in support of regulatory approvals and post-approval safety studies.\nEMA (2017): HMA/EMA Joint Big Data Task Force, establish a roadmap for the use of RWD in regulatory assessments\nChallenges of using RCT\nFocus: whether RWD can be trusted to reliably measure treatment effects of new drugs, causal relationship. The main difference between RCT and RWD is the confounding bias."
  },
  {
    "objectID": "inference/covariate_adjustment.html",
    "href": "inference/covariate_adjustment.html",
    "title": "Covariate adjustment",
    "section": "",
    "text": "https://jbetz-jhu.github.io/CovariateAdjustmentTutorial/\nBaseline covariates are variables measured prior to randomization, expected to have strong association with outcome. Potential confounding occurs when the distribution of baseline covariates between treatment groups are imbalanced.\nTo address confounding:\n\ndesign stage: stratified randomization to reduce imbalance\nanalysis stage: covariate adjustment\n\nBenefits:\n\nreduce sample size\nimprove precision - smaller CI, higher power\nsome CA do not depend on a correctly specified model\n\nRecent FDA guidance requires distinction between conditional and marginal treatment effects. These two coincide in linear models, but not in non-linear models (e.g. binary, ordinal, count, time-to-event outcomes)."
  },
  {
    "objectID": "programming/webr.html",
    "href": "programming/webr.html",
    "title": "WebR: Use with an existing quarto website",
    "section": "",
    "text": "When you create a new quarto website, inside _quarto.yml the output-dir isn’t specified. In this case, when you follow the tutorial by James J Balamuta (creator of the extension quarto-webr) you will be able to render a functional webR page. Yet, if you already have an existing quarto website deployed by Github actions (with output-dir: docs), the default solution might not work.\nFortunately, the fix is simple enough. I think this feature will be added to the future versions of quarto-webr extension, because this extension is just great."
  },
  {
    "objectID": "programming/shiny.html",
    "href": "programming/shiny.html",
    "title": "Shiny",
    "section": "",
    "text": "Number of bins:"
  },
  {
    "objectID": "programming/webr.html#installation",
    "href": "programming/webr.html#installation",
    "title": "WebR: Use with an existing quarto website",
    "section": "Installation",
    "text": "Installation\nOpen terminal, install the extension in the root of the current quarto project. This is important, as quarto extensions are project-based, i.e. need to be included in each quarto project.\nquarto add coatless/quarto-webr"
  },
  {
    "objectID": "programming/webr.html#configuration",
    "href": "programming/webr.html#configuration",
    "title": "WebR: Use with an existing quarto website",
    "section": "Configuration",
    "text": "Configuration\nAdd the following lines in the yaml header in the quarto file you want to run webR. For example,\ntitle: \"WebR demo\"\nengine: knitr\nformat: html\nfilters: \n  - webr\nwebr: \n  channel-type: \"post-message\"\nImportant bits:\n\nspecify engine to knitr\nspecify filters to - webr. This could alternatively be specified in the overall _quarto.yml file to apply to every qt document.\nadd channel-type: \"post-message\" under webr. No dash in front."
  },
  {
    "objectID": "programming/webr.html#configuration-1",
    "href": "programming/webr.html#configuration-1",
    "title": "WebR",
    "section": "Configuration",
    "text": "Configuration\nNow use the curly bracket {webr-r} for your code chunk (which used to be just {r}),\nLoading\n  webR...\n\n\n  \n\n\nA histogram that changes every time you click RUN CODE\nLoading\n  webR..."
  },
  {
    "objectID": "programming/webr.html#execution",
    "href": "programming/webr.html#execution",
    "title": "WebR: Use with an existing quarto website",
    "section": "Execution",
    "text": "Execution\nNow use the curly bracket {webr-r} for your code chunk (which used to be just {r}),\nLoading\n  webR...\n\n\n  \n\n\nA histogram that changes every time you click RUN CODE. This proves that we are running interactively the R code inside the web browser.\nLoading\n  webR..."
  },
  {
    "objectID": "programming/r_pkg_wf.html",
    "href": "programming/r_pkg_wf.html",
    "title": "R package Engineering Workflow",
    "section": "",
    "text": "Useful references:\nSteps suggested:"
  },
  {
    "objectID": "programming/r_pkg_wf.html#step-2-design-docs",
    "href": "programming/r_pkg_wf.html#step-2-design-docs",
    "title": "R package Engineering Workflow",
    "section": "Step 2: Design docs",
    "text": "Step 2: Design docs\nPurpose and scope: the package pkg_name shall …\nPackage requirements:\n\nlibrary(gt)\ndf &lt;- data.frame(obligation = c('Duty', 'Desire', 'Intension'), \n                 keyword = c('shall', 'should', 'will'), \n                 description = c('must have', 'nice to have', 'optional'))\n\ngt(df) |&gt; \n  cols_label(obligation = md('**Obligation**'), \n             keyword = md('**Key word**'), \n             description = md('**Description**'))\n\n\n\n\n\n  \n    \n    \n      Obligation\n      Key word\n      Description\n    \n  \n  \n    Duty\nshall\nmust have\n    Desire\nshould\nnice to have\n    Intension\nwill\noptional\n  \n  \n  \n\n\n\n\nUse some documentation tools (md, qmd, or diagram draw.io)"
  },
  {
    "objectID": "programming/r_pkg_wf.html#step-3-packaging",
    "href": "programming/r_pkg_wf.html#step-3-packaging",
    "title": "R package Engineering Workflow",
    "section": "Step 3: packaging",
    "text": "Step 3: packaging\n\ncreate basic project\nCopy and paste exisint R scripts, refactor if necessary (i.e. give self-explanatory names)\nCreate R generic functions (print, summary)\nDocument\n\n\n\n\n\n\n\nTo do\n\n\n\n\nreturn result as a list, with class attribute\nreturn argument"
  },
  {
    "objectID": "programming/r_pkg_wf.html#step-4-qualiy-code",
    "href": "programming/r_pkg_wf.html#step-4-qualiy-code",
    "title": "R package Engineering Workflow",
    "section": "Step 4: Qualiy code",
    "text": "Step 4: Qualiy code\nIt is important to have clean code, so that it is easier to read and maintain; easier to exntend; and the code runs faster.\n\nnaming. Make the function and argument names easy to understand\nformatting. Indentation, spacing, bracketing should be consistent\nsimplicity. Avoid unnecesary complexity. Split large source files into smaller chunks, preferably less than 1000 lines.\nsingle responsibility principle (SRP). Each function should have ONE single purpose.\ndon’t repeat yourself. Make a function!\ncomment.\nerror handling. Include error handling messages. tryCatch()\n\n\n\n\n\n\n\nTo do\n\n\n\n\nError handling.\n\n\n\n\nTesting\nWhy do you need unit tests? In short, increase reliability and maintainability of the code.\nOther types of tests exist: integration testing, performance testing, snapshot testing. Package testthat allows not only unit tests, but also the other types of tests.\n\n\n\n\n\n\nNote\n\n\n\nA more comprehensive guide see my other note: R package: tests\n\n\nTest coverage: use covr. Ideally should cover 100%.\n\n\nPackage quality check\nR CMD Check\n\n\nCode style\nUse tidyverse style guide.\n\nstyler: restyle text, files or entire project.\nlintr: perform automated checks to confirm that our code conform to the style guide.\ndevtools::spell_check."
  },
  {
    "objectID": "programming/r_pkg_tests.html",
    "href": "programming/r_pkg_tests.html",
    "title": "R package: tests",
    "section": "",
    "text": "Useful references:\nUnit test: tests whether your function returns values as expected.\nBenefits:\nExample situations:"
  },
  {
    "objectID": "programming/r_pkg_wf.html#step-5.-publication",
    "href": "programming/r_pkg_wf.html#step-5.-publication",
    "title": "R package Engineering Workflow",
    "section": "Step 5. Publication",
    "text": "Step 5. Publication\npkgdown website might be the most useful place.\nVersioning\n\nx.y.z\nx is major, breaking changes\ny is minor, new features\nz is patch, bug fixes\ntry usethis::use_version()"
  },
  {
    "objectID": "programming/revealjs.html",
    "href": "programming/revealjs.html",
    "title": "Make slides with reveal.js",
    "section": "",
    "text": "iframe stands for inline frame. It is an HTML element that loads another HTML page within the document."
  },
  {
    "objectID": "programming/test_slides.html#hello-there",
    "href": "programming/test_slides.html#hello-there",
    "title": "Quarto Presentations",
    "section": "",
    "text": "This presentation will show you examples of what you can do with Quarto and Reveal.js, including:\n\nPresenting code and LaTeX equations\nIncluding computations in slide output\nImage, video, and iframe backgrounds\nFancy transitions and animations\nPrinting to PDF"
  },
  {
    "objectID": "programming/revealjs.html#embed-in-your-own-web-page",
    "href": "programming/revealjs.html#embed-in-your-own-web-page",
    "title": "Make slides with reveal.js",
    "section": "",
    "text": "iframe stands for inline frame. It is an HTML element that loads another HTML page within the document."
  },
  {
    "objectID": "programming/index.html#r-programming",
    "href": "programming/index.html#r-programming",
    "title": "Programming",
    "section": "",
    "text": "R related topics from basic to advanced.\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nOOP in R: S3\n\n\n\n\nError handling in R\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "programming/index.html#r-package",
    "href": "programming/index.html#r-package",
    "title": "Programming",
    "section": "R package",
    "text": "R package\nR package development\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nR package: tests\n\n\n\n\nR package Engineering Workflow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "programming/index.html#web",
    "href": "programming/index.html#web",
    "title": "Programming",
    "section": "Web",
    "text": "Web\nWeb and quarto topics\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nWebR: Use with an existing quarto website\n\n\n\n\nWeb basics\n\n\n\n\nMake slides with reveal.js\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "programming/r_pkg_0_wf.html",
    "href": "programming/r_pkg_0_wf.html",
    "title": "R package Engineering Workflow",
    "section": "",
    "text": "Useful references:\nSteps suggested:"
  },
  {
    "objectID": "programming/r_pkg_0_wf.html#step-2-design-docs",
    "href": "programming/r_pkg_0_wf.html#step-2-design-docs",
    "title": "R package Engineering Workflow",
    "section": "Step 2: Design docs",
    "text": "Step 2: Design docs\nPurpose and scope: the package pkg_name shall …\nPackage requirements:\n\nlibrary(gt)\ndf &lt;- data.frame(obligation = c('Duty', 'Desire', 'Intension'), \n                 keyword = c('shall', 'should', 'will'), \n                 description = c('must have', 'nice to have', 'optional'))\n\ngt(df) |&gt; \n  cols_label(obligation = md('**Obligation**'), \n             keyword = md('**Key word**'), \n             description = md('**Description**'))\n\n\n\n\n\n\n\n\nObligation\nKey word\nDescription\n\n\n\n\nDuty\nshall\nmust have\n\n\nDesire\nshould\nnice to have\n\n\nIntension\nwill\noptional\n\n\n\n\n\n\n\n\nUse some documentation tools (md, qmd, or diagram draw.io)"
  },
  {
    "objectID": "programming/r_pkg_0_wf.html#step-3-packaging",
    "href": "programming/r_pkg_0_wf.html#step-3-packaging",
    "title": "R package Engineering Workflow",
    "section": "Step 3: packaging",
    "text": "Step 3: packaging\n\ncreate basic project\nCopy and paste exisint R scripts, refactor if necessary (i.e. give self-explanatory names)\nCreate R generic functions (print, summary)\nDocument\n\n\n\n\n\n\n\nTo do\n\n\n\n\nreturn result as a list, with class attribute\nreturn argument"
  },
  {
    "objectID": "programming/r_pkg_0_wf.html#step-4-qualiy-code",
    "href": "programming/r_pkg_0_wf.html#step-4-qualiy-code",
    "title": "R package Engineering Workflow",
    "section": "Step 4: Qualiy code",
    "text": "Step 4: Qualiy code\nIt is important to have clean code, so that it is easier to read and maintain; easier to exntend; and the code runs faster.\n\nnaming. Make the function and argument names easy to understand\nformatting. Indentation, spacing, bracketing should be consistent\nsimplicity. Avoid unnecesary complexity. Split large source files into smaller chunks, preferably less than 1000 lines.\nsingle responsibility principle (SRP). Each function should have ONE single purpose.\ndon’t repeat yourself. Make a function!\ncomment.\nerror handling. Include error handling messages. tryCatch()\n\n\n\n\n\n\n\nTo do\n\n\n\n\nError handling.\n\n\n\n\nTesting\nWhy do you need unit tests? In short, increase reliability and maintainability of the code.\nOther types of tests exist: integration testing, performance testing, snapshot testing. Package testthat allows not only unit tests, but also the other types of tests.\n\n\n\n\n\n\nNote\n\n\n\nA more comprehensive guide see my other note: R package: tests\n\n\nTest coverage: use covr. Ideally should cover 100%.\n\n\nPackage quality check\nR CMD Check\n\n\nCode style\nUse tidyverse style guide.\n\nstyler: restyle text, files or entire project.\nlintr: perform automated checks to confirm that our code conform to the style guide.\ndevtools::spell_check."
  },
  {
    "objectID": "programming/r_pkg_0_wf.html#step-5.-publication",
    "href": "programming/r_pkg_0_wf.html#step-5.-publication",
    "title": "R package Engineering Workflow",
    "section": "Step 5. Publication",
    "text": "Step 5. Publication\npkgdown website might be the most useful place.\nVersioning\n\nx.y.z\nx is major, breaking changes\ny is minor, new features\nz is patch, bug fixes\ntry usethis::use_version()\n\nAdding badges\n\n# lifecycle\nusethis::use_lifecycle_badge(stage = 'experimental')\n# R-CMD-check \nusethis::use_github_action_check_standard()"
  },
  {
    "objectID": "programming/intv_rdev_topics.html",
    "href": "programming/intv_rdev_topics.html",
    "title": "Topic coverage: R developer",
    "section": "",
    "text": "This topic list does NOT include\n\n\n\n\nshiny developer\nstatistical programmer (would require more on specific statistical packages)\ndata scientist with a focus on engineering\n\n\n\nResources:\n\nAdvanced R\nR packages (2e)\n\nTypical thought process:\n\nWhat is x\nWhen do you use x\nWhat are the pros and cons of x\n\n\n\n\n\n\n\nScattered knowledge dumpster\n\n\n\n\nvector: diff na.rm = T and na.omit(x)\nenvironment: function in global env. what does it mean (scope)\nerror message: difference between stop() and rlang::abort()\n\ndata manipulation\n\ndata table: why do I prefer to use it compared to tibble and df\n\n\n\n\nR Programming\nFoundations\n\nData structures in R\nEnvironments\n\nFunctions and functional programming\n\napply() functions\nEvaluation\nArgument\n\nOOP\n\nS3 generics\n\nDebugging\n\nSomething related to debugging\n\n\n\nData manipulation\n\nTidyverse suite\nText processing\n\n\n\nR package\n\nDevelopment workflow\nR code and functions\nData\nDESCRIPTION\nDependencies and enviroment\nTesting\nDocumentation: code\nDocumentation: vignettes\nDoucmentation: website\nMaintenance"
  },
  {
    "objectID": "programming/r_pkg_tests.html#use-testthat-in-a-package",
    "href": "programming/r_pkg_tests.html#use-testthat-in-a-package",
    "title": "R package: tests",
    "section": "Use testthat in a package",
    "text": "Use testthat in a package\n\n1. Initialize\nCreate tests/testthat/ directory\n\nusethis::use_testthat(3)\n\nThis creates the directory with\n\nan empty folder testthat where you write your tests\nan R script testthat.R where tests are run when R CMD check is run. Do not modify this file.\n\n\n\n2. Create a test\nTest files must have names that start with test. For example, a function is R/fn_name.R, then test is tests/testthat/test-fn_name.R.\n\nusethis::use_test('testname')\n\n\n\n3. Run a test\n\ntestthat::test_file('tests/testthat/test-foofy.R')\nRun Tests button\ndevtools::test() for entire test suite. Cmd + Shift + T\ndevtools::check()\n\n\n\n\n\n\n\nA workflow worked for me\n\n\n\n\nCreate a simple function\nCreate a test file immediately, with clear naming. Inside this test file, can write various tests for the same function.\n\n\nhave at least a test that expects the correct result (expect_identical() or else)\nhave at least a test that expects error, (expect_error()). Inside the original function, error should be thrown by rlang::abort().\n\n\nRun test\nRun test coverage, covr::package_coverage() or covr::code_coverage()\nCheck"
  },
  {
    "objectID": "programming/r_functions.html",
    "href": "programming/r_functions.html",
    "title": "Functions",
    "section": "",
    "text": "Special argument .... This type of argument is varargs (variable arguments). The function can take any numbers of arguments.\nPrimary uses:\n\nyour function takes a function as an argument, and we want to pass additional arguments\nS3 generic, need to allow methods to take extra arguments\n\nDownsides:\n\nneed to explain where the arguments go to\nmisspelled argument will not raise an error\n\n\n\nThe ... can be something like na.rm = F.\nGiven that the first argument is being averaged upon, if a vector is not specified correctly, only the first element is being averaged; and the other elements are treated as additional arguments that are not necessarily used.\n\n# mean(c(1,2,3)) \nmean(1, 2, 3)\n\n[1] 1\n\nmean(c(1, 2), 3)\n\n[1] 1.5\n\n\n\nfplus &lt;- function(a, ...){\n  list(sum(a), ...)\n}\nfplus(a = c(1,2,3))\n\n[[1]]\n[1] 6\n\nfplus(a = c(1,2,3), 4)\n\n[[1]]\n[1] 6\n\n[[2]]\n[1] 4\n\n\n\nf1 &lt;- function(a, ...){\n  args &lt;- list(...)\n  if('y' %in% names(args)){\n    args$y &lt;- 0.5 + args$y\n    do.call(f2, args) # second arg need to be a list\n  }else{\n    a+1\n  }\n\n}\nf2 &lt;- function(y){return(y)}"
  },
  {
    "objectID": "programming/r_functions.html#dot-dot-dot",
    "href": "programming/r_functions.html#dot-dot-dot",
    "title": "Functions",
    "section": "",
    "text": "Special argument .... This type of argument is varargs (variable arguments). The function can take any numbers of arguments.\nPrimary uses:\n\nyour function takes a function as an argument, and we want to pass additional arguments\nS3 generic, need to allow methods to take extra arguments\n\nDownsides:\n\nneed to explain where the arguments go to\nmisspelled argument will not raise an error\n\n\n\nThe ... can be something like na.rm = F.\nGiven that the first argument is being averaged upon, if a vector is not specified correctly, only the first element is being averaged; and the other elements are treated as additional arguments that are not necessarily used.\n\n# mean(c(1,2,3)) \nmean(1, 2, 3)\n\n[1] 1\n\nmean(c(1, 2), 3)\n\n[1] 1.5\n\n\n\nfplus &lt;- function(a, ...){\n  list(sum(a), ...)\n}\nfplus(a = c(1,2,3))\n\n[[1]]\n[1] 6\n\nfplus(a = c(1,2,3), 4)\n\n[[1]]\n[1] 6\n\n[[2]]\n[1] 4\n\n\n\nf1 &lt;- function(a, ...){\n  args &lt;- list(...)\n  if('y' %in% names(args)){\n    args$y &lt;- 0.5 + args$y\n    do.call(f2, args) # second arg need to be a list\n  }else{\n    a+1\n  }\n\n}\nf2 &lt;- function(y){return(y)}"
  },
  {
    "objectID": "programming/r_pkg_functions.html",
    "href": "programming/r_pkg_functions.html",
    "title": "My workflow in working with functions in a package",
    "section": "",
    "text": "Ongoing notes\n\n\n\nSome of the content are being added as I go.\nFunctions make up the whole R package, except for data-only packages. The workflow should help me navigate the process.\nA short example borrowed from GSWEP4R workshop is documented here, function code and test code"
  },
  {
    "objectID": "programming/r_pkg_functions.html#structure",
    "href": "programming/r_pkg_functions.html#structure",
    "title": "My workflow in working with functions in a package",
    "section": "Structure",
    "text": "Structure"
  },
  {
    "objectID": "programming/r_pkg_functions.html#write",
    "href": "programming/r_pkg_functions.html#write",
    "title": "My workflow in working with functions in a package",
    "section": "Write",
    "text": "Write\n\nWhat to return?\nPut the arguments in a list named result, then attach other outputs to result.\n\nresult &lt;- list(arg1 = arg1, arg2 = arg2)\n\nAfterwards, set class attribute to something meaningful, for example,\n\nresult &lt;- structure(result, class = 'SimulationResult')\n\nThis allows us to implement generics.\n\n\nGenerics\nCommon generics\n\nprint\nsummary\nplot\n\n\nprint.classname &lt;- function(x, ...){\n  # x is an object of `classname`\n  # it should be a list, and have class attribute `classname`\n}"
  },
  {
    "objectID": "programming/r_pkg_functions.html#test",
    "href": "programming/r_pkg_functions.html#test",
    "title": "My workflow in working with functions in a package",
    "section": "Test",
    "text": "Test\nTests for functions and generic functions seem to be the same as before. Need to go back to the function and add error messages."
  },
  {
    "objectID": "programming/r_pkg_functions.html#document",
    "href": "programming/r_pkg_functions.html#document",
    "title": "My workflow in working with functions in a package",
    "section": "Document",
    "text": "Document\nThe way to document generic functions is exactly the same as any other function.\n\n#' Print method\n#'\n#' @description\n#' Generic function to print a `SimulationResult` object\n#'\n#' @param x a \\code{SimulationResult} object to print\n#' @param ... further arguments to pass from other methods\n#'\n#' @return something printed\n#' @export\n#'\n#' @examples\n#' simd &lt;- fsim(n1 = 10, n2 = 10, mean1 = 0, mean2 = 5, sd1 = 1, sd2 = 1)\n#' print(simd)"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git_branching.html",
    "href": "dev/oldnotes_programming/git/Git_branching.html",
    "title": "Update from origin and update branches",
    "section": "",
    "text": "as the same user (main github account)\n\ncloned the repo Paper3, it created a new path /Documents/GitHub/Paper3/"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git_branching.html#branches",
    "href": "dev/oldnotes_programming/git/Git_branching.html#branches",
    "title": "Update from origin and update branches",
    "section": "Branches",
    "text": "Branches\nType 1 branch: local branches\nType 2 branch: remote-tracking branches\n\n\n\ngit_branching\n\n\n\nCreate and switch to branch\ngit checkout -b my_new_branch\n\n# what it does:: \ngit branch my_new_branch\ngit checkout my_new_branch"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git_branching.html#master-is-ahead-of-branch",
    "href": "dev/oldnotes_programming/git/Git_branching.html#master-is-ahead-of-branch",
    "title": "Update from origin and update branches",
    "section": "master is ahead of branch:",
    "text": "master is ahead of branch:\nUpdate the branch.\ngit checkout my_new_branch  # go to branch\ngit status # this branch is still up to date with origin/my_new_branch, even though it is behind master\nCan check the difference between these two. Note that the order matter: these two are not the same!\ngit diff my_new_branch master \ngit diff master my_new_branch\nmerge into master (even though branch is behind master!)\ngit merge master  # when on branch\nAt this stage, the branch will be ahead of the remote branch origin/my_new_branch. Now need to push the updates through, to make the remote branch updated too.\n\n\n\ngit_merge_eg2"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git_branching.html#branch-is-ahead-of-master",
    "href": "dev/oldnotes_programming/git/Git_branching.html#branch-is-ahead-of-master",
    "title": "Update from origin and update branches",
    "section": "branch is ahead of master",
    "text": "branch is ahead of master\n\nOption 1: merge branch into master locally, then push master\nNeed to check difference to make sure everything is correct.\ngit merge my_new_branch  # when on master. \n\n\n\ngit_merge_eg3\n\n\nNote: from network of branches, this merge will NOT appear as the merge is done locally.\n\n\nOption 2: push to remote branch, pull request, and merge into master\nOn branch, NOT master!\n\nIf push to branch (NOT master, WITHOUT merge) at this point, the changes will appear on github under this branch (remote), which requires a pull request.\nIf merge master (on branch), nothing will happen.\n\n\n\n\ngit_merge_eg"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git_branching.html#delete-branch-after-pull-requested-and-merging-from-remote",
    "href": "dev/oldnotes_programming/git/Git_branching.html#delete-branch-after-pull-requested-and-merging-from-remote",
    "title": "Update from origin and update branches",
    "section": "delete branch (after pull requested and merging from remote)",
    "text": "delete branch (after pull requested and merging from remote)\ngit fetch -p \n\n# check which branch is there\ngit branch\ngit branch --merged # which merged branch we can delete"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html",
    "href": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html",
    "title": "Template 1: minimal",
    "section": "",
    "text": "_quarto.yml"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#metadata",
    "href": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#metadata",
    "title": "Template 1: minimal",
    "section": "",
    "text": "_quarto.yml"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#index",
    "href": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#index",
    "title": "Template 1: minimal",
    "section": "index",
    "text": "index\nindex.qmd"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#about",
    "href": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#about",
    "title": "Template 1: minimal",
    "section": "about",
    "text": "about\nabout.qmd"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#style-definition",
    "href": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#style-definition",
    "title": "Template 1: minimal",
    "section": "style definition",
    "text": "style definition\nstyle.css\n\n\nTo do\nimport one file from inst"
  },
  {
    "objectID": "dev/oldnotes_programming/unix/BashAndLinux.html",
    "href": "dev/oldnotes_programming/unix/BashAndLinux.html",
    "title": "General",
    "section": "",
    "text": "it is a standard subdirectory of the root directory / in unix-like OS that contains tthe executables. It contains the most basic programs (ls, rm, …) . Name means binaries, compiled programs. (most important one)\nwhich programName will give where the program is (path). For example, which R will give /usr/local/bin/R and a few other useful programs are under here (psql, brew etc).\n/usr stands for Unix System Resources\n`` is called ‘backticks’.\n\n\n\n\n\n\nvariable\npurpose\nValue\n\n\n\n\nHOME\nhome direcotry\n/Users/andrea\n\n\nPWD\npresent working directory\npwd\n\n\nSHELL\nwhich shell is used\n/bin/bash\n\n\nUSER\nuser’s ID\nandrea\n\n\n\nset | grep HISTFILESIZE # history file size \nsetting\n\n\necho text  # print text\necho USER  # print USER, it is a variable name\necho $USER # print value of the variable, which is andrea \necho $OSTYPE # darwin18, in other places might be linux-gnu\n\n\n\n\nlocal, user defined\nselfdefinedvar=seasonal/winter.csv  # without space! \necho $selfdefinedvar\nhead -n 1 $selfdefinedvar     # note $ is necessary\n\n\n\nlocale  # check local variables \nnano ~/.profile  # write what is necessary \n. ~/.profile  # execute\njust use language “en_US.UTF-8”\nLC_ALL=\"en_US.UTF-8\"\n\n\n\nChiZhangWork:~ andrea$ pwd\n/Users/andrea\n\n\n\nman head   # manual for head\n\n\n\nhead summer.csv  # reports error, wrong directory\ncd seasonal/\n\n!head   # useful! automatically runs \nhistory  # displays history \n!3      # if that is the 3rd command \n\n\n\nread, write, execute.\nls -l [path]\n-rwxr----x    \n\nfirst character is file type: - means normal file, d means directory\nnext 3 characters are permissions for owner. if -, absence of permission.\nnext 3 characters are permissions for the group. Read, write and execute. In this example, no permission apart from read.\nlast 3 are permissions for others.\n\nchmod [permission][path]\n# example \nchmod g+x frog.png\n\nwho change? [ugoa]: user (owner), group, others, all\ngranting or revoking the permission? + or -\nwhich permission are we setting? r, w, x.\n\nPermission 755 is -rwxr-xr-x, which means standard file -, owner has all permission rwx, group and others have the same permission r-x.\nchmod 755 frog.png"
  },
  {
    "objectID": "dev/oldnotes_programming/unix/BashAndLinux.html#view-content",
    "href": "dev/oldnotes_programming/unix/BashAndLinux.html#view-content",
    "title": "General",
    "section": "View content",
    "text": "View content\n\nselect rows\ncat  # concatenate\nless bashLearning/Bash.md blogdown.md  # view 2, use :n and :p to shift file \nmore \nhead # 10 rows \nhead -n 3 file.txt\ntail \n\n\nList all\nls -R  # R for recursive \nls -F  # F will give a * after all the runable program (i.e. C++)\n\n\nselect columns\n-f for fields to specify columns\n-d for delimiter for separator\ncut -f 2-5, 8, -d , values.csv    # select col 2 to 5 and 8, using comma as separator. \n\n\nselect with search\nselect lines according to what file contains\ngrep keyword file.txt     # returns lines that match keyword \n\n-c # count of matching lines \n-n # line number\n-v # those don't contain\n-h # suppress the file names (default when there's only 1 file)\ngrep -c incisor seasonal/autumn.csv seasonal/winter.csv # count occurence in 2 files \n\n\ncount\nwc -l # line count\nwc -c # characters,bytes\nwc -w # word count\n\n\nsort\nsort -n  # numerically\nsort -r # reverse \nsort -b # ignore leading blanks \nsort -f # fold case\nOften used with unique,\ncut -d , -f 2 seasonal/winter.csv | grep -v Tooth | sort |uniq -c  # give unique and count\n\n\nredirect output\nhead -n 5 seasonal/summer.csv &gt; top.csv # saved to top.csv\nNote: never in the middle of a pipe!\n\n\npipe\nhead -n 5 seasonal/summer.csv | tail -n 3\n\n\nWild cards\n*.csv\n?017.txt     # single character \n201[78].txt. # either 2017 or 2018. ONLY ONE CHARACTER\n{*.txt, *.csv}  # anything inside \n\n\nloop\nfor … variable … in .. list .. ; do .. body.. ; done\nITskills andrea$ for filename in *.md; do echo $filename; done  # this gives the names of md files in ITskills\nIt is convenient to name a bunch of files with one shell variable, then refer to it later on.\nfiles=seasonal/*.csv\nfor f in $files; do echo $f; done    # the $ in the first part is important"
  },
  {
    "objectID": "dev/oldnotes_programming/unix/BashAndLinux.html#edit-files",
    "href": "dev/oldnotes_programming/unix/BashAndLinux.html#edit-files",
    "title": "General",
    "section": "Edit files",
    "text": "Edit files\ncreate a blank file: touch fileName.\nnano file.txt\nCtrl + K: delete\nCtrl + U: un-delete\nCtrl + O: save file (output)\nCtrl + X: exit editor"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-debugging.html",
    "href": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-debugging.html",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "The chapter of debugging\n\ntraceback()"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt3-oop.html",
    "href": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt3-oop.html",
    "title": "OOP in R",
    "section": "",
    "text": "Chapter 3: object oriented programming with R"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt3-oop.html#base-types",
    "href": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt3-oop.html#base-types",
    "title": "OOP in R",
    "section": "Base types",
    "text": "Base types\nEverything in R is an object, but not object-oriented.\nobject (in R)\n|\n|_base objects\n|_OO objects\nEvery object has a base type.\ntypeof(1:10)  # integer\ntypeof(mtcars)  # list\n\n# object type\notype(1:10)  # base\notype(mtcars)  # s3\n\nvectors\n\nNULL\nlogical, integer, double, character, complex, raw\nlist\n\nFunctions\n\nclosure (regular R functions)\nspecial (internal functions, such as [)\nbuiltin (primitive functions, such as sum)\n\nlanguage components\n\nsymbol, language, pairlist"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/r_regular_expression.html",
    "href": "dev/oldnotes_programming/r_advanced_r_book/r_regular_expression.html",
    "title": "Regular expressions",
    "section": "",
    "text": "Regular expressions\ncombination of literals (i.e. words) and metacharacters (i.e. grammar)\nmatching literals (exact words) are not enough, we need a way to express\n\nwhitespace\nsets of literals\nBeginning and end of a line\nalternatives (‘war’ or ‘peace’)\n…\n\nuse meta characters!\n\n^: start of a line\n^i think matches\ni think we are ..\ni think this …\n\n\n$: end of a line\nmorning$ matches\nwell they had something this morning\ngood morning\n\n\n[] character classes\n[Bb][Uu][Ss][Hh] matches\nBush\nbush\n^[Ii] am matches I am, i am\n^[0-9][a-zA-Z] matches 7th …, 2nd…, 3am…, …. (line starts with one number and one letter)\n\n\n^ inside []: Not in the class\n[^?.]$ matches characters/sentences that do NOT end with ? or .\ni like basketballs\n6 and 9\nanyway!\n\n\n.: any character\n9.11 matches\n9-11\n9.11\n169.114\n9:11:46AM\n… 8199119725 …\n\n\n\\.: period\n(. in this case is not a meta character, but a literal character)\n\n\n?: optional (outside [])\n[Gg]eorge( [Ww]\\.)? [Bb]ush\ngeorge bush\nGeorge W. Bush\ntwo george bushes\n\n\n|: or\nflood|fire matches firewire …, global flood makes sense,…, floods, horricanes\n^[Gg]ood|[Bb]ad means good/Good needs to be at the beginning, or Bad/bad anywhere\n^([Gg]ood|[Bb]ad) means good/Good and Bad/bad need to be at the beginning\n\n\n*: any number, including none\n(.*) matches\nchat? (24, m, germany)\n()\n\n\n+: at least one of the item\n[0-9]+ (.*)[0-9]+\nmeans a number can be repeated many times\n\n\n{}: inerval\n[Bb]ush( +[^ ]+ +){1, 5} debate\nat least one space, followed by NOT space, followed by a space === a word, repeated between 1 and 5 times\nBush has historically won all major debates he’s done"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt1-basics.html",
    "href": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt1-basics.html",
    "title": "Names and values",
    "section": "",
    "text": "Notes from book Advanced R (part 1, foundations)"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt1-basics.html#handlers",
    "href": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt1-basics.html#handlers",
    "title": "Names and values",
    "section": "handlers",
    "text": "handlers\ntryCatch() for error\nfail_with &lt;- function(expr, value = NULL){\n  tryCatch(\n    error = function(cnd) value,  # this function here is necessary\n    expr\n  )\n}\nfail_with(log(10))   # execute normally\nfail_with(log('x'))  # here value is prompted\nfail_with(log('x'), value = NA)   # returns NA\nDifference between try() and tryCatch(): if the former meets an error, it passes onto the next code chunk to execute. The later will modify the behaviour of the exit, for example, give a specific value if an error occurs.\nwithCallingHandlers() for warning and messages"
  },
  {
    "objectID": "dev/oldnotes_stat/survival/notes_survival_intro.html",
    "href": "dev/oldnotes_stat/survival/notes_survival_intro.html",
    "title": "KM plot",
    "section": "",
    "text": "probability of surviving\nsurvival function"
  },
  {
    "objectID": "dev/oldnotes_stat/survival/notes_survival_intro.html#life-tables",
    "href": "dev/oldnotes_stat/survival/notes_survival_intro.html#life-tables",
    "title": "KM plot",
    "section": "life tables",
    "text": "life tables\nmeasure p of death at a given age, and life expectancy at varying ages\ncohort/generational life tables, current/period life tables"
  },
  {
    "objectID": "dev/oldnotes_stat/inference/notes_sample_size.html",
    "href": "dev/oldnotes_stat/inference/notes_sample_size.html",
    "title": "Sample size calculation",
    "section": "",
    "text": "Two types of statistical inference: confidence intervals, hypothesis testing\nTwo approaches\nSource of error\ntruth + systematic error + random error\nsystematic error (bias): faulty design, lack of randomization, blinding\nrandom error can be reduced by increasing the sample size\nThe effect of random error decreases by the square roo of n as samples increases\nSE of the mean: sigma/sqrt(n)\nSE of a proportion: sqrt(p(1-p)/n)"
  },
  {
    "objectID": "dev/oldnotes_stat/inference/notes_sample_size.html#precision-based",
    "href": "dev/oldnotes_stat/inference/notes_sample_size.html#precision-based",
    "title": "Sample size calculation",
    "section": "Precision based",
    "text": "Precision based\n\nmean\nfor given precision a, standard deviation sigma, number of observation required:\nn = 4 sigma^2 / a^2\n\n\nproportion\nn = 4 p(1-p)/a^2"
  },
  {
    "objectID": "dev/oldnotes_stat/inference/notes_sample_size.html#hypothesis-testing",
    "href": "dev/oldnotes_stat/inference/notes_sample_size.html#hypothesis-testing",
    "title": "Sample size calculation",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nPower = P(reject h0 | ha true), i.e. identifying a difference between groups if there is a difference\n80%"
  },
  {
    "objectID": "dev/oldnotes_stat/inference/notes_sample_size.html#effect-size",
    "href": "dev/oldnotes_stat/inference/notes_sample_size.html#effect-size",
    "title": "Sample size calculation",
    "section": "Effect size",
    "text": "Effect size\nformula depends on type of analysis\n\ntwo sample t-test\npaired -test\ncompare proportions\n\nEffect size\n\nclinically relevant difference, \\(\\Delta\\)\nstadard deviation in both groups, sigma\neffect size: \\(\\Delta\\)/sigma\n\nExample\n\ncompare two groups, difference set to be \\(\\Delta = 0.5\\), standard deviation \\(\\sigma = 1\\)\nEffect size: \\(0.5/1 = 0.5\\)\nSet significance level alpha = 0.05, power 1-beta = 0.8\n(by nomogram) total sample size is 120, i.e. each group needs 60"
  },
  {
    "objectID": "dev/oldnotes_stat/inference/notes_sample_size.html#formula-two-means-indep-samples",
    "href": "dev/oldnotes_stat/inference/notes_sample_size.html#formula-two-means-indep-samples",
    "title": "Sample size calculation",
    "section": "Formula (two means, indep samples)",
    "text": "Formula (two means, indep samples)\nn = 2 sigma2/delta2 k\n\nk = (u+v)^2\nu: one sided percentage point of normal distribution corresponding to beta\n\nu = 0.84 for 80% power\n\nv: two-sided percentage point of normal distribution corresponding to alpha\n\nv = 1.96 for 5% significance level\n\n\n(above example)\n\nn = 2/0.5^2 *(1.96+0.84)^2 = 62.72"
  },
  {
    "objectID": "dev/oldnotes_stat/inference/notes_sample_size.html#formula-paired-data",
    "href": "dev/oldnotes_stat/inference/notes_sample_size.html#formula-paired-data",
    "title": "Sample size calculation",
    "section": "Formula: paired data",
    "text": "Formula: paired data\ncross-over study\neffect size: 2 delta/sigma d\n\nsigma d is the sd of difference between the two measurements"
  },
  {
    "objectID": "dev/oldnotes_stat/inference/notes_sample_size.html#proportions-in-two-groups",
    "href": "dev/oldnotes_stat/inference/notes_sample_size.html#proportions-in-two-groups",
    "title": "Sample size calculation",
    "section": "Proportions in two groups",
    "text": "Proportions in two groups\n\np1, p2\nrelevant difference: p1-p2\naverage proportion: pbar = (p1+p2)/2\neffect size: (p1-p2)/(sqrt(pbar * (1-pbar)))\n\nExample:\nproportions are 0.1, 0.2\naverage is 0.15\neffect size: (0.2-0.1)/sqrt(0.15*(1-0.15)) = 0.28"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/diagnostics.html",
    "href": "dev/oldnotes_stat/lm_glm/diagnostics.html",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "(most of them will be negative)\nif increase, indicates better fit"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/diagnostics.html#log-likelihood",
    "href": "dev/oldnotes_stat/lm_glm/diagnostics.html#log-likelihood",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "(most of them will be negative)\nif increase, indicates better fit"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/count_4_bayes_glm_count.html",
    "href": "dev/oldnotes_stat/lm_glm/count_4_bayes_glm_count.html",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "count data modeling in the bayesian framework\n\nData: yi | beta ~ pois(lambda_i), log(lambda_i) = bx_i\nPriors: normal priors\n\nBeta ~ N()\n\n\nsetting prior for beta (normal)\n\nmean: assume typical average response is around 7\n\ncan set beta_0 ~ N(2, sigma) (on log scale)\nLog(7) = 1.95\n\nsd\n\nlogged number is around 2 (between 1-3, for instance)\nactual number is between 3 to 20 (exp(1), exp(3))"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/count_2_overdisp.html",
    "href": "dev/oldnotes_stat/lm_glm/count_2_overdisp.html",
    "title": "Quasi poisson",
    "section": "",
    "text": "Over dispersion\n\nThese are still in the GLM framework. They all have the same log-linear mean function \\(log(\\mu) = X\\beta\\), but the remaining assumptions are different.\nsandwich covariates\nAfter using this, the SE is larger -&gt; more conservative for significant covariates -&gt; fewer significant\n\nQuasi poisson\n\\(\\phi\\) estimated from data, not fixed at 1\n\\(\\beta\\) estimates are the same, but inference is different\nqp is not a full maximum likelihood model, but a quasi-ml model\nless assumption:\n\nlog-linear relationship between e(y|x) and xb\nlinear relationship between variance and expectation\n\nuse robust sandwich standard errors\n\n\nestimated differently, can not use ML. Some tests can not be used\n(doesn’t seem to have a bayesian version)\n\n\nNB regression\n\\(\\phi = 1\\), \\(V(\\mu) = \\mu + \\mu^2/\\theta\\)\nGeometric model (theta = 1)\npositive reciprocal dispersion parameter \\(\\phi\\) included, such that\n\\(sd(y|x) = \\sqrt{E(y|x) + E(y|x)^2 /\\phi}\\)\n\nSmaller \\(\\phi\\), more dispersion (i.e. larger sd(y|x))\n\\(\\phi\\) -&gt; inf, approach poisson (equal mean and var)"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/count_3_zeroinf.html",
    "href": "dev/oldnotes_stat/lm_glm/count_3_zeroinf.html",
    "title": "Excess 0",
    "section": "",
    "text": "Assumes 0 comes from 2 different process. These are not GLMs. 0’s likelihood is increased\nKey difference between the 2 types of subject:\n\nhurdle: 1. never experience outcome; 2. experience at least once\nzero inflated: 1. never experience outcome; 2. experience, but not always. (Can be zero)\n\n\n\nbinary process.\n\noff with probably \\(\\pi\\), only zero counts possible\non with probablity \\(1-\\pi\\).\n\n\n\n\n\n\nCount need to be at least 1.\nUse zero-truncated PD\n\n\n\nConceptual example: first decide whether buy things: yes or no. If yes, can end up buying (positive counts), can also end up buying nothing (out of stock)\nUse an usual discrete PD for counts, such as Poisson or NB"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/count_3_zeroinf.html#zero-part-same-for-both",
    "href": "dev/oldnotes_stat/lm_glm/count_3_zeroinf.html#zero-part-same-for-both",
    "title": "Excess 0",
    "section": "",
    "text": "binary process.\n\noff with probably \\(\\pi\\), only zero counts possible\non with probablity \\(1-\\pi\\)."
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/count_3_zeroinf.html#counting-part",
    "href": "dev/oldnotes_stat/lm_glm/count_3_zeroinf.html#counting-part",
    "title": "Excess 0",
    "section": "",
    "text": "Count need to be at least 1.\nUse zero-truncated PD\n\n\n\nConceptual example: first decide whether buy things: yes or no. If yes, can end up buying (positive counts), can also end up buying nothing (out of stock)\nUse an usual discrete PD for counts, such as Poisson or NB"
  },
  {
    "objectID": "dev/oldnotes_stat/multivariate/Correspondence_analysis.html",
    "href": "dev/oldnotes_stat/multivariate/Correspondence_analysis.html",
    "title": "Simple CA",
    "section": "",
    "text": "Used for categorical (mostly survey) data, to identify\n\na group of individuals with similar profiles in their answers\nassociation between variable categories\n\n\ncontingency table (counts)\nx_ij: number of individuals with category i of V1, and category j of V2.\nindependence events: P(A and B) = P(A) * P(B)\nindependence qualitative variables: \\(p_{ij} = p_{.i} * p_{.j}\\)\n\njoint probability = product of marginal probabilities\n\nequivalently, \\(\\frac{p_{ij}}{p_{.i}} = p_{.j}\\)\n\nconditional probability = marginal probability\n\nCA works with table of probabilities, but it is not a test. aims to visualise links between variables\nhttp://www.sthda.com/english/articles/22-principal-component-methods/67-correspondence-analysis-course-using-factominer/\nhttps://www.youtube.com/watch?v=Z5Lo1hvZ9fA\nhttp://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/114-mca-multiple-correspondence-analysis-in-r-essentials/\n\n\nSimple CA\n\nstep 0: create contingency table\nThis requires summarising the raw data into counts.\n\n\nstep 1: chi-sq test for independence\nexamine association between row and column\n# Chi-square statistics\nchi2 &lt;- 1944.456\n# Degree of freedom\ndf &lt;- (nrow(housetasks) - 1) * (ncol(housetasks) - 1)\n# P-value\npval &lt;- pchisq(chi2, df = df, lower.tail = FALSE)\npval\n\n\nstep 2: CA, check eigenvalue\nLargest eigenvalue corresponds to the axis that explains the most variance\n\n\n\nMultivariate CA"
  },
  {
    "objectID": "dev/oldnotes_stat/multivariate/Notes_factor_analysis.html",
    "href": "dev/oldnotes_stat/multivariate/Notes_factor_analysis.html",
    "title": "Exploratory factor analysis",
    "section": "",
    "text": "Exploratory factor analysis\ngoal: identify underrlying relationships between variables"
  },
  {
    "objectID": "dev/oldnotes_stat/multivariate/Notes_multivariate_analysis.html",
    "href": "dev/oldnotes_stat/multivariate/Notes_multivariate_analysis.html",
    "title": "covariance, correlation, distance",
    "section": "",
    "text": "covariance, correlation, distance\ncovariance: linear dependence of two rv\n\n\nPCA\nexploratory, not inferential\nintuition\n\nPC1: find a straightline to fit your data (maximise the variance in this direction; minimise the error with this line)\nPC2: find a straightline to fit the error from above\n\n\ntechnical details\ndescribe correlated X variables by uncorrelated Z variables, which are linear combinations of the Xs\nPC1 = \\(z_1 = a_{11}x_1 + a_{12} x2 + ... + a_{1q}x_q\\)\nSample variance is greatest among all such linear combinations, under the constraint that \\(a_1^Ta_1 = 1\\)\nSample variance of \\(z_1 = a_1^TSa_1\\) (a1 is the vector)\n\\(a_1\\) that satisfies the solution is the either vector of the sample covariance matrix S correponding to the largest eigenvalue.\n\\(\\sum_{i= 1}^{q} \\lambda_i = s_1^2 + s_2^2 + ... + s_q^2\\), equivalently \\(\\sum_{i= 1}^{q} \\lambda_i = trace(S)\\)\n\n\nExtracted on sample correlation matrix\nPCA is not scale-invariant. to be safe, use correlation matrix"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/book_ros_chpt9_pred_bayesinf.html",
    "href": "dev/oldnotes_stat/lm_glm/book_ros_chpt9_pred_bayesinf.html",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "Need to understand whether it is necessary to use bayesian prediction.\n\nrstanarm (different from arm)\nsection 1.6, 8.4, 9.5\nrstanarm::stan_glm: bayesian glm with optionnal prior distribution for the coefficients, intercept and auxilliary parameters\nFor a regression, the results are\n\nintercept\nslope\nresidual standard deviation (reported as auxiliary parameters)\n\nmedian, mad_sd (median absolute deviation)\n\nprediction (bayesian)\npoint prediction: \\(\\hat{a} + \\hat{b}x^{new}\\)\n\none point, no uncertainty\ne.g. point prediction is the best estimate of the average blood pressure in the population, conditional on dose \\(x^{new}\\)\n\nLinear predictor with uncertainty: \\(a + bx^{new}\\)\n\nContains inferential uncertainty in a, b\ndistribution of uncertainty of expected or average \\(y\\) for a new observation\nn -&gt; inf, uncertainty -&gt; 0 as a and b are more and more precise\n\npredictive distribution for a new observation: \\(a + bx^{new} + error\\)\n\nuncertainty about new obervation\nblood pressure of a single person drawn at random from this population\nn-&gt;inf, uncertainty -&gt; residual standard deviation sigma\n\n\n\npriors\nweekly informative prior (default)\n\nlinear model:\n\nbk ~N(0, 2.5*sd(y)/sd(xk))\na ~ N(mu_y, 2.5*sd(y))\nsigma ~ exp(1/sd(y))\n\n\ncheck prior: prior_summary()\n\n\nimplementation of arm::sim()\ngetMethod(arm::sim, 'lm') can provide how beta and sigma are simulated\nhttps://stats.stackexchange.com/questions/192996/why-does-the-sim-function-in-gelmans-arm-package-simulate-sigma-from-inverse-ch"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/count_1_poireg.html",
    "href": "dev/oldnotes_stat/lm_glm/count_1_poireg.html",
    "title": "Poisson regression",
    "section": "",
    "text": "Poisson regression\na very good resource: https://data.princeton.edu/wws509/notes/c4.pdf\nWhen count is large, can also use linear. (as Poisson distribution can be approximated for large \\(\\lambda\\), 20)\n\nhowever the concern is the variance, it is not constant: larger count has larger variance due to \\(\\mu\\)\n\n\nCAUTION: for poisson example, it’s NOT \\(log(y) = xb + e\\), it is \\(log(\\mu) = xb\\). Log in the mean!\n\nLog-linear regression (an inappropriate alternative)\n\n\\(log (y) = x'\\beta + u, u \\sim N(0, \\sigma^2)\\)\nmean function is scaled by \\(exp(0.5*\\sigma^2)\\), the expectation of error term (log linear)\nignores the discrete nature of data (y=0, 1, 2…)\n0 count inadmissible: log(0) not defined\ninference problem: standard error\nbias introduced: parameters estimated away from 0.\n\n\n\nInterpretation: additive on the log scale\n\\(exp(\\beta_1 x_1 + \\beta_2 x_2) = exp(\\beta_1 x_1) * exp(\\beta_2 x_2)\\)\nInterpreted as (the natural logarithm of) ratios\n\nincidence rate ratio\nrelative risk\n\nOne increase in \\(x_i\\) increases the log mean by \\(\\beta\\) units, \\(log(\\mu_i) = x_i \\beta\\)\nOne increase in \\(x_i\\) increases the mean by \\(exp(\\beta)\\) times, \\(mu_i = exp(x_i \\beta)\\)\ndispersion parameter \\(\\phi = 1\\)\n\n\nDeviance\ndifference: model deviance, residual devviance + degree of freedom\n\n\nInference\nUnderestimates the variance in the data, hence need to use more robust measures, such as sandwich covariates\nAnalytical SE\n\n\n\nQuestions\nhow to judge whether the prediction is good, given that the results are counts? with Logistic reg there are classification metrics, with linear reg there are MSE, how about counts?"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/GLM.html",
    "href": "dev/oldnotes_stat/lm_glm/GLM.html",
    "title": "Formulation",
    "section": "",
    "text": "\\(E(Y|X) = \\mu = g^{-1} (X\\beta)\\)\n\n\\(E(Y|X)\\) is the expected value of Y conditional on X\n\\(g\\) is the link function, from exponential family, need to be invertible\nmean function is for the outcome. e.g. \\(\\mu = g^{-1} =exp(X\\beta)\\) for poisson regression.\n\n\\(Var(Y|X) = V(\\mu) = V(g^{-1}(X\\beta))\\)\nCAUTION: for poisson example, it’s NOT \\(log(y) = xb + e\\), it is \\(log(\\mu) = xb\\)\nwhere is the error term\n\n\n\\(f(x|\\theta) = h(x)*exp[\\eta(\\theta)T(x) - A(\\theta)]\\)\n\nT(x) is the sufficient statistic\n\\(\\eta(x)\\) is the natural parameter\n\\(h(x)\\) is the base\n\\(A(\\theta)\\) is the cumulant generating function\n\nEF is used for lower variance of unbiased estimator (??)\nA sufficient statistic for the parameter exists\n\\(A(\\eta(\\theta))\\) can be used to derive moments (mean, variance) by taking first and second derivative\n\n\n\n\nLogit model (logistic regression):\\(\\eta(\\theta) = log(\\frac{p}{1-p})\\), it is the natural parameter of bernoulli distribution\nPoisson distribution, \\(\\eta(\\theta) = log(\\lambda)\\)\n\nNote that other distributions can have natural parameters, but do not correspond directly to GLM. such as normal.\n\n\n\nLogistic reg has logit link \\(log(\\frac{p}{1-p})\\). Only predicts 1 and 0\nBinomial regression has logit link \\(log(\\frac{p}{n-p})\\), predicts number of success in n trials, 0, 1, .. N.\n\n\n\nPredict the values of the conditional mean \\(E(Y|X) = g^{-1}(X\\beta)\\) through the linear predictor.\n\nfitted(model) is equivalent to predict(model, type = 'response'), returns \\(g^{-1}(X\\hat\\beta)\\) , the conditional response\npredict(model, type = 'link') returns \\(X\\hat{\\beta}\\)\n\nCI need to be extracted manually; or use the other function confint\n\n\n\n?? more predictors, larger variance, less power??\nThe response is heterskedastic, unlike linear case.\n\n\n\n\n\ndifference of log-likelihod between fitted model and saturated model. it is a generalisation of residual sum of squares RSS (i.e. sum of squared error SSE)\nResidual deviance: \\(D := -2[loglik(\\hat\\beta) - loglik_s]\\phi\\)\nNull deviance: \\(D_0 := -2[loglik(\\hat\\beta_0) - loglik_s]\\phi\\), the intercept model and saturated model. It iis a generalisation of total sum of squares, SST = sum(yi - mean(y)^2)\n\n\n\ndeviance\n\n\nThe deviance is approximately chi-square distributed with \\(n-p\\) degrees of freedom.\n\nNull deviance will be n-1, for the intercept"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/GLM.html#prediction",
    "href": "dev/oldnotes_stat/lm_glm/GLM.html#prediction",
    "title": "Formulation",
    "section": "",
    "text": "Predict the values of the conditional mean \\(E(Y|X) = g^{-1}(X\\beta)\\) through the linear predictor.\n\nfitted(model) is equivalent to predict(model, type = 'response'), returns \\(g^{-1}(X\\hat\\beta)\\) , the conditional response\npredict(model, type = 'link') returns \\(X\\hat{\\beta}\\)\n\nCI need to be extracted manually; or use the other function confint"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/GLM.html#inference",
    "href": "dev/oldnotes_stat/lm_glm/GLM.html#inference",
    "title": "Formulation",
    "section": "",
    "text": "?? more predictors, larger variance, less power??\nThe response is heterskedastic, unlike linear case."
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/GLM.html#diagnostics-and-selection",
    "href": "dev/oldnotes_stat/lm_glm/GLM.html#diagnostics-and-selection",
    "title": "Formulation",
    "section": "",
    "text": "difference of log-likelihod between fitted model and saturated model. it is a generalisation of residual sum of squares RSS (i.e. sum of squared error SSE)\nResidual deviance: \\(D := -2[loglik(\\hat\\beta) - loglik_s]\\phi\\)\nNull deviance: \\(D_0 := -2[loglik(\\hat\\beta_0) - loglik_s]\\phi\\), the intercept model and saturated model. It iis a generalisation of total sum of squares, SST = sum(yi - mean(y)^2)\n\n\n\ndeviance\n\n\nThe deviance is approximately chi-square distributed with \\(n-p\\) degrees of freedom.\n\nNull deviance will be n-1, for the intercept"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/book_ros_chpt15_glm.html",
    "href": "dev/oldnotes_stat/lm_glm/book_ros_chpt15_glm.html",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "glm in the bayesian framework\n\n\nglm components\n\nvector of outcome data y\nmatrix of predictors X, vector of coefficients \\(\\beta\\) forming a linear predictor vector \\(X\\beta\\)\nlink function \\(g\\), giving transformed data \\(\\hat{y} = g^{-1}(X\\beta)\\)\ndata distribution \\(y|\\hat{y}\\)\nother parameters\n\n\n\nexamples\n\nLinear regression, \\(g(u) = u\\), data distribution is normal with sd \\(\\sigma\\)\nlogistic regression, \\(g^{-1}(u) = logit^{-1}(u)\\)\n\nLogit (log odds) link, \\(g(p) = ln(p/1-p)\\)\nLogit(y|x) = xb\ndata distribution defined by probability, \\(p(y=1) = \\hat{y}\\)\n\nPoisson, nb\n\nlog link, log(y|x) = xb\nLog(lambda_i) = b0 + b1x1i + b2x2i + …\n\\(y_i\\) ~ \\(poisson(exp^{x_i\\beta})\\)\nequal mean and variance conditional on x: larger mean, larger variance (more sparse)\n\nlogistic-binomial (also a count)\n\ny is number of successes in n_i trials\ndependent on a limit (total trials)\nuse poisson/nb when the limit is much larger (e.g. population)"
  },
  {
    "objectID": "dev/oldnotes_stat/mixed_effect/Notes_mixed_models.html",
    "href": "dev/oldnotes_stat/mixed_effect/Notes_mixed_models.html",
    "title": "Mixed effects models",
    "section": "",
    "text": "Mixed effects models\nhow is it different from multilevel modeling\nhow is it different from longitudinal analysis\nhierarchical: e.g. repeated observations nested within students, students nested within schools, schools nested in districts\n\n\nrepeated measurement (e.g. longitudinal)\ngpa example\nModel 1: linear regression\n\ngpa = \\(\\beta_0\\) + \\(\\beta\\) * occasion + \\(\\epsilon\\)\n\nModel 2: random intercept\n\nOne intercept for one student (student effect)\ngpa = \\(\\beta_0\\) + effect_i + \\(\\beta\\) * occasion + \\(\\epsilon\\)\ngpa = \\(\\beta_{0,i}\\) + \\(\\beta\\) * occasion + \\(\\epsilon\\) , where \\(\\beta_{0,i} ~ N(\\beta_0, \\tau)\\)\nrandom intercept is distributed around overall intercept\n\n?? variance output for the randomeffect \n?? correlation with fixed effect? \n\n\ncross effect and nested effect\nnot sure how the former is different"
  },
  {
    "objectID": "dev/oldnotes_programming/database/Database-psql101.html",
    "href": "dev/oldnotes_programming/database/Database-psql101.html",
    "title": "Database",
    "section": "",
    "text": "Relational database management systems. Relational model shapes whatever information to be stored by defining them as related entities with attributes across tables (i.e. schemas)\n\n\ndisadvantages:\nThe way certain functionality gets handled with MySQL (e.g. references, transactions, auditing etc.) renders it a little-less reliable compared to some other RDBMSs.\nSince MySQL does not [try to] implement the full SQL standard, this tool is not completely SQL compliant. If you might need integration with such RDBMSs, switching from MySQL will not be easy.\n\n\n\nCompared to other RDBMSs, PostgreSQL differs itself with its support for highly required and integral object-oriented and/or relational database functionality, such as the complete support for reliable transactions, i.e. Atomicity, Consistency, Isolation, Durability (ACID)."
  },
  {
    "objectID": "dev/oldnotes_programming/database/Database-psql101.html#installation",
    "href": "dev/oldnotes_programming/database/Database-psql101.html#installation",
    "title": "Database",
    "section": "installation",
    "text": "installation\nhttps://www.postgresql.org/download/macosx/\n# install homebrew\n\nbrew install postgresql\nbrew services stop postgresql  \nbrew services start postgresql  # necessary\ncheck version\npostgres -V  \n\ninstall a graphical user interface\nI choose postico"
  },
  {
    "objectID": "dev/oldnotes_programming/database/Database-psql101.html#getting-started",
    "href": "dev/oldnotes_programming/database/Database-psql101.html#getting-started",
    "title": "Database",
    "section": "Getting started",
    "text": "Getting started\nandrea$ psql postgres\n\n1. Create user\ncreate with psql (CREATE ROLE) and give it permission ALTER ROLE.\npostgres=# CREATE ROLE chizhang WITH LOGIN PASSWORD 'mypassword'; (andrea is my password)\npostgres=# ALTER ROLE chizhang CREATEDB;\nboth show some information of the database\npostgres=# \\du\npostgres=&gt; \\list\nto quit\npostgres=# \\q \n\n\n2. connect to a default database\nchange user, then create database. The prompt &gt; indicate now it’s not a super user account (andrea).\nandrea$ psql postgres -U chizhang\nCREATE DATABASE mimicdata;\npostgres=&gt; GRANT ALL PRIVILEGES ON DATABASE mimicdata TO chizhang;\n\n\n-------- updated 12.2\n\nCREATE DATABASE demo;\npostgres=&gt; GRANT ALL PRIVILEGES ON DATABASE demo TO chizhang;\npostgres=&gt; \\connect demo \ndemo=&gt; \\dt \n\n-------- updated 19.1.10\n\n# in postico, let user chizhang connect to database demo. \n\\dt lists the tables in currently connected db\n\n\n3. create and drop table\n\nwith GUI postico - create table\nIn postico,\nCREATE TABLE fav_sports3 (\n\n   name char(20),\n   age integer,\n   sport char(20),\n   gender char(20)\n);\nColumns must be consistent with the csv file.\n\n\nOther operations related to table\nto drop table,\nDROP TABLE tablename\nchange table name,\nALTER TABLE table_name RENAME TO new_name;\n\n\n\n4. import csv (or gz)\n\nIssue with importing csv privilege\nIn postico, I need to be a superuser to COPY data from csv. The database demo is owned by user chizhang, which is not a superuser.\n\nSolution 1: Use command line\npostgres=# \\connect demo\ndemo=# \\copy fav_sports3 FROM '/Users/andrea/Documents/PhdProjects/Project-Paper2/Database/trialdata.csv' DELIMITER ',' CSV HEADER;\nNote the difference \\copy. But my code are all written using COPY as SQL. This method doesn’t work for a lot of files. At least I don’t know a fast way.\n\n\nSolution 2: change user privilege\npostgres=# ALTER USER chizhang WITH SUPERUSER;\n\\du\nNow user chizhang has the superuser privilege so can copy csv files using the ready SQL scripts.\nCOPY fav_sports3 FROM '/Users/andrea/Documents/PhdProjects/Project-Paper2/Database/trialdata.csv' DELIMITER ',' CSV HEADER;\nAfter finished, change back.\npostgres=# ALTER USER chizhang WITH NOSUPERUSER;\n\n\nSolution 3: import via postico\nBut need to create table first.\nthen in Postico, do the usual SQL stuff.\nSELECT * FROM fav_sports;\n\n\n\nimport from gz\nThis is an example on how to import from gzip.\n\\copy ADMISSIONS FROM PROGRAM 'gzip -dc ADMISSIONS.csv.gz' DELIMITER ',' CSV HEADER NULL ''\nCOPY FROM PROGRAM\n\ngzip\na lossless data compression, the result usually has suffix .gz. Usage: gzip [OPTION]... [FILE]...\n-d : decompress\n-c: standard output, keep original files unchanged\n\n\n\n\n5. Data type\nWhen creating a table, it is necessary to specify the data types.\nDROP TABLE IF EXISTS ADMISSIONS CASCADE;  -- and all other objects that depends on it\nCREATE TABLE ADMISSIONS\n(\n  ROW_ID INT NOT NULL,   -- not null constraint enforces a column must not accept NULL values\n  SUBJECT_ID INT NOT NULL,\n  HADM_ID INT NOT NULL,\n  ADMITTIME TIMESTAMP(0) NOT NULL,   -- 0 is precision\n  DISCHTIME TIMESTAMP(0) NOT NULL,\n  DEATHTIME TIMESTAMP(0),\n  ADMISSION_TYPE VARCHAR(50) NOT NULL,    -- character varying = variable-length with limit, stores up to n characters\n  ADMISSION_LOCATION VARCHAR(50) NOT NULL,\n  DISCHARGE_LOCATION VARCHAR(50) NOT NULL,\n  INSURANCE VARCHAR(255) NOT NULL,\n  LANGUAGE VARCHAR(10),\n  RELIGION VARCHAR(50),\n  MARITAL_STATUS VARCHAR(50),\n  ETHNICITY VARCHAR(200) NOT NULL,\n  EDREGTIME TIMESTAMP(0),\n  EDOUTTIME TIMESTAMP(0),\n  DIAGNOSIS VARCHAR(255),\n  HOSPITAL_EXPIRE_FLAG SMALLINT,  -- numeric type, +-32768\n  HAS_CHARTEVENTS_DATA SMALLINT NOT NULL,\n  CONSTRAINT adm_rowid_pk PRIMARY KEY (ROW_ID),  -- == unique not null\n  CONSTRAINT adm_hadm_unique UNIQUE (HADM_ID)  -- unique constraint for all rows\n) ;\nDate and time needs special attention.\nSET datestyle = dmy;\nCOPY ADMISSIONS FROM '/Users/andrea/Desktop/Database/DataDemo/ADMISSIONS.csv' DELIMITER ',' CSV HEADER NULL '';\n\nSELECT * FROM ADMISSIONS;\n‘du’"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-tidyeval.html",
    "href": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-tidyeval.html",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "Tidy evaluation, one of the metaprogramming techniques\n\nA special type of non-standard evaluation (metaprogramming)\nThe dataframe itself becomes a temporary workspace (meaning that can access column using col instead of df$col)\n\ndata masking\nwrite my_variable not df$my_variable\n\n\ntidy selection\nchoose variables easily based on position, name, type, such as start_with('x')\n\n\nvectorisation\na function is vectorised when it returns a vector as long as the input, and when applying the function on a scalar is the same as doing it on the vector.\nfn(x[[i]]) == fn(x)[[i]]"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/R cpp notes.html",
    "href": "dev/oldnotes_programming/r_advanced_r_book/R cpp notes.html",
    "title": "R cpp notes",
    "section": "",
    "text": "run an argument many times, then print out the summary\n\n\n\ncheck storage mode: storage.mode(x)\nconvert an integer to a double by adding .0 after the number. can also use casting to change data types:\nevalCpp('(double)(40+2)')\n\n\n\nneed to define the type of input and return object\ndouble functionName(double x, double y){\n    \n    double res = sqrt(x*x + y*y); // body of the function\n  return res;\n}\n\n\n\nRprintf() prints formatted output. inside the message, can only use double quotes\n\n\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n//[[Rcpp::export]]\nint timesTwo(inx){\n  return 2*x ;\n}\nLoad into R\nlibrary(Rcpp)\nsourceCpp('code.cpp')\n\n# then call it like any other R function \ntimesTwo(21)\n\n\nif(condition){\n//\n}else{\n//\n}\n\n\n\nfor(init; condition; increment){\n    body\n}\ntypically (note i starts from 0)\nfor (init i = 0; i&lt;n; i++){\n\n}"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/R cpp notes.html#scripting",
    "href": "dev/oldnotes_programming/r_advanced_r_book/R cpp notes.html#scripting",
    "title": "R cpp notes",
    "section": "",
    "text": "#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n//[[Rcpp::export]]\nint timesTwo(inx){\n  return 2*x ;\n}\nLoad into R\nlibrary(Rcpp)\nsourceCpp('code.cpp')\n\n# then call it like any other R function \ntimesTwo(21)\n\n\nif(condition){\n//\n}else{\n//\n}\n\n\n\nfor(init; condition; increment){\n    body\n}\ntypically (note i starts from 0)\nfor (init i = 0; i&lt;n; i++){\n\n}"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt2-fp.html",
    "href": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt2-fp.html",
    "title": "Functional",
    "section": "",
    "text": "Functional programming in R\n\nFunctional programming is complementatry to OOP\n\nfunctional language have first-class functions. functions behave like other data structure; assign to variables, store in lists, pass as argument to other functions, create inside functions, return as the result of a function.\nrequires pure functions: same input, same output. This means functions such as runif(), Sys.time() are not pure functions.\n\n\nFunctional\nA function that takes a function as an input, and returns a vector as output.\n\nMap\nI already know how to use it\n\n\nReduce, accumulate\nreduce is like f(f(f(x))).\nlst &lt;- map(1:4, ~ sample(1:10, 15, replace = T))\nlst\n# find the values that occur in every element \nz &lt;- lst[[1]]\nz &lt;- intersect(z, lst[[2]])\nz &lt;- intersect(z, lst[[3]])\nz &lt;- intersect(z, lst[[4]])\nz\n# can use: \nreduce(lst, intersect)\n\n# accumulate shows all the intermediate results \naccumulate(lst, intersect)\n\n\n\nFunction operator\nA function that takes a function as input, and returns a function as output.\nCan be thought of as a wrapper. I think it can provide alternative ways to deal with error, instead of tryCatch().\nUseful way to debug when using map suite: purrr::safely(function) and purrr::transpose(list)\nx &lt;- list(c(0.1, 0.1, 0.2),\n          c(0.3, 0.4, 0.5), \n          c(1, 0.2, 0.2), \n          'error')\n\n# use a FUNCTIONAL\nmap_dbl(x, sum)  # error will prevent any results from printing \n\n# use safely(), a FUNCTION OPERATOR with a functional \n# map(x, safe_sum). alternatively, \nout2 &lt;- map(x, safely(sum))\nmap(out2, pluck(1))   # results\nmap(out2, pluck(2))   # errors\n\n\n# inconvenient way of printing: use purrr::transpose \n# equivalent to transpose(out2)\nout3 &lt;- transpose(map(x, safely(sum)))\nout3$result\nout3$error\nWith purrr::possibly(function), it replaces error into a value\nl &lt;- list('a', 10, 100)\nmap_dbl(l, possibly(log, NA))  # need 'otherwise' argument"
  },
  {
    "objectID": "dev/oldnotes_programming/unix/Bash scripting.html",
    "href": "dev/oldnotes_programming/unix/Bash scripting.html",
    "title": "Bash scripting",
    "section": "",
    "text": "the content of headers.sh is head -n 1 seasonal/*.csv\nRun by\nbash headers.sh\nwith pipe: cut -d , -f 1 seasonal/*.csv | grep -v Date | sort | uniq is the all-dates.sh\nbash all-dates.sh &gt; dates.out   # save in the output \n\n\n$@ : all the command-line parameters given to the script\neg. sort $@ | uniq is the unique-lines.sh. then the following will process 2 files, replacing $@.\nbash unique-lines.sh seasonal/summer.csv seasonal/autumn.csv\n\n\n\n# Print the first and last data records of each file.\n# no semi colon\n\nfor filename in $@\ndo\n    head -n 2 $filename | tail -n 1\n    tail -n 1 $filename\ndone\n\n\n\nAn example script: script1.sh\n#!/bin/bash\n# a simple bash script \n\necho Hello World! \nLine 1: shebang\nfollowed by path to the interpreter to run. no space, always first line.\nLine 2: comment\nLine 3: command\n\n\nandrea$ ./script1.sh\n-bash: ./script1.sh: Permission denied  # no space in between \n# dot is the current dir, otherwise have to specify $PATH. \n# equivalent to \nandrea$ bash script1.sh\n\nandrea$ ls -l script1.sh\n-rw-r--r--@ 1 andrea  staff  55 Apr 24 13:23 script1.sh\n\n# allow owner to write and modify, everyone to execute\nandrea$ chmod 755 script1.sh    # change access permission\nandrea$ ls -l script1.sh\n-rwxr-xr-x@ 1 andrea  staff  55 Apr 24 13:23 script1.sh\n\n\n\nrefering to / reading a variable: put a $ before variable name\nsetting a variable, no $. For example, andrea$ HOSTNAME will give host name\nSeems like I can only use $1, $2 but not $A, or $arg1. This is because these are the special variables.\n$0 # name of Bash script \n$1-9 # first 9 arguments in Bash script \n$@ # all arguments \n$USER # username \n$HOSTNAME # hostname of the machine the script is running on \nexample see script2.sh\n\n\nsee script3.sh.\n\n\n\nsingle quotes does not allow substition of variables, it treats characters literally: when use a $ it is printed exactly like it is.\nvar1='hello world'\n\nvar2='see $var1' \necho $var2  # will give see $var1\nvar3=\"see $var1\" \necho $var3 # will give see hello world\n\n\n\nvar=$(ls | wc -l)  # count how many elements\necho There are $var entries in this directory  \n\n\n\nsee scripts 4, 4-2.\n\n\n\n\n(script5.sh) use the command read and save the user response into the variable varname\n\n\n\n(script6.sh)\nlet and expr.\n\n\n\n\nif [&lt;some test&gt;]  # if_[_cmd_]  the space matters!\nthen\n    &lt;commands&gt;\nfi\nwhen checking,\ntest 001 = 1  # false, string comparison\necho $?\n\ntest 001 -eq 1  # true, numerical comparison\necho $?\nNote that $? returns the last run process (exit status).\n0 means 0 error (true), other values indicate some unusual condition (e.g. 1 means false in the above situation)\n\n\nif [&lt;some test&gt;]  \nthen\n    &lt;commands&gt;\nelif [&lt;some test&gt;]  \nthen \n    &lt;different commands&gt;\nelse \n    &lt;other commands&gt;\nfi\n\n\n\nwhile [&lt;some test&gt;]  \ndo\n    &lt;commands&gt;\ndone\n\n\n\nvery similar to while.\nuntil [&lt;some test&gt;]  \ndo\n    &lt;commands&gt;\ndone\n\n\n\nfunction_name(){\n    &lt;commands&gt;\n}\n\n# or \n\nfunction function_name {\n    &lt;commands&gt;\n}"
  },
  {
    "objectID": "dev/oldnotes_programming/unix/Bash scripting.html#another-tutorial",
    "href": "dev/oldnotes_programming/unix/Bash scripting.html#another-tutorial",
    "title": "Bash scripting",
    "section": "",
    "text": "An example script: script1.sh\n#!/bin/bash\n# a simple bash script \n\necho Hello World! \nLine 1: shebang\nfollowed by path to the interpreter to run. no space, always first line.\nLine 2: comment\nLine 3: command\n\n\nandrea$ ./script1.sh\n-bash: ./script1.sh: Permission denied  # no space in between \n# dot is the current dir, otherwise have to specify $PATH. \n# equivalent to \nandrea$ bash script1.sh\n\nandrea$ ls -l script1.sh\n-rw-r--r--@ 1 andrea  staff  55 Apr 24 13:23 script1.sh\n\n# allow owner to write and modify, everyone to execute\nandrea$ chmod 755 script1.sh    # change access permission\nandrea$ ls -l script1.sh\n-rwxr-xr-x@ 1 andrea  staff  55 Apr 24 13:23 script1.sh\n\n\n\nrefering to / reading a variable: put a $ before variable name\nsetting a variable, no $. For example, andrea$ HOSTNAME will give host name\nSeems like I can only use $1, $2 but not $A, or $arg1. This is because these are the special variables.\n$0 # name of Bash script \n$1-9 # first 9 arguments in Bash script \n$@ # all arguments \n$USER # username \n$HOSTNAME # hostname of the machine the script is running on \nexample see script2.sh\n\n\nsee script3.sh.\n\n\n\nsingle quotes does not allow substition of variables, it treats characters literally: when use a $ it is printed exactly like it is.\nvar1='hello world'\n\nvar2='see $var1' \necho $var2  # will give see $var1\nvar3=\"see $var1\" \necho $var3 # will give see hello world\n\n\n\nvar=$(ls | wc -l)  # count how many elements\necho There are $var entries in this directory  \n\n\n\nsee scripts 4, 4-2.\n\n\n\n\n(script5.sh) use the command read and save the user response into the variable varname\n\n\n\n(script6.sh)\nlet and expr."
  },
  {
    "objectID": "dev/oldnotes_programming/unix/Bash scripting.html#if-statements-loops-functions",
    "href": "dev/oldnotes_programming/unix/Bash scripting.html#if-statements-loops-functions",
    "title": "Bash scripting",
    "section": "",
    "text": "if [&lt;some test&gt;]  # if_[_cmd_]  the space matters!\nthen\n    &lt;commands&gt;\nfi\nwhen checking,\ntest 001 = 1  # false, string comparison\necho $?\n\ntest 001 -eq 1  # true, numerical comparison\necho $?\nNote that $? returns the last run process (exit status).\n0 means 0 error (true), other values indicate some unusual condition (e.g. 1 means false in the above situation)\n\n\nif [&lt;some test&gt;]  \nthen\n    &lt;commands&gt;\nelif [&lt;some test&gt;]  \nthen \n    &lt;different commands&gt;\nelse \n    &lt;other commands&gt;\nfi\n\n\n\nwhile [&lt;some test&gt;]  \ndo\n    &lt;commands&gt;\ndone\n\n\n\nvery similar to while.\nuntil [&lt;some test&gt;]  \ndo\n    &lt;commands&gt;\ndone\n\n\n\nfunction_name(){\n    &lt;commands&gt;\n}\n\n# or \n\nfunction function_name {\n    &lt;commands&gt;\n}"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/project_template.html",
    "href": "dev/oldnotes_programming/rstudio_extension/project_template.html",
    "title": "Parts needed",
    "section": "",
    "text": "template function\ntemplate metadata\n\n\n\nOnly need one function\n\n\n\nthis file defines what’s shown on the project wizard\n\n\nBinding: which function to generate the project\nTitle: title in thhe project wizard\nOpenFiles:"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/project_template.html#template-function",
    "href": "dev/oldnotes_programming/rstudio_extension/project_template.html#template-function",
    "title": "Parts needed",
    "section": "",
    "text": "Only need one function"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/project_template.html#template-metadata-.dcf",
    "href": "dev/oldnotes_programming/rstudio_extension/project_template.html#template-metadata-.dcf",
    "title": "Parts needed",
    "section": "",
    "text": "this file defines what’s shown on the project wizard\n\n\nBinding: which function to generate the project\nTitle: title in thhe project wizard\nOpenFiles:"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git for Version Control.html",
    "href": "dev/oldnotes_programming/git/Git for Version Control.html",
    "title": "Git for Version Control",
    "section": "",
    "text": "workflow http://marklodato.github.io/visual-git-guide/index-en.html\nhttps://swcarpentry.github.io/git-novice/02-setup/index.html\ngit --version check version\nmkdir planets\ncd planets\ngit init\ngit config user.name 'Vlad Dracula' \nls -a  # list all \nls -la # list long format\n\ncat .git/config  # shows name Vlad Dracula\ngit config core.editor 'nano -w'  # set up editor, can also use atom\ngit status # should be nothing there\n\n# -- inside .git folder to change user name etc\ncd .git\ncat config\ngit config user.name 'name1'\n\n\nremember to set the global username and email! otherwise your contributions are not counted.\n$ git config --global user.name 'andrea'\n$ git config --global user.email 'andreachizhang@yahoo.com'\nnano mars.txt # create some content\ncat mars.txt # display \ngit status # check status again\n\ngit add mars.txt\ngit status # check status again\ngit commit -m 'satr notes on Mars as a base'  # message\ngit log  # display log \n\n# change someting in mars, then check status again\n\ngit diff # check difference \n\ngit add mars.txt\ngit commit -m 'add concerns about effects of Mars moon'\n\ngit add mars.txt\ngit diff  # compares working area with staging area, disappears after git add, before commit\ngit diff --staged  # comparing staging area with repositories\ngit diff HEAD # working area vs repositories \nif I have a separate file called mars2.txt, and I do git diff, nothing will happen until I do git add.\nIt’s possible that changes in both files are displayed at the same time, can be seen using git status and git diff, and git add . will update both.\nIf I happen to have deleted something accidently\ngit checkout 8dcecd668a9ae311aae0512ebe97f501595fe16b mars.txt\ngit checkout master\ntouch a.input b.input\nls\nnano .gitignore # and put a.input\ngit status # a.input will be ignored \nssh doesn’t need user name and pass, html needs\npwd  # should be planets repo (local), /Users/andrea/Desktop/planets\ngit remote -v # check if any server is connected\ngit remote add origin https://github.com/yymmhaha/planets.git  # origin is the name for https + blahblah, easier for later so we don't need to type again\ngit push origin master  # sometimes it asks for user and pass \n\ngit log --oneline\ngit diff c7c2eea..f88029b mars2.txt\ncollaborate, clone other’s repo to my desktop but another folder\ngit clone https://github.com/tinavisnovska/planets.git ~/Desktop/tina-planets\ngit remote -v  # now I should see my collaborator's name \n# change something then, add and commit \ngit push origin master\n\n# ------- pull to my own \ncd ~/Desktop/planets\ngit pull origin master\natom --wait mars.txt  # use atom to open \n\n\n\ncd Documents/Programming/R/Project-git\ngit init\ngit remote add origin https://github.com/yymmhaha/Paper.git  # unneeded if already \ngit pull origin master --allow-unrelated-histories\ngit rm \"Worklog - Thoughts and undone's.md\"  # remove something - has to use git before!! \ngit commit -m 'deleted thoughts and undones since unnecessary'\n\n\nProgress andrea$ git add 'Part 4- intervention-practicals.md'\nProgress andrea$ git commit -m 'added AB testing'\nProgress andrea$ git push\nwhen there is a conflict asking me to enter commit message to explain merge:\n# in vim\ni\n# write merge message \nesc\n:wq\n# then enter\nto be more specific, ESC and colon : makes the cursor go to bottom. w is write, q is quit.\n\n\n\nbefore committing, can use .gitignore to specify files unneeded. Do it before add anything. Add these lines in the file to ignore the hidden directories.\n.*\n!/.gitignore\nunstaged, use\n$ git checkout -- &lt;file&gt;\n$ git checkout '*.DS_Store'\nto discard changes in working directory.\nor\n$ git reset HEAD\n\n\n\n$ wget --no-check-certificate --content-disposition https://github.com/MIT-LCP/mimic-code/blob/master/concepts/sepsis/angus.sql\nor\n$ curl -LJO https://github.com/MIT-LCP/mimic-code/blob/master/concepts/sepsis/angus.sql\n\n\n\nWhat is a pull request?\nIt lets you tell others about changes you’ve pushed.\nFork\nEdit\nthen pull request\n\n\n\ngit checkout is switching between branches in a repo you have, for example if you’re on master branch and want to go to develop_branch, git checkout develop_branch.\ngit clone is fetching repo you don’t have\n\n\n\nFor the project, I have re-set up the github account on the pink computer. Then, go to the parent directory (‘Work’) and do\ngit clone ### then add the .git file that appears \nAfter any changes made in the pink computer, push as usual.\nIn this computer, git pull. This will make sure the changes are synced.\n\n\n(on the main computer) first try git pull\nIt will say there are some files that are untracked, and modified locally.\nModified locally: force over write (i.e. ignore all local changes) by\ngit reset --hard\ngit pull\nUntracked:\ngit clean -i\nc # for clean\nIn the end pull again."
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git for Version Control.html#quick-guide-for-me",
    "href": "dev/oldnotes_programming/git/Git for Version Control.html#quick-guide-for-me",
    "title": "Git for Version Control",
    "section": "",
    "text": "cd Documents/Programming/R/Project-git\ngit init\ngit remote add origin https://github.com/yymmhaha/Paper.git  # unneeded if already \ngit pull origin master --allow-unrelated-histories\ngit rm \"Worklog - Thoughts and undone's.md\"  # remove something - has to use git before!! \ngit commit -m 'deleted thoughts and undones since unnecessary'\n\n\nProgress andrea$ git add 'Part 4- intervention-practicals.md'\nProgress andrea$ git commit -m 'added AB testing'\nProgress andrea$ git push\nwhen there is a conflict asking me to enter commit message to explain merge:\n# in vim\ni\n# write merge message \nesc\n:wq\n# then enter\nto be more specific, ESC and colon : makes the cursor go to bottom. w is write, q is quit."
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git for Version Control.html#ignore",
    "href": "dev/oldnotes_programming/git/Git for Version Control.html#ignore",
    "title": "Git for Version Control",
    "section": "",
    "text": "before committing, can use .gitignore to specify files unneeded. Do it before add anything. Add these lines in the file to ignore the hidden directories.\n.*\n!/.gitignore\nunstaged, use\n$ git checkout -- &lt;file&gt;\n$ git checkout '*.DS_Store'\nto discard changes in working directory.\nor\n$ git reset HEAD"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git for Version Control.html#download-online-files-from-github",
    "href": "dev/oldnotes_programming/git/Git for Version Control.html#download-online-files-from-github",
    "title": "Git for Version Control",
    "section": "",
    "text": "$ wget --no-check-certificate --content-disposition https://github.com/MIT-LCP/mimic-code/blob/master/concepts/sepsis/angus.sql\nor\n$ curl -LJO https://github.com/MIT-LCP/mimic-code/blob/master/concepts/sepsis/angus.sql"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git for Version Control.html#fork-and-pull-request",
    "href": "dev/oldnotes_programming/git/Git for Version Control.html#fork-and-pull-request",
    "title": "Git for Version Control",
    "section": "",
    "text": "What is a pull request?\nIt lets you tell others about changes you’ve pushed.\nFork\nEdit\nthen pull request"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git for Version Control.html#checkout",
    "href": "dev/oldnotes_programming/git/Git for Version Control.html#checkout",
    "title": "Git for Version Control",
    "section": "",
    "text": "git checkout is switching between branches in a repo you have, for example if you’re on master branch and want to go to develop_branch, git checkout develop_branch.\ngit clone is fetching repo you don’t have"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git for Version Control.html#using-another-computer",
    "href": "dev/oldnotes_programming/git/Git for Version Control.html#using-another-computer",
    "title": "Git for Version Control",
    "section": "",
    "text": "For the project, I have re-set up the github account on the pink computer. Then, go to the parent directory (‘Work’) and do\ngit clone ### then add the .git file that appears \nAfter any changes made in the pink computer, push as usual.\nIn this computer, git pull. This will make sure the changes are synced.\n\n\n(on the main computer) first try git pull\nIt will say there are some files that are untracked, and modified locally.\nModified locally: force over write (i.e. ignore all local changes) by\ngit reset --hard\ngit pull\nUntracked:\ngit clean -i\nc # for clean\nIn the end pull again."
  },
  {
    "objectID": "models/mixed_models.html#overview",
    "href": "models/mixed_models.html#overview",
    "title": "Mixed models for repeted measurements",
    "section": "",
    "text": "MMRM has one distinct feature compared to other linear mixed models: subject-specific random effects are considered as residual effects (part of error correlation matrix)."
  },
  {
    "objectID": "programming/misc_software_papers.html",
    "href": "programming/misc_software_papers.html",
    "title": "Software papers",
    "section": "",
    "text": "Before writing a software paper, one needs to create a worthy software."
  },
  {
    "objectID": "programming/misc_software_papers.html#select-a-journal",
    "href": "programming/misc_software_papers.html#select-a-journal",
    "title": "Software papers",
    "section": "Select a journal",
    "text": "Select a journal\n\nR journal\nJournal of Open Source Software\n\n\nR journal\nTo create a template, use rjtools::create_article(). This compiles both a pdf and html.\nThe article should have the following folders\n\ndata\nfigures\nscripts\nmotivation-letter\n\n\nTroubleshooting\nWhen the error is related to missing tex package:\n\nin terminal, tlmgr install texpkgname"
  },
  {
    "objectID": "programming/r_pkg_dependency.html",
    "href": "programming/r_pkg_dependency.html",
    "title": "Dependencies",
    "section": "",
    "text": "Resources:"
  },
  {
    "objectID": "programming/r_pkg_dependency.html#overview-namespace-and-import",
    "href": "programming/r_pkg_dependency.html#overview-namespace-and-import",
    "title": "Dependencies",
    "section": "Overview: NAMESPACE and import",
    "text": "Overview: NAMESPACE and import\n\nError: could not find function xyz\nWhere the function xyz is from a dependency package, such as ggplot2.\n\ncould not find function “ggplot”\n\nThis happens when dependency package such as ggplot2 is not imported. When checking NAMESPACE, you see that this package is not in the import list.\nTo tackle this, locate the function where the error occurs, add the Roxygen2 tags as appropriate:\n\n#' @importFrom aaapkg aaa_fun\n#' @import bbbpkg\n#' @export\nmy_function &lt;- function(x,y){\n  ...\n}\n\nThen document (either devtools::document() or Build -&gt; Document button). Check NAMESPACE again, see if the package is imported."
  },
  {
    "objectID": "programming/r_pkg_dependency.html#packages-in-imports-and-depends",
    "href": "programming/r_pkg_dependency.html#packages-in-imports-and-depends",
    "title": "Dependencies",
    "section": "Packages in Imports and Depends",
    "text": "Packages in Imports and Depends\nInside DESCRIPTION, there are three categories of dependencies: imports, suggests and depends. For now focus on imports and depends.\nGenerally, three ways to call a function in a dependency:\n\nuse deppkg::function();\nuse Roxygen2 tag @importFrom deppkg function, then call this particular function directly;\nuse Roxygen2 tag @import deppkg and call anything directly.\n\n\nImports\nPackages are required to make functions run, but are NOT loaded when loading your own package.\nWhen a pkg is listed in DESCRIPTION under Imports: recommended to call with deppkg::function() syntax.\n\nmy_function &lt;- function(x,y){\n  z &lt;- dplyr::select(...)\n}\n\nAvoid importing anything with @import tag, so that it is easier to understand what function is not local.\n\n\nDepends\nVery similar to Imports, but loaded when your package is loaded.\nHowever, even after putting pkg (such as ggplot2) under Depends, you still need to call the functions in a correct way (the three options above)."
  },
  {
    "objectID": "programming/r_pkg_dependency.html#common-error-warning-and-notes",
    "href": "programming/r_pkg_dependency.html#common-error-warning-and-notes",
    "title": "Dependencies",
    "section": "Common error, warning and notes",
    "text": "Common error, warning and notes\n\nError: could not find function xyz\nWhere the function xyz is from a dependency package, such as ggplot2.\n\ncould not find function “ggplot”\n\nThis happens when dependency package such as ggplot2 is not imported. When checking NAMESPACE, you see that this package is not in the import list.\nTo tackle this, locate the function where the error occurs, add the Roxygen2 tags as appropriate:\n\n#' @importFrom aaapkg aaa_fun\n#' @import bbbpkg\n#' @export\nmy_function &lt;- function(x,y){\n  ...\n}\n\nThen document (either devtools::document() or Build -&gt; Document button). Check NAMESPACE again, see if the package is imported.\n\n\nNote: all declared imports should be used\n\nNamespaces in Imports field not imported from: ‘data.table’ ‘gt’ ‘gtExtras’ ‘magrittr’ ‘rlang’ All declared Imports should be used.\n\nThis might happen when these external packages are not used. I can be more explicit when I actually use these packages somewhere. See 11.4.1 of the book for more information."
  },
  {
    "objectID": "inference/intro_causal.html",
    "href": "inference/intro_causal.html",
    "title": "Intro to Causal Effects",
    "section": "",
    "text": "Smoking example\n\ndoes smoking cause lung cancer?\ndoes lung cancer cause people to smoke?\nis there a third factor that causes both smoking and lung cancer?\n\n\n\n\n\n\n\n\n\nData generation\nCorrect causal model\nCorrect causal effect\n\n\n\n\nCollider\nY ~ X\n1\n\n\nConfounder\nY ~ X; Z\n0.5\n\n\nMediator\nDirect effect: Y ~ X; Z. Total effect: Y ~ X\nDirect: 0; total: 1\n\n\nM-Bias\nY ~ X\n1"
  },
  {
    "objectID": "inference/rwd_rwe.html#use-cases-for-rwd",
    "href": "inference/rwd_rwe.html#use-cases-for-rwd",
    "title": "Real-world Data, Real-world Evidence",
    "section": "Use cases for RWD",
    "text": "Use cases for RWD\nA few examples\n\ncharacerise health conditions, interventions, care pathways and patient outcomes\npatient-reported outcomes, quality of life\nestimate economic burden\nestimate test accuracy or reproducibility of biomarker test results\n\nRather than using RWD to replace RCT, there are a few ways to improve RCT in smaller populations. See Wieseler 2023\n\nRWD in oncology\nChallenging to incorporate RWD in regulatory evidence, treatment decisions and efficacy results are dependent on clinical characteristics that are not normally observed in RWD sources:\n\ndisease staging\nperformance status\nmutation tests\n…"
  },
  {
    "objectID": "inference/rwd_rwe.html#methods",
    "href": "inference/rwd_rwe.html#methods",
    "title": "Real-world Data, Real-world Evidence",
    "section": "Methods",
    "text": "Methods\nPropensity score matching (PSM) to reproduce the effects of randomization\nHow RWD is used\n\ncombined with non-randomized single-arm trial: as external RWD control\ntarget trial emulation from RWD"
  },
  {
    "objectID": "inference/propensity_score.html",
    "href": "inference/propensity_score.html",
    "title": "Weighting",
    "section": "",
    "text": "MatchIt package\nPS matching is ONE of the many matching techniques that uses PS as the difference."
  },
  {
    "objectID": "inference/overview.html#estimands",
    "href": "inference/overview.html#estimands",
    "title": "Overview: causal inference",
    "section": "Estimands",
    "text": "Estimands\nGreifer, N., & Stuart, E. A. (2021). Choosing the estimand when matching or weighting in observational studies. arXiv preprint arXiv:2106.10577.\n\n\n\n\n\nATE: average treatment effect in the population\n\\(E[Y(1) - Y(0)]\\)\nATT: average treatment effect among the treated\n\\(E[Y(1) - Y(0) | Z = 1]\\)\nATC: average treatment effect among the controls\n\\(E[Y(1) - Y(0) | Z = 0]\\)\nATM: average treatment effect among the matched"
  },
  {
    "objectID": "inference/overview.html#effects",
    "href": "inference/overview.html#effects",
    "title": "Overview: causal inference",
    "section": "",
    "text": "\\(Y^{a=1}, Y^{a=0}\\) are the potential outcomes under treatment 1 and 0. They are random variables. Treatment A has causal effect if \\(Y^{a=1} \\neq Y^{a=0}\\).\nFor individual \\(i\\), \\(Y_i^{a=1}, Y_i^{a=0}\\) are deterministic.\nIn reality we do not observe both potential outcomes for an individual, since we only have ONE outcome. We observe \\(Y\\) and \\(A\\). For a population, average treatment effect (ATE) can be estimate"
  },
  {
    "objectID": "inference/overview.html#graphical-reppresentation",
    "href": "inference/overview.html#graphical-reppresentation",
    "title": "Overview: causal inference",
    "section": "Graphical reppresentation",
    "text": "Graphical reppresentation\n\n\n\n\n\n\n\n\nData generation\nCorrect causal model\nCorrect causal effect\n\n\n\n\nCollider\nY ~ X\n1\n\n\nConfounder\nY ~ X; Z\n0.5\n\n\nMediator\nDirect effect: Y ~ X; Z. Total effect: Y ~ X\nDirect: 0; total: 1\n\n\nM-Bias\nY ~ X\n1"
  },
  {
    "objectID": "inference/case_study_liggetid_1.html",
    "href": "inference/case_study_liggetid_1.html",
    "title": "Case study: length of hospital stay",
    "section": "",
    "text": "The data liggetid was collected at the Geriatric Department at Ullevål Sykehus. This dataset has been used for teaching at University of Oslo, MF9130E. The course material can be accessed here.\nWe will focus on the following variables:\nlos &lt;- readRDS('data/los.rds')\nhead(los, 3)\n\n  admission_year age    sex admission_from stroke los\n1           1987  81 female           home      0  13\n2           1987  96 female           home      0  17\n3           1987  79 female           home      0   6\nSome variables need to be recoded and put into factor to display nicely.\nShow the code\n# library(dplyr)\nlibrary(ggplot2)\n\n# remove NA\nlos &lt;- dplyr::filter(los, !is.na(sex) & !is.na(stroke) & !is.na(admission_from))\n\n# code admission from with text\n# unique(los$admission_from)\nlos$admission_from &lt;- factor(los$admission_from, \n                            levels = c('home', 'div_surgery', \n                                       'div_medicine', 'div_other', \n                                       'other_hospital', 'nursing_home'), \n                            labels = c('home', 'div_surgery', \n                                       'div_medicine', 'div_other', \n                                       'other_hospital', 'nursing_home'))\n\n# code admission year with text\nlos$admission_year &lt;- factor(los$admission_year,\n                            levels = c(1981:1987),\n                            labels = as.character(1981:1987))\n\nlos$stroke &lt;- factor(los$stroke, \n                          levels = c(0, 1), \n                          labels = c('no','yes'))"
  },
  {
    "objectID": "dev/internal_notes/proposal_ab_los.html",
    "href": "dev/internal_notes/proposal_ab_los.html",
    "title": "Analysis + Simulation",
    "section": "",
    "text": "Investigate antibiotics effect on length of hospital stay\nAll AB combined together\n\nAnalysis + Simulation\n\nWithout causal (can use liggetid data to develop)\n\nsurvival curve comparison\ncox model adjusting age, sex\nmixed model, with department as levels\nmissing data imputation\n\n\n\nCausal\n\nPS matching\nPS weighting\n\ncompare different techniques impact on the estimates\nsensitivity analysis\nLiterarature\nhttps://www.fda.gov/drugs/regulatory-science-action/exploiting-real-world-data-optimize-use-antibiotics\nhttps://www.nature.com/articles/s41598-021-86853-4\nhttps://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-022-02027-w"
  },
  {
    "objectID": "inference/mixed_models.html",
    "href": "inference/mixed_models.html",
    "title": "Mixed models for repeted measurements",
    "section": "",
    "text": "Resources:\n\nmmrm package vignette\nMixed models with R\nGLMM FAQ\n\n\nMixed modeling in R\n\nLinear mixed models: nlme::lme, lme4::mner, brms::brm\ngeneralized linear mixed models (GLMM): lme4::glmer, glmmTMB; brms::brm for Bayesian\nnonlinear mixed models: nlme::nlme, lme4::nlmer; brms::brm\n\n\n\n\n\n\n\n\n\nequation\nformula\nmeaning\n\n\n\n\n\\(B_0 + B_1X_i + e_i\\)\n\nno random effect\n\n\n\\((B_0+b_{g,0} + B_1X_i + e_i)\\)\nx+(1|group)\nrandom group intercept\n\n\n\n(x|group)\nrandom slopt of x within group, with correlated intercept\n\n\n\n(1+x|group)\n\n\n\n\n(0+x | group)\nrandom slop of x within group, no variation in intercept\n\n\n\n(-1+x | group)\n\n\n\n\n(1| group) + (0+x|group)\nuncorrelated random intercept and random slopt within group\n\n\n\n\n\nRandom intercept\nEach group has different intercelpt, but the slope is the same\n\\[gpa = (b_0 + effect_{student} + b_{occasion} * occasion + e)\\]\n\n\nMMRM\nMMRM has one distinct feature compared to other linear mixed models: subject-specific random effects are considered as residual effects (part of error correlation matrix).\n\nMethodology\nBasic linear mixed-effects model for a single level of grouping\n\\[\ny_i = X_i \\beta + Z_i b_i + \\epsilon_i, i = 1, ..., n\n\\] \\[\nb_i \\sim N(0, \\Psi), \\epsilon \\sim N(0, \\sigma^2 I)\n\\]\n\n\\(\\beta\\) is p-dim vector of fixed effects\n\\(b\\) is q-dim vecor of random patient specific effects\n\\(X_i\\) of size \\(n_i \\times p\\) and \\(Z_i\\) of size \\(n_i \\times q\\) are regressor matrices relating observations to the fixed effects and random effects.\n\\(\\epsilon_i\\) is \\(n_i\\)-dimensional within-subject error"
  },
  {
    "objectID": "inference/propensity_score.html#matching",
    "href": "inference/propensity_score.html#matching",
    "title": "Weighting",
    "section": "",
    "text": "MatchIt package\nPS matching is ONE of the many matching techniques that uses PS as the difference."
  },
  {
    "objectID": "inference/propensity_score.html#weighting",
    "href": "inference/propensity_score.html#weighting",
    "title": "Weighting",
    "section": "Weighting",
    "text": "Weighting\nUsed in weighting\nATE (population) \\(w_{ATE} = \\frac{Z_i}{p_i} + \\frac{1-Z_i}{1-p_i}\\)\nATT \\(w_{ATT} = \\frac{p_i Z_i}{p_i} + \\frac{p_i(1-Z_i)}{1-p_i}\\)\nATC \\(w_{ATC} = \\frac{(1 - p_i) Z_i}{p_i} + \\frac{(1 - p_i)(1-Z_i)}{1-p_i}\\)\nPropensity score\n(Rosenbaum and Rubin, 1983)\n\nin observational studies, conditioning on propensity scores can lead to unbiased estimates of the exposure effect\ngiven that there are no unmeasured confounders\nevery subject has a non-zero probability of receiving exposure\n\nFit a logistic regression:\n\nbinary outcome: exposure (1,0)\ncovariates: all but exposure\n\nPredict the values (probability), they are the propensity scores.\nPS can also be estimated using other methods that produce probabilities, not just logistic regression: random forest, lasso logistic regression etc."
  },
  {
    "objectID": "inference/survival.html",
    "href": "inference/survival.html",
    "title": "Survival",
    "section": "",
    "text": "Links\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html\nhttps://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/\nhttps://www.danieldsjoberg.com/ggsurvfit/\nhttps://www.coursera.org/learn/survival-analysis-r-public-health\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html\nhttps://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/\njmpost: combines survival analysis, mixed effect model https://genentech.github.io/jmpost/main/"
  },
  {
    "objectID": "inference/overview.html#graphical-representation",
    "href": "inference/overview.html#graphical-representation",
    "title": "Overview: causal inference",
    "section": "Graphical representation",
    "text": "Graphical representation\n\n\n\n\n\n\n\n\nData generation\nCorrect causal model\nCorrect causal effect\n\n\n\n\nCollider\nY ~ X\n1\n\n\nConfounder\nY ~ X; Z\n0.5\n\n\nMediator\nDirect effect: Y ~ X; Z. Total effect: Y ~ X\nDirect: 0; total: 1\n\n\nM-Bias\nY ~ X\n1"
  },
  {
    "objectID": "inference/overview.html#selection-bias",
    "href": "inference/overview.html#selection-bias",
    "title": "Overview: causal inference",
    "section": "Selection bias",
    "text": "Selection bias\nThis bias is the result of selecting a common effect of 2 other variables (collider): a treatment, an outcome.\n\nnon-response, missing data\nself-selection, volunteer bias\nselection affected by treatment before study started\n\nA form of lack of exchangeability between the treated and untreated.\nCorrect for selection bias: IP weighting"
  },
  {
    "objectID": "inference/index.html#inference",
    "href": "inference/index.html#inference",
    "title": "Inference and models",
    "section": "Inference",
    "text": "Inference\nNotes on causal inference and other related topics.\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nOverview: causal inference\n\n\nCollider, confounder, mediator and M-bias \n\n\n\n\nWeighting\n\n\nPropensity score, matching, weighting \n\n\n\n\nTarget trial emulation\n\n\nTTE \n\n\n\n\nG-Computation\n\n\nG-Computation \n\n\n\n\nMissing data and imputation\n\n\nOverview of multiple imputation \n\n\n\n\nMultiple imputation in R\n\n\nMICE, regression, PMM \n\n\n\n\nIntervals\n\n\nConfidence, credible and prediction intervals \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "inference/index.html#models",
    "href": "inference/index.html#models",
    "title": "Inference and models",
    "section": "Models",
    "text": "Models\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nMixed models for repeted measurements\n\n\n\n\n\n\n\nSurvival\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "inference/index.html#case-studies",
    "href": "inference/index.html#case-studies",
    "title": "Inference and models",
    "section": "Case studies",
    "text": "Case studies\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nCase study: length of hospital stay\n\n\nPart 1: EDA \n\n\n\n\nCase study: CTN\n\n\nData challenge R/Medicine 2024 \n\n\n\n\nCase study: CTN-51\n\n\nFocusing on CTN 51 data \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "inference/g_computation.html",
    "href": "inference/g_computation.html",
    "title": "G-Computation",
    "section": "",
    "text": "G-computation\n\nFit a model for y ~ x + z, where z is all covariates\nCreate a duplicate for each level of x\nSet the value of x to a single value for each cloned dataset: x = 1 for one, x = 0 for the other\npredict\ncalculate estimate, mean(x_1) - mean(x_0)\n\nAdvantages\n\nflexible\nprecise (compared to propensity-score based methods)\nbasis of other important models (e.g. TMLE)"
  },
  {
    "objectID": "inference/case_study_liggetid_1.html#visualization",
    "href": "inference/case_study_liggetid_1.html#visualization",
    "title": "Case study: length of hospital stay",
    "section": "Visualization",
    "text": "Visualization\n\nLOS vs age and sex\n\n\nShow the code\nplt_scat2 &lt;- ggplot(data = los, \n                    mapping = aes(x = age, y = los, shape = sex, color = sex))\nplt_scat2 &lt;- plt_scat2 + geom_point(size = 2, alpha = 0.7)\n# customize\nplt_scat2 &lt;- plt_scat2 + labs(\n  x = 'Age', \n  y = 'Length of hosptial stay (days)', \n  title = 'Length of stay versus age'\n)\nplt_scat2 &lt;- plt_scat2 + theme_bw() # make white background\n# change text size\nplt_scat2 &lt;- plt_scat2 + theme(\n  axis.text = element_text(size = 12),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15)\n)\n# change color\nplt_scat2 &lt;- plt_scat2 + scale_color_brewer(palette = 'Set1')\nplt_scat2\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggridges)\nplt_ridge &lt;- ggplot(data = los, \n                    mapping = aes(x = los, y = admission_year, fill = sex))\nplt_ridge &lt;- plt_ridge + geom_density_ridges(alpha = 0.6) \nplt_ridge &lt;- plt_ridge + theme_ridges()\nplt_ridge &lt;- plt_ridge + labs(\n  x = 'Length of hosptial stay (days)', \n  y = 'Admission year', \n  title = 'Length of stay in each year, for each gender'\n)\n# change color\nplt_ridge &lt;- plt_ridge + scale_fill_brewer(palette = 'Set1')\nplt_ridge\n\n\nPicking joint bandwidth of 32.7\n\n\n\n\n\n\n\n\n\n\n\nLOS vs year of admission\n\n\nShow the code\nplt_box &lt;- ggplot(data = los, \n                  mapping = aes(x = admission_year, y = los, fill = sex))\nplt_box &lt;- plt_box + geom_boxplot(outlier.size = 1)\n# plt_box &lt;- plt_box + facet_wrap( ~ sex)\nplt_box &lt;- plt_box + coord_flip()\n\n# customize\nplt_box &lt;- plt_box + theme_bw() # make white background\nplt_box &lt;- plt_box + labs(\n  x = 'Admission year', \n  y = 'Length of hosptial stay (days)', \n  title = 'Length of stay in each year, both men and women'\n)\nplt_box &lt;- plt_box + theme(\n  axis.text = element_text(size = 12),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15), \n  strip.text = element_text(size = 12)\n)\n\nplt_box &lt;- plt_box + scale_fill_brewer(palette = 'Set1')\nplt_box \n\n\n\n\n\n\n\n\n\n\n\nLOS vs types of admission\n\n\nShow the code\nplt_box2 &lt;- ggplot(data = los, \n                   mapping = aes(x = admission_year, y = los, fill = sex))\nplt_box2 &lt;- plt_box2 + geom_boxplot(outlier.size = 0.8)\nplt_box2 &lt;- plt_box2 + facet_wrap( ~ admission_from)\n\n\n# customize\nplt_box2 &lt;- plt_box2 + theme_bw() # make white background\nplt_box2 &lt;- plt_box2 + labs(\n  x = 'Admission year', \n  y = 'Length of hosptial stay (days)', \n  title = 'Length of stay in each year, each type of admission'\n)\nplt_box2 &lt;- plt_box2 + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15), \n  strip.text = element_text(size = 12), \n  axis.text.x = element_text(angle = 45) # more readable\n)\n\nplt_box2 &lt;- plt_box2 + scale_fill_brewer(palette = 'Set1')\nplt_box2"
  },
  {
    "objectID": "inference/index.html#settings",
    "href": "inference/index.html#settings",
    "title": "Inference and models",
    "section": "",
    "text": "Different settings to apply methods.\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nReal-world Data, Real-world Evidence\n\n\nRWD, RWE \n\n\n\n\nGenomics in Drug Discovery\n\n\nUse of machine learning techniques \n\n\n\n\nAntibiotics\n\n\nBackground of antimicrobial drugs and resistance \n\n\n\n\nRWD EHR Vendor Engagement\n\n\nOverview of vendor engagement \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "inference/overview_ci.html",
    "href": "inference/overview_ci.html",
    "title": "Overview: causal inference",
    "section": "",
    "text": "\\(Y^{a=1}, Y^{a=0}\\) are the potential outcomes under treatment 1 and 0. They are random variables. Treatment A has causal effect if \\(Y^{a=1} \\neq Y^{a=0}\\).\nFor individual \\(i\\), \\(Y_i^{a=1}, Y_i^{a=0}\\) are deterministic.\nIn reality we do not observe both potential outcomes for an individual, since we only have ONE outcome. We observe \\(Y\\) and \\(A\\). For a population, average treatment effect (ATE) can be estimate"
  },
  {
    "objectID": "inference/overview_ci.html#effects",
    "href": "inference/overview_ci.html#effects",
    "title": "Overview: causal inference",
    "section": "",
    "text": "\\(Y^{a=1}, Y^{a=0}\\) are the potential outcomes under treatment 1 and 0. They are random variables. Treatment A has causal effect if \\(Y^{a=1} \\neq Y^{a=0}\\).\nFor individual \\(i\\), \\(Y_i^{a=1}, Y_i^{a=0}\\) are deterministic.\nIn reality we do not observe both potential outcomes for an individual, since we only have ONE outcome. We observe \\(Y\\) and \\(A\\). For a population, average treatment effect (ATE) can be estimate"
  },
  {
    "objectID": "inference/overview_ci.html#estimands",
    "href": "inference/overview_ci.html#estimands",
    "title": "Overview: causal inference",
    "section": "Estimands",
    "text": "Estimands\nGreifer, N., & Stuart, E. A. (2021). Choosing the estimand when matching or weighting in observational studies. arXiv preprint arXiv:2106.10577.\n\n\n\n\n\nATE: average treatment effect in the population\n\\(E[Y(1) - Y(0)]\\)\nATT: average treatment effect among the treated\n\\(E[Y(1) - Y(0) | Z = 1]\\)\nATC: average treatment effect among the controls\n\\(E[Y(1) - Y(0) | Z = 0]\\)\nATM: average treatment effect among the matched"
  },
  {
    "objectID": "inference/overview_ci.html#graphical-representation",
    "href": "inference/overview_ci.html#graphical-representation",
    "title": "Overview: causal inference",
    "section": "Graphical representation",
    "text": "Graphical representation\n\n\n\n\n\n\n\n\nData generation\nCorrect causal model\nCorrect causal effect\n\n\n\n\nCollider\nY ~ X\n1\n\n\nConfounder\nY ~ X; Z\n0.5\n\n\nMediator\nDirect effect: Y ~ X; Z. Total effect: Y ~ X\nDirect: 0; total: 1\n\n\nM-Bias\nY ~ X\n1"
  },
  {
    "objectID": "inference/overview_ci.html#selection-bias",
    "href": "inference/overview_ci.html#selection-bias",
    "title": "Overview: causal inference",
    "section": "Selection bias",
    "text": "Selection bias\nThis bias is the result of selecting a common effect of 2 other variables (collider): a treatment, an outcome.\n\nnon-response, missing data\nself-selection, volunteer bias\nselection affected by treatment before study started\n\nA form of lack of exchangeability between the treated and untreated.\nCorrect for selection bias: IP weighting"
  },
  {
    "objectID": "inference/genomics.html",
    "href": "inference/genomics.html",
    "title": "Genomics in Drug Discovery",
    "section": "",
    "text": "Why do we need precision medicine? Late-stage failures cost the most, and small improvements in failure rates at early stage yield largest savings - use better targets.\nAs of 2016, 10-15% targets have genetic data; increased to 50%, expect 13-15% cost reduction.\nNot all genes are targets, and not all targets are genes.\nFTO (fat mass and obesity gene). Can search FTO in clinical trial gov website.\nOverall nearly 60% are pursued by multiple companies, 26% by more than 5 companies: once a drug is made a target, redundancy is high. But before a target is proven, high diversity and novelty."
  },
  {
    "objectID": "inference/genomics.html#machine-learning",
    "href": "inference/genomics.html#machine-learning",
    "title": "Genomics in Drug Discovery",
    "section": "Machine learning",
    "text": "Machine learning\nGenomics data alone are insufficient for therapeutic development. How they interact with other types of data such as compounds, proteins, EHR, images, texts etc need to be investigated.\nTarget discovery: identify the molecure that can be targeted by a drug to produce a therapeutic effect, such as inhibition, to block the disease process.\nTherapeutic discovery: design potent therapeutic agents to modulate the target and block disease pathway. ML can be used to predict drug response in cell lines. Drug combination screening.\nDuring clinical studies, ML can help characterize patient groups and identify eligible patients from gene expression data and EHRs.\nDuring post-market studies, mining EHR and other RWD to provid additional evidence, such as patients’ drug response given different patient characteristics.\n\nSupervised learning\nRegression and classification, e.g. \n\ndrug sensitivity prediction\ngene expression signitures that predict clinical trial success\n\n\n\nUnsupervised learning\nClustering, e.g. \n\nfeature reduction in single-cell data to identify cell types\ncell types and biomarkers from single-cell RNA data\n\nExample: drug sensitivity predictive model. Identify biomarkers and build drug sensitivity predictive models using preclinical data, then apply to patients in early-stage clinical trials. Once validated, the model can be used for patient stratification and disease indicaation selection to support clinical development of a drug.\n(Example from (Vamathevan et al. 2019))"
  },
  {
    "objectID": "inference/missing_data.html",
    "href": "inference/missing_data.html",
    "title": "Missing data",
    "section": "",
    "text": "Missing data can occur in various situations:\n\nunit non-response: individuals decline to participate\nitem non-response: some questions or measurements left out by participating individuals\nloss of follow-up: drop outs\n\nFor unit non-response and loss of follow-up, weighting the data could help. Need to know the response rate, characteristics of the non-responders and how the respondent differ.\nSolutions (to item non-response)\n\ncomplete case analysis (listwise deletion)\nsingle imputation\n\nmean imputation\nconditional imputation: make a prediction, plus minus some noise\n\nmultiple imputation: repeat single imputation M times, produce M complete datasets, use Rubin’s rules to pool the estimates.\n\n\n\nThree categories of missing mechanism. Example: disease status, level of exposure, age. Exposure is missing for some subjects.\n\nMCAR: any two individuals have the same probability of having missing value for exposure. This is unlikely.\nMAR: any two individuaals with the same disease status and age have the same chance of having the exposure missing\nMNAR: chance of having missing exposure depends on the value of exposure\n\nData could also have special missing structures: multi-level, longitudinal and repeated measurements. In RCT a specific technique reference based imputation can be applied."
  },
  {
    "objectID": "dev/oldnotes_stat/unsupervised.html",
    "href": "dev/oldnotes_stat/unsupervised.html",
    "title": "Genomics data (microarray)",
    "section": "",
    "text": "A tool to detect expression of thousands of genes at the same time\nGene chip / DNA chip: each spot has a known DNA sequence or gene\nDNA molecules act as probes to detect gene expression (transcriptome) / mRNA transcripts\ncDNA (complementary DNA) from tissue 1 are labeled red, from tissue 2 are green\nred spot means gene is expressed in tissue 1\nyellow means gene is expressed in both tissues\ngene: a region of DNA coding for mRNA encoding the amino acidiee sequence\ngene expression: process by which genetic information in DNA is transcribed into mRNA\n\n\n\nderived from cancerous growths of humans or animals\nImmortalised in lab, proliferate indefinitely within an in vitro environment\nanalysing tumor cell lines allows us to identify targets that are expressed in cancer cells but not in normal cells\n\n\n\ngene x sample (or cell line)\neach entry is the expression level of a gene in a sample.\nexpression level: a gene generate more (or less) transcripts. this is a relative measurement of the number of transcripts\ncommonly taken log. Positive, increased level of expression; negative, decreased.\ncan compare GEM from healthy and cancer patients to find diagnostic biomarkers"
  },
  {
    "objectID": "dev/oldnotes_stat/unsupervised.html#k-means",
    "href": "dev/oldnotes_stat/unsupervised.html#k-means",
    "title": "Genomics data (microarray)",
    "section": "K-means",
    "text": "K-means\nNeed to pre-specify number of clusters K\nA good clustering achieves within-cluster variation as small as possible\nWCV (Ck), measures how much data within a cluster differ from each other\n\ntypically Euclidean distance\n\nNeed to minimize the TOTAL WCV for all clusters\nalgorithm\n\nrandomly assign a number from 1 to K to each observation, as initial cluster assignment\niterate until assignment stops changing:\n\nfor each of the K clusters: compute cluster centroid\nAssign each observation to the cluster whose centroid is closest (defined by Euclidean distance)"
  },
  {
    "objectID": "dev/oldnotes_stat/unsupervised.html#hierarchical-clustering",
    "href": "dev/oldnotes_stat/unsupervised.html#hierarchical-clustering",
    "title": "Genomics data (microarray)",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nDoes not require number of clusters K (still need to choose afterwards)\nBottom-up / agglomerative: build a hierarchy from bottom up, distance-based\nLinkage (dissimilarity between pairs of clusters)\n\ncentroid is common in genomics\nsingle (closest), complete (farthest two points), average\n\nDistance metrics\n\nEuclidean\ncorrelation based\nother distances that make sense in context\n\nPractical issues\n\nScaling of variables is important\nnumber of clusters\nchoice of dissimilarity metrics and linkage\nwhich features to use"
  },
  {
    "objectID": "inference/rwd_rwe.html#training-material",
    "href": "inference/rwd_rwe.html#training-material",
    "title": "Real-world Data, Real-world Evidence",
    "section": "Training material",
    "text": "Training material\nOnline courses\n\nReal-world evidence 1: Routinely collected data for clinical research by University of Basel\nReal-world evidence 2: Pragmatic trials - study designs for real-world decision making by University of Basel\n\nKeywords\n\nroutinely collected data for randomized trials (RCD-RCT)\nmeta-research\npragmatic trial\ngeneralizability, applicability and external validity of RCT\noptimal study design to support decision-making\nReal-world evidence in Pharmacoepidemiology by LSHTM\n\nKeywords\n\nsources of error, bias and confounding\nquantitative bias analysis (QBA)\nmissing data\nconfounding by indication\n\n\nReferences\nFang 2019\nSheffield 2020\n(Jemielita et al. 2021)\n\n\nJemielita, Thomas, Linnea Widman, Claire Fox, Stina Salomonsson, Kai Li Liaw, and Andreas Pettersson. 2021. “Replication of Oncology Randomized Trial Results using Swedish Registry Real World-Data: A Feasibility Study.” Clinical Pharmacology and Therapeutics 110 (6): 1613–21. https://doi.org/10.1002/cpt.2424."
  },
  {
    "objectID": "inference/missing_data.html#overview",
    "href": "inference/missing_data.html#overview",
    "title": "Missing data",
    "section": "",
    "text": "Missing data can occur in various situations:\n\nunit non-response: individuals decline to participate\nitem non-response: some questions or measurements left out by participating individuals\nloss of follow-up: drop outs\n\nFor unit non-response and loss of follow-up, weighting the data could help. Need to know the response rate, characteristics of the non-responders and how the respondent differ.\nSolutions (to item non-response)\n\ncomplete case analysis (listwise deletion)\nsingle imputation\n\nmean imputation\nconditional imputation: make a prediction, plus minus some noise\n\nmultiple imputation: repeat single imputation M times, produce M complete datasets, use Rubin’s rules to pool the estimates.\n\n\n\nThree categories of missing mechanism. Example: disease status, level of exposure, age. Exposure is missing for some subjects.\n\nMCAR: any two individuals have the same probability of having missing value for exposure. This is unlikely.\nMAR: any two individuaals with the same disease status and age have the same chance of having the exposure missing\nMNAR: chance of having missing exposure depends on the value of exposure\n\nData could also have special missing structures: multi-level, longitudinal and repeated measurements. In RCT a specific technique reference based imputation can be applied."
  },
  {
    "objectID": "dev/side_projects/ahus_proposals.html",
    "href": "dev/side_projects/ahus_proposals.html",
    "title": "Paper 1",
    "section": "",
    "text": "Paper 1\nQuality assurance of AB use\nTarget journal: BMJ quality and safety https://qualitysafety.bmj.com\n\nInitial analysis\nAB skjema + Mv processer\nCol E-H (doctor hypothesis)\nSo far keep all types, produce wide format data with time stamps\nMerge AB categories\nLocation, time of the day\npay attention to\n\ntype\ntime consistency\nactual use (compare with process table time stamps)\nfirst Tx for infection\n\n\n\n\nKeywords (Paper 2)\nsubgroup analysis\njoint model\ncausal inference (estimand framework)\nmixed model\nRandom effect (at an individual level)?\n\n\nJoint modeling\nCombines probability distribution from linear mixed effect with random effect (longitudinal), and survival cox model (computes hazard ratio).\nhttps://pubmed.ncbi.nlm.nih.gov/34674542/"
  },
  {
    "objectID": "inference/missing_data.html#consideration",
    "href": "inference/missing_data.html#consideration",
    "title": "Missing data",
    "section": "Consideration",
    "text": "Consideration\nNeed to account for the missinng data process, preserve the relations in the data and uncertainty in the relations.\n\nMAR assumption whether plausible. (FCS can handle both MAR anad MNAR)\nform of imputation model: structure and error distribution\npredictors, as many relevant as possible, including interactions\nthe order in which to impute\nset up starting imputation and number of iteration\ndecide number of imputed datasets"
  },
  {
    "objectID": "inference/antibiotics.html",
    "href": "inference/antibiotics.html",
    "title": "Antibiotics",
    "section": "",
    "text": "Drug-resistant diseases kill 700k people (Plackett2020) yearly, however fewer new antibiotic drugs are reaching the market.\nDevelopment cost: 1.5 billion USD (for one drug, based on 2017 study) while revenue is 45 million USD.\n\nnot enough demand: physicians prescribe less, treatment cycle is short compared to chronic diseases\nprice is low\ndevelopment is difficult\n\nhttps://www.nature.com/articles/s41429-023-00629-8"
  },
  {
    "objectID": "inference/antibiotics.html#economics-and-production",
    "href": "inference/antibiotics.html#economics-and-production",
    "title": "Antibiotics",
    "section": "",
    "text": "Drug-resistant diseases kill 700k people (Plackett2020) yearly, however fewer new antibiotic drugs are reaching the market.\nDevelopment cost: 1.5 billion USD (for one drug, based on 2017 study) while revenue is 45 million USD.\n\nnot enough demand: physicians prescribe less, treatment cycle is short compared to chronic diseases\nprice is low\ndevelopment is difficult\n\nhttps://www.nature.com/articles/s41429-023-00629-8"
  },
  {
    "objectID": "inference/antibiotics.html#antibiotics-stewardship",
    "href": "inference/antibiotics.html#antibiotics-stewardship",
    "title": "Antibiotics",
    "section": "Antibiotics stewardship",
    "text": "Antibiotics stewardship\nCoursera course: Antibiotics stewardship\nAntimicrobials are the 2nd most frequently prescribed claass of pharmaceuticals. As much as 50% of antibiotic use is inappropriate.\nFive D’s: (if it is the right) drug, dose, delivery, deescalation, duration.\nImpact of inappropriate AB use: poor patient outcomes (adverse reactions, organ toxicity, AB resistance, increased mortality); excess costs (drug acquisition cost, complication management, prolonged hospital stays, costs associated with AB resistance)\n\nPrinciples\nProphylactic to prevent infection, preemptive to abort infection, empiric to provide initial control in absence of knowledge of its etiology, definitive to cure infectiou of a knownn etiology or its antimicrobial susceptibility\nEmpiric use is very common - e.g. community and hospital acquired pneumonia, sepsis.\n\n\nPKPD\nPharmacokinetics (PK): what the body does to the drug - absorption, distribution, metabolism, elimination\nPharmacodynamics (PD): what the drug does to the body / target organism - measured drug concentration and antimicrobial effect (e.g. adverse effect, safety)\nMIC: minimum inhibitory concentration, from no visible growth to 99.9% bacteria kill (MBC)\nPharmacodynamic measures\n\nCmax, AB peak concentration\nCmin, AB trough concentration\nCmax / MIC, AUC / MIC, T&gt;MIC (on concentration time curve)\n\nConcentration-dependent AB classes: large, infrequent doses\n\naminoglycosides (e.g. gentamicin, tobramycin, aminkacin for life-threatening nosocomial infections)\nfluoroquinolonse (e.g. norfloxacin, ciprofloxacin, levofloxacin)\npolymyxin\n\nTime-dependent AB classes (beta-lactam): optimize the duration\n\npenicillins\ncephalosporins\ncarbapenems\nmacrolides"
  },
  {
    "objectID": "inference/antibiotics.html#research-on-antibiotics-use",
    "href": "inference/antibiotics.html#research-on-antibiotics-use",
    "title": "Antibiotics",
    "section": "Research on antibiotics use",
    "text": "Research on antibiotics use\n(relevant to my research)\nhttps://jamanetwork.com/journals/jamanetworkopen/fullarticle/2814214\nhttps://jamanetwork.com/journals/jamanetworkopen/fullarticle/2814216\nhttps://pubmed.ncbi.nlm.nih.gov/37760690/"
  },
  {
    "objectID": "inference/imputation_2.html",
    "href": "inference/imputation_2.html",
    "title": "Multiple imputation in R",
    "section": "",
    "text": "Here I use the small dataset nhanes included in mice package. It has 25 rows, and three out of four variables have missings.\nThe original NHANES data is a large national level survey, some are publicly available via R package nhanes.\nlibrary(mice)\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n# load example dataset from mice\nhead(nhanes)\n\n  age  bmi hyp chl\n1   1   NA  NA  NA\n2   2 22.7   1 187\n3   1   NA   1 187\n4   3   NA  NA  NA\n5   1 20.4   1 113\n6   3   NA  NA 184\n\nsummary(nhanes)\n\n      age            bmi             hyp             chl       \n Min.   :1.00   Min.   :20.40   Min.   :1.000   Min.   :113.0  \n 1st Qu.:1.00   1st Qu.:22.65   1st Qu.:1.000   1st Qu.:185.0  \n Median :2.00   Median :26.75   Median :1.000   Median :187.0  \n Mean   :1.76   Mean   :26.56   Mean   :1.235   Mean   :191.4  \n 3rd Qu.:2.00   3rd Qu.:28.93   3rd Qu.:1.000   3rd Qu.:212.0  \n Max.   :3.00   Max.   :35.30   Max.   :2.000   Max.   :284.0  \n                NA's   :9       NA's   :8       NA's   :10\nExamine missing pattern with md.pattern(data).\n# 27 missing in total\n# by col: 8 for hyp, 9 for bmi, 10 for chl\n# by row: n missing numbers\n\nmd.pattern(nhanes)\n\n\n\n\n\n\n\n\n   age hyp bmi chl   \n13   1   1   1   1  0\n3    1   1   1   0  1\n1    1   1   0   1  1\n1    1   0   0   1  2\n7    1   0   0   0  3\n     0   8   9  10 27"
  },
  {
    "objectID": "inference/imputation_2.html#imputation-without-model",
    "href": "inference/imputation_2.html#imputation-without-model",
    "title": "Multiple imputation in R",
    "section": "Imputation without model",
    "text": "Imputation without model\n\nImpute with mean\n\n# only run once, since it's just the mean\nimp &lt;- mice(data = nhanes, \n            method = 'mean', \n            m = 1, \n            maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimp # this is not the imputed value\n\nClass: mids\nNumber of multiple imputations:  1 \nImputation methods:\n   age    bmi    hyp    chl \n    \"\" \"mean\" \"mean\" \"mean\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n# check imputed values for bmi: same as mean(nhanes$bmi, na.rm=T)\nimp$imp$bmi\n\n         1\n1  26.5625\n3  26.5625\n4  26.5625\n6  26.5625\n10 26.5625\n11 26.5625\n12 26.5625\n16 26.5625\n21 26.5625\n\n# impute the dataset\nnhanes_imp &lt;- complete(imp)\nnhanes_imp\n\n   age     bmi      hyp   chl\n1    1 26.5625 1.235294 191.4\n2    2 22.7000 1.000000 187.0\n3    1 26.5625 1.000000 187.0\n4    3 26.5625 1.235294 191.4\n5    1 20.4000 1.000000 113.0\n6    3 26.5625 1.235294 184.0\n7    1 22.5000 1.000000 118.0\n8    1 30.1000 1.000000 187.0\n9    2 22.0000 1.000000 238.0\n10   2 26.5625 1.235294 191.4\n11   1 26.5625 1.235294 191.4\n12   2 26.5625 1.235294 191.4\n13   3 21.7000 1.000000 206.0\n14   2 28.7000 2.000000 204.0\n15   1 29.6000 1.000000 191.4\n16   1 26.5625 1.235294 191.4\n17   3 27.2000 2.000000 284.0\n18   2 26.3000 2.000000 199.0\n19   1 35.3000 1.000000 218.0\n20   3 25.5000 2.000000 191.4\n21   1 26.5625 1.235294 191.4\n22   1 33.2000 1.000000 229.0\n23   1 27.5000 1.000000 131.0\n24   3 24.9000 1.000000 191.4\n25   2 27.4000 1.000000 186.0\n\n\n\n\nImpute by sampling\n\nimps &lt;- mice(data = nhanes, \n            method = 'sample', \n            m = 1, \n            maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimps$imp$bmi\n\n      1\n1  27.4\n3  22.0\n4  22.5\n6  22.7\n10 20.4\n11 33.2\n12 22.7\n16 29.6\n21 25.5"
  },
  {
    "objectID": "inference/imputation_2.html#imputation-with-regression",
    "href": "inference/imputation_2.html#imputation-with-regression",
    "title": "Multiple imputation in R",
    "section": "Imputation with regression",
    "text": "Imputation with regression\nRegression methods (continuous, normal outcome) are implemented in mice with methods starting with norm.\n\nLinear regression without parameter uncertainty, mice.impute.norm.nob\nLinear regression through prediction, mice.impute.norm.predict\nBayesian linear regression, mice.impute.norm\nLinear regression bootstrap, mice.impute.norm.boot\n\n\nExample: Regression without parameter uncertainty\nWe can generate two imputed datasets by setting m=2.\nThere is a certain level of randomness, so would be a good idea to set seed.\n\nset.seed(1)\nimpr0 &lt;- mice(nhanes, method = 'norm.nob', m=2, maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  1   2  bmi  hyp  chl\n\nimpr0\n\nClass: mids\nNumber of multiple imputations:  2 \nImputation methods:\n       age        bmi        hyp        chl \n        \"\" \"norm.nob\" \"norm.nob\" \"norm.nob\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\nnhanes_impr0 &lt;- complete(impr0) # by default, returns the first imputation\nnhanes_impr0\n\n   age      bmi       hyp      chl\n1    1 35.53430 1.2503841 256.6153\n2    2 22.70000 1.0000000 187.0000\n3    1 27.31412 1.0000000 187.0000\n4    3 25.31243 2.3880837 267.1435\n5    1 20.40000 1.0000000 113.0000\n6    3 17.94547 1.5855064 184.0000\n7    1 22.50000 1.0000000 118.0000\n8    1 30.10000 1.0000000 187.0000\n9    2 22.00000 1.0000000 238.0000\n10   2 26.99782 1.0810473 206.9927\n11   1 32.71511 0.7819353 213.7222\n12   2 27.65399 0.7904680 209.6716\n13   3 21.70000 1.0000000 206.0000\n14   2 28.70000 2.0000000 204.0000\n15   1 29.60000 1.0000000 252.1596\n16   1 27.47980 0.6071353 145.9557\n17   3 27.20000 2.0000000 284.0000\n18   2 26.30000 2.0000000 199.0000\n19   1 35.30000 1.0000000 218.0000\n20   3 25.50000 2.0000000 245.7884\n21   1 35.12809 0.5807116 232.4652\n22   1 33.20000 1.0000000 229.0000\n23   1 27.50000 1.0000000 131.0000\n24   3 24.90000 1.0000000 268.3929\n25   2 27.40000 1.0000000 186.0000\n\n\nWhen we have two imputed datasets, we can check the values for each of the variables. For example, extract bmi variable from the imputed data imp,\n\n# two imputed datasets (m=2)\nimpr0$imp$bmi\n\n          1        2\n1  35.53430 32.26078\n3  27.31412 22.55473\n4  25.31243 14.90410\n6  17.94547 22.59196\n10 26.99782 25.08534\n11 32.71511 27.71485\n12 27.65399 25.76286\n16 27.47980 30.34985\n21 35.12809 29.89142\n\n\nWe can also specify which imputed dataset to use as our complete data. Set index to 0 (action = 0) returns the original dataset with missing values.\nHere we check which of the imputed data is being used as the completed dataset. First take a note of the row IDs (based on bmi, for example). Then we generate completed dataset.\n\nif no action argument is set, then it returns the first imputation by default\naction=0 corresponds to the original data with missing values\n\n\n# check which imputed data is used for the final result, take note of row id\nid_missing &lt;- which(is.na(nhanes$bmi))\nid_missing\n\n[1]  1  3  4  6 10 11 12 16 21\n\nnhanes_impr0_action0 &lt;- complete(impr0, action = 0) \nnhanes_impr0_action0[id_missing, ] # original data with missing bmi\n\n   age bmi hyp chl\n1    1  NA  NA  NA\n3    1  NA   1 187\n4    3  NA  NA  NA\n6    3  NA  NA 184\n10   2  NA  NA  NA\n11   1  NA  NA  NA\n12   2  NA  NA  NA\n16   1  NA  NA  NA\n21   1  NA  NA  NA\n\nnhanes_impr0_action1 &lt;- complete(impr0, action = 1) \nnhanes_impr0_action1[id_missing, ] # using first imputation\n\n   age      bmi       hyp      chl\n1    1 35.53430 1.2503841 256.6153\n3    1 27.31412 1.0000000 187.0000\n4    3 25.31243 2.3880837 267.1435\n6    3 17.94547 1.5855064 184.0000\n10   2 26.99782 1.0810473 206.9927\n11   1 32.71511 0.7819353 213.7222\n12   2 27.65399 0.7904680 209.6716\n16   1 27.47980 0.6071353 145.9557\n21   1 35.12809 0.5807116 232.4652\n\nnhanes_impr0_action2 &lt;- complete(impr0, action = 2) \nnhanes_impr0_action2[id_missing, ] # using second imputation\n\n   age      bmi       hyp      chl\n1    1 32.26078 0.4616324 228.0022\n3    1 22.55473 1.0000000 187.0000\n4    3 14.90410 1.4558818 212.7958\n6    3 22.59196 1.7664882 184.0000\n10   2 25.08534 1.2940549 201.5872\n11   1 27.71485 0.9410698 169.2427\n12   2 25.76286 1.3570093 168.5961\n16   1 30.34985 0.6878971 163.7262\n21   1 29.89142 1.0452062 212.9144\n\n\n\n\nOther imputation by linear regression\nOther various of imputaton via linear regression can be implemented simply by changing the method argument.\n\nLinear regression through prediction, mice.impute.norm.predict\nBayesian linear regression, mice.impute.norm\nLinear regression bootstrap, mice.impute.norm.boot\n\n\nimpr &lt;- mice(nhanes, method = 'norm.predict', m=1, maxit=1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimpr$imp$bmi\n\n          1\n1  28.33396\n3  28.33396\n4  22.75613\n6  21.17519\n10 27.19573\n11 29.12443\n12 26.26576\n16 30.28688\n21 28.33396\n\n\nBayesian linear regression\n\nimpb &lt;- mice(nhanes, method = 'norm', m=1, maxit=1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimpb$imp$bmi\n\n          1\n1  33.82959\n3  28.98754\n4  20.88810\n6  19.11391\n10 27.32990\n11 29.44117\n12 22.68062\n16 32.13267\n21 22.03164\n\n# nhanes_impb &lt;- complete(impb)\n\nBootstrap\n\nimpbt &lt;- mice(nhanes, method = 'norm.boot', m=1, maxit=1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimpbt$imp$bmi\n\n          1\n1  24.19248\n3  28.77464\n4  22.42321\n6  23.47542\n10 21.95529\n11 23.12703\n12 25.84230\n16 27.68216\n21 26.43770"
  },
  {
    "objectID": "inference/imputation_2.html#predictive-mean-matching-pmm",
    "href": "inference/imputation_2.html#predictive-mean-matching-pmm",
    "title": "Multiple imputation in R",
    "section": "Predictive Mean Matching PMM",
    "text": "Predictive Mean Matching PMM\nThe idea behind PMM is as follow.\n\nwith complete data, estimate a linear regression of Y (some missing) on Z (no missing), results in coefficients \\(\\beta\\).\ndraw \\(\\beta^*\\) from the posterior predictive distribution of \\(\\beta\\) (multivariate normal with mean b and covariance matrix of b).\ngenerate predicted values for \\(Y_{hat}\\) (complete cases) and \\(Y_{star}\\) (missing)\nfor each \\(Y_{star}\\), identify a few cases (7,12) whose predicted values \\(Y_{hat}\\) are close to the predicted \\(Y_{star}\\) (10 in the illustration below)\nrandomly draw one value from the observed \\(Y\\) from the doner cases (6, 11).\n\n\n\n\n\n\nAssumption for PMM: distribution of missing is the same as obsereved data of the candidates that produce the closest values to the predicted value by the missing entry.\nPMM is robust to transformation, less vulnerable to model misspecification.\nImplementation in mice:\n\nPredictive mean matching, mice.impute.pmm\nWeighted predictive mean matching, mice.impute.midastouch\nMultivariate predictive mean matching, mice.impute.mpmm\n\n\nimp_pmm &lt;- mice(nhanes, method = 'pmm', m=1, maxit=10)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n  6   1  bmi  hyp  chl\n  7   1  bmi  hyp  chl\n  8   1  bmi  hyp  chl\n  9   1  bmi  hyp  chl\n  10   1  bmi  hyp  chl\n\nimp_pmm\n\nClass: mids\nNumber of multiple imputations:  1 \nImputation methods:\n  age   bmi   hyp   chl \n   \"\" \"pmm\" \"pmm\" \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n# imputations for bmi\nimp_pmm$imp$bmi\n\n      1\n1  35.3\n3  27.2\n4  27.4\n6  22.5\n10 26.3\n11 22.5\n12 26.3\n16 33.2\n21 35.3\n\n\n\nimp_pmms &lt;- mice(nhanes, method = 'midastouch', m=1, maxit=10)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n  6   1  bmi  hyp  chl\n  7   1  bmi  hyp  chl\n  8   1  bmi  hyp  chl\n  9   1  bmi  hyp  chl\n  10   1  bmi  hyp  chl\n\nimp_pmm\n\nClass: mids\nNumber of multiple imputations:  1 \nImputation methods:\n  age   bmi   hyp   chl \n   \"\" \"pmm\" \"pmm\" \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\nimp_pmms$imp$bmi\n\n      1\n1  22.5\n3  30.1\n4  27.2\n6  27.4\n10 27.4\n11 30.1\n12 28.7\n16 22.5\n21 27.2"
  },
  {
    "objectID": "inference/imputation_1_overview.html",
    "href": "inference/imputation_1_overview.html",
    "title": "Missing data and imputation",
    "section": "",
    "text": "Missing data can occur in various situations:\n\nunit non-response: individuals decline to participate\nitem non-response: some questions or measurements left out by participating individuals\nloss of follow-up: drop outs\n\nFor unit non-response and loss of follow-up, weighting the data could help. Need to know the response rate, characteristics of the non-responders and how the respondent differ.\nSolutions (to item non-response)\n\ncomplete case analysis (listwise deletion)\nsingle imputation\n\nmean imputation\nconditional imputation: make a prediction, plus minus some noise\n\nmultiple imputation: repeat single imputation M times, produce M complete datasets, use Rubin’s rules to pool the estimates.\n\n\n\nThree categories of missing mechanism. Example: disease status, level of exposure, age. Exposure is missing for some subjects.\n\nMCAR: any two individuals have the same probability of having missing value for exposure. This is unlikely.\nMAR: any two individuaals with the same disease status and age have the same chance of having the exposure missing\nMNAR: chance of having missing exposure depends on the value of exposure\n\nData could also have special missing structures: multi-level, longitudinal and repeated measurements. In RCT a specific technique reference based imputation can be applied."
  },
  {
    "objectID": "inference/imputation_1_overview.html#overview",
    "href": "inference/imputation_1_overview.html#overview",
    "title": "Missing data and imputation",
    "section": "",
    "text": "Missing data can occur in various situations:\n\nunit non-response: individuals decline to participate\nitem non-response: some questions or measurements left out by participating individuals\nloss of follow-up: drop outs\n\nFor unit non-response and loss of follow-up, weighting the data could help. Need to know the response rate, characteristics of the non-responders and how the respondent differ.\nSolutions (to item non-response)\n\ncomplete case analysis (listwise deletion)\nsingle imputation\n\nmean imputation\nconditional imputation: make a prediction, plus minus some noise\n\nmultiple imputation: repeat single imputation M times, produce M complete datasets, use Rubin’s rules to pool the estimates.\n\n\n\nThree categories of missing mechanism. Example: disease status, level of exposure, age. Exposure is missing for some subjects.\n\nMCAR: any two individuals have the same probability of having missing value for exposure. This is unlikely.\nMAR: any two individuaals with the same disease status and age have the same chance of having the exposure missing\nMNAR: chance of having missing exposure depends on the value of exposure\n\nData could also have special missing structures: multi-level, longitudinal and repeated measurements. In RCT a specific technique reference based imputation can be applied."
  },
  {
    "objectID": "inference/imputation_1_overview.html#consideration",
    "href": "inference/imputation_1_overview.html#consideration",
    "title": "Missing data and imputation",
    "section": "Consideration",
    "text": "Consideration\nNeed to account for the missinng data process, preserve the relations in the data and uncertainty in the relations.\n\nMAR assumption whether plausible. (FCS can handle both MAR anad MNAR)\nform of imputation model: structure and error distribution\npredictors, as many relevant as possible, including interactions\nthe order in which to impute\nset up starting imputation and number of iteration\ndecide number of imputed datasets"
  },
  {
    "objectID": "inference/intervals.html",
    "href": "inference/intervals.html",
    "title": "Intervals",
    "section": "",
    "text": "Key difference: confidence and credible intervals: about the (unknown) parameter; prediction interval: about individual (unseen) observations."
  },
  {
    "objectID": "inference/intervals.html#prediction-intereval",
    "href": "inference/intervals.html#prediction-intereval",
    "title": "Intervals",
    "section": "Prediction intereval",
    "text": "Prediction intereval\nIn a regression model, you might want to know both confidence and prediction intervals.\n\nCI for mean value of \\(y\\) when \\(x =0\\), the mean response (e.g. growth of GDP), this is a parameter, an average\nPI for \\(y\\) when \\(x=0\\), this is an individual observation.\n\n\nIn simple linear regression\nStandard deviation for linear predictor \\(\\alpha + \\beta x\\) is\n\\(\\hat{\\sigma}_{linpred} = \\hat{\\sigma} \\sqrt{\\frac{1}{n} + \\frac{(x_{new} - \\bar{x})^2}{\\sum(x_i - \\bar{x})}}\\)\nConfidence interval\n\\(\\hat{y_{new}} \\pm t_{1-\\frac{\\alpha}{2}, n-2} \\times \\sqrt{\\hat{\\sigma}^2 (\\frac{1}{n} + \\frac{(x_{new} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2})}\\)\nStandard deviation for the predicted value \\(\\alpha + \\beta x + \\epsilon\\) is\n\\(\\hat{\\sigma}_{prediction} = \\hat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(x_{new} - \\bar{x})^2}{\\sum(x_i - \\bar{x})}}\\)\nPrediction interval (frequentist)\n\\(\\hat{y_{new}} \\pm t_{1-\\frac{\\alpha}{2}, n-2} \\times \\sqrt{\\hat{\\sigma}^2 (1 + \\frac{1}{n} + \\frac{(x_{new} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2})}\\)"
  },
  {
    "objectID": "inference/intervals.html#predictive-distribution",
    "href": "inference/intervals.html#predictive-distribution",
    "title": "Confidence, credible and prediction intervals",
    "section": "Predictive distribution",
    "text": "Predictive distribution\nPrior and posterior predictive distribution\nDifference between posterior distribution and posterior predictive distribution PPD\n\nposterior dist \\(p(\\theta|x) = c \\times p(x|\\theta)p(\\theta)\\), depends on the parame*ter \\(\\theta\\)\nPPD does not depend on \\(\\theta\\) as it is integrated out, for unobserved \\(x^*\\),\n\n\\(p(x^*|x) = \\int_{\\Theta} c \\times p(x^*, \\theta|x) d\\theta = \\int_{\\Theta} c \\times p(x^*|\\theta)p(\\theta|x) d\\theta\\)"
  },
  {
    "objectID": "inference/intervals.html#confidence-vs-credible-interval",
    "href": "inference/intervals.html#confidence-vs-credible-interval",
    "title": "Intervals",
    "section": "Confidence vs credible interval",
    "text": "Confidence vs credible interval\nConfidence interval (frequentist), about the unknown but fixed parameter. That means, the parameter is not treated as a random variable so does not have a probability distribution. CI is random because it is based on your sample.\nCredible interval (Bayesian), associated with posterior distribution of the parameter. The parameter is treated as a random variable hence has a probability distribution."
  },
  {
    "objectID": "inference/intervals.html#posterior-predictive-distribution",
    "href": "inference/intervals.html#posterior-predictive-distribution",
    "title": "Intervals",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\nDifference between posterior distribution and posterior predictive distribution PPD\n\nposterior dist \\(p(\\theta|x) = c \\times p(x|\\theta)p(\\theta)\\), depends on the parameter \\(\\theta\\)\nPPD does not depend on \\(\\theta\\) as it is integrated out, for unobserved \\(x^*\\),\n\n\\(p(x^*|x) = \\int_{\\Theta} c \\times p(x^*, \\theta|x) d\\theta = \\int_{\\Theta} c \\times p(x^*|\\theta)p(\\theta|x) d\\theta\\)\nPD is part of PPD formulation.\n\nPD explains the unknown parameter (treated as a random variable), conditional on the evidence observed (data).\nPPD is the distribution for the future predicted data based on the data you have already seen."
  },
  {
    "objectID": "inference/intervals.html#predict-mortality-using-2000-2019-data",
    "href": "inference/intervals.html#predict-mortality-using-2000-2019-data",
    "title": "Intervals",
    "section": "Predict mortality using 2000-2019 data",
    "text": "Predict mortality using 2000-2019 data\n\n# take pre 2019 data\ndt &lt;- d[year &lt;= 2019, .(year, deaths_vs_pop_per_100k)]\n\n# prediction \ndnew &lt;- data.frame(year = c(2020, 2021, 2022, 2023))\n\n### Linear regression with lm\n\nm_linear &lt;- lm(deaths_vs_pop_per_100k ~ year, \n               data = dt)\n\n# summary(m_linear)\n\n# produce two intervals\npred_freq_ci &lt;- predict(m_linear, newdata = dnew, interval = 'confidence')\npred_freq_pi &lt;- predict(m_linear, newdata = dnew, interval = 'prediction')\n\n\nVerify from formula\n\n# verify with formula\nn &lt;- nrow(dt)\n\n# option 1\nfitted_val &lt;- m_linear$fitted.values\nmse &lt;- sum((dt$deaths_vs_pop_per_100k - fitted_val)^2)/(n-2)\nmse\n\n[1] 4.383279\n\n# option 2\nsum((m_linear$residuals)^2)/(n-2)\n\n[1] 4.383279\n\n# option 3\nsummary(m_linear)$sigma^2\n\n[1] 4.383279\n\n# option 4\ndvmisc::get_mse(m_linear)\n\n[1] 4.383279\n\n# t-val\ntval &lt;- qt(p=0.975, df=n-2)\nmean_x &lt;- mean(dt$year)\n\n# sum(xi - xbar)^2\nssx &lt;- sum((dt$year - mean_x)^2)\n\nsd_confint &lt;- sqrt(mse * (1/20+ ((dnew$year - mean_x)^2)/ssx))\nsd_predint &lt;- sqrt(mse * (1 + 1/20+ ((dnew$year - mean_x)^2)/ssx))\n\n\n# point prediction\nb0 &lt;- coef(m_linear)[1]\nb &lt;- coef(m_linear)[2]\nprednew &lt;- b0 + b*dnew$year\nprednew\n\n[1] 85.68773 82.50765 79.32757 76.14749\n\n# prediction interval\npredint_linear &lt;- data.frame(fit = prednew, \n                             lwr = prednew - tval*sd_predint, \n                             upr = prednew + tval*sd_predint)\n\npredint_linear\n\n       fit      lwr      upr\n1 85.68773 80.83777 90.53770\n2 82.50765 77.59214 87.42316\n3 79.32757 74.34154 84.31360\n4 76.14749 71.08617 81.20880\n\n# confidence interval\nconfint_linear &lt;- data.frame(fit = prednew, \n                             lwr = prednew - tval*sd_confint, \n                             upr = prednew + tval*sd_confint)\n\nconfint_linear\n\n       fit      lwr      upr\n1 85.68773 83.64447 87.73100\n2 82.50765 80.31334 84.70196\n3 79.32757 76.97954 81.67560\n4 76.14749 73.64355 78.65142\n\n\n\n\nLinear regression with rstanarm\n\nm_bayes &lt;- rstanarm::stan_glm(\n  deaths_vs_pop_per_100k ~ year, \n  data = dt, \n  family = gaussian,\n  iter = 2000,\n  chains = 8,\n  refresh = 0\n)\n\nm_bayes \n\nstan_glm\n family:       gaussian [identity]\n formula:      deaths_vs_pop_per_100k ~ year\n observations: 20\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 6506.5  175.7\nyear          -3.2    0.1\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.2    0.4   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nsims &lt;- as.matrix(m_bayes)\nhead(sims)\n\n          parameters\niterations (Intercept)      year    sigma\n      [1,]    6227.938 -3.040142 3.106669\n      [2,]    6569.432 -3.210131 1.821779\n      [3,]    6750.044 -3.299792 2.419353\n      [4,]    6559.827 -3.204924 3.050782\n      [5,]    6306.464 -3.079356 1.727595\n      [6,]    6458.871 -3.155199 1.824202\n\nmedian &lt;- apply(sims, 2, median)\nmedian\n\n(Intercept)        year       sigma \n6506.530315   -3.178598    2.190397 \n\nmad_sd &lt;- apply(sims, 2, mad) \n# median absolute deviation (similar to sd)\nmad_sd\n\n(Intercept)        year       sigma \n175.6739842   0.0874817   0.3800231 \n\n\n\nCredible interval\n\n# credible interval about the fit\ncred &lt;- rstanarm::posterior_interval(m_bayes, prob = 0.95)\ncred\n\n                   2.5%       97.5%\n(Intercept) 6142.839155 6874.270468\nyear          -3.361534   -2.998133\nsigma          1.611480    3.201237\n\n# equivalent to\nsims &lt;- as.matrix(m_bayes)\napply(sims, 2, quantile, probs = c(0.025, 0.5, 0.975))\n\n       parameters\n        (Intercept)      year    sigma\n  2.5%     6142.839 -3.361534 1.611480\n  50%      6506.530 -3.178598 2.190397\n  97.5%    6874.270 -2.998133 3.201237\n\n\n\n\nPoint prediction\n\n# point predict\n# uses median from the posterior sim\ny_point_pred &lt;- predict(m_bayes, newdata = dnew)\ny_point_pred\n\n       1        2        3        4 \n85.69776 82.51901 79.34027 76.16153 \n\na_hat &lt;- coef(m_bayes)[1] # median\nb_hat &lt;- coef(m_bayes)[2] # median\na_hat + b_hat*dnew$year\n\n[1] 85.76176 82.58316 79.40456 76.22596\n\n\n\n# linear predictor with uncertainty via a's and b's\ny_linpred &lt;- rstanarm::posterior_linpred(m_bayes, newdata = dnew)\nhead(y_linpred)\n\n          \niterations        1        2        3        4\n      [1,] 86.85001 83.80987 80.76972 77.72958\n      [2,] 84.96646 81.75633 78.54619 75.33606\n      [3,] 84.46453 81.16473 77.86494 74.56515\n      [4,] 85.88082 82.67590 79.47097 76.26605\n      [5,] 86.16433 83.08497 80.00562 76.92626\n      [6,] 85.36923 82.21403 79.05883 75.90363\n\n# focus on one year: 2023\nhist(y_linpred[, 4])\n\n\n\nhead(y_linpred[, 4])\n\n[1] 77.72958 75.33606 74.56515 76.26605 76.92626 75.90363\n\ny_linpred_byhand &lt;- sims[,1] + dnew$year[4] * sims[,2]\ny_linpred_byhand[1:6]\n\n[1] 77.72958 75.33606 74.56515 76.26605 76.92626 75.90363\n\n\n\n# posterior predictive dist (with sigma)\ny_postpred &lt;- rstanarm::posterior_predict(m_bayes, newdata = dnew)\nhead(y_postpred)\n\n            1        2        3        4\n[1,] 84.26813 84.62143 86.88816 80.15932\n[2,] 84.77948 79.19925 79.47003 74.50026\n[3,] 84.28265 82.71001 71.98839 70.65769\n[4,] 86.88798 82.69186 84.09118 79.60984\n[5,] 87.15609 82.98913 76.53739 76.95372\n[6,] 87.23112 82.82839 80.20166 74.13253\n\n# by hand\n# focus on one year: 2023\nn_sim &lt;- nrow(sims)\ny_postpred_byhand &lt;- y_linpred_byhand + rnorm(n_sim, 0, sims[,3])\nhist(y_postpred_byhand)\n\n\n\nhist(y_postpred[,4])\n\n\n\ny_postpred[,4] |&gt; summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  64.27   74.44   76.10   76.13   77.83   88.49 \n\ny_postpred_byhand |&gt; summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  63.13   74.43   76.14   76.15   77.84   88.06 \n\n\n### Prediction\n\npred_all &lt;- rstanarm::posterior_predict(m_bayes, d)\n\n\npred_all_quantile &lt;- apply(pred_all, 2, quantile, \n                    probs = c(0.025, 0.5, 0.975)) |&gt; \n  t()  |&gt; \n  as.data.frame()\ntail(pred_all_quantile)\n\n       2.5%      50%    97.5%\n19 87.21596 92.02281 96.74796\n20 84.02340 88.91247 93.65092\n21 80.59699 85.69280 90.81009\n22 77.41676 82.45745 87.62077\n23 74.31205 79.28714 84.50176\n24 70.96315 76.13478 81.39219\n\n\n\n\nShow the code\npd &lt;- copy(d)\nhead(pd)\n\n\n   year deaths_n     age pop_jan1_n deaths_vs_pop_per_100k\n1: 2000     5427 000_059    3613275               150.1962\n2: 2001     5289 000_059    3636329               145.4489\n3: 2002     5303 000_059    3656613               145.0249\n4: 2003     5144 000_059    3679479               139.8024\n5: 2004     5069 000_059    3694134               137.2175\n6: 2005     4957 000_059    3705027               133.7912\n\n\nShow the code\npd &lt;- cbind(pd, pred_all_quantile)\n\nsetnames(pd, old = '2.5%', new = 'exp_death_per100k_025')\nsetnames(pd, old = '50%', new = 'exp_death_per100k_50')\nsetnames(pd, old = '97.5%', new = 'exp_death_per100k_975')\n\n# also compute the expected death overall\n\npd[, exp_death_025 := exp_death_per100k_025 * pop_jan1_n/100000]\npd[, exp_death_50 := exp_death_per100k_50 * pop_jan1_n/100000]\npd[, exp_death_975 := exp_death_per100k_975 * pop_jan1_n/100000]\n\npd[, alert := fcase(\n  deaths_vs_pop_per_100k &gt; exp_death_per100k_975, \"Higher than expected\",\n  default = \"Expected\"\n)]\n\npd[, type := fcase(\n  year &lt;= 2019, paste0(\"Baseline (2000-2019)\"),\n  default = \"Pandemic years (2020-2023)\"\n)]\n\n\n# make plot\nq &lt;- ggplot(pd, aes(x = year))\nq &lt;- q + geom_ribbon(mapping = aes(ymin = exp_death_per100k_025, \n                                   ymax = exp_death_per100k_975), \n                     alpha = 0.3)\nq &lt;- q + geom_line(mapping = aes(y = exp_death_per100k_50, lty = type), linewidth = 1)\nq &lt;- q + geom_point(mapping = aes(y = deaths_vs_pop_per_100k, color = alert), size = 3)\nq &lt;- q + geom_vline(xintercept = 2019.5, color = \"red\", lty = 2)\nq &lt;- q + expand_limits(y=0)\nq &lt;- q + scale_y_continuous(\"Number of death per 100k\", expand = expansion(mult = c(0, 0.1)))\nq &lt;- q + scale_x_continuous(\"Year\", breaks = seq(2000, 2023, 2))\nq &lt;- q + scale_linetype_discrete(NULL)\nq &lt;- q + scale_color_brewer(NULL, palette = \"Set1\", direction = -1)\nq &lt;- q + theme_bw()\nq &lt;- q + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15), \n  axis.text.x = element_text(angle = 45, vjust = 0.5) \n)\nq &lt;- q + theme(legend.position = \"bottom\", legend.direction = \"horizontal\")\nq &lt;- q + theme(legend.box = \"horizontal\", legend.margin = margin(2, 2, 2, 2))\nq"
  },
  {
    "objectID": "inference/intervals.html#explore-the-data",
    "href": "inference/intervals.html#explore-the-data",
    "title": "Intervals",
    "section": "Explore the data",
    "text": "Explore the data\n\n\nShow the code\nq1 &lt;- ggplot(d, aes(x = year, y = deaths_n, group))\nq1 &lt;- q1 + geom_line()\nq1 &lt;- q1 + geom_point(size = 2)\nq1 &lt;- q1 + geom_vline(xintercept = 2019.5, color = \"red\", lty = 2)\nq1 &lt;- q1 + theme_bw()\nq1 &lt;- q1 + scale_x_continuous(breaks = seq(2000, 2023, 2))\nq1 &lt;- q1 + labs(\n  x = 'Year', \n  y = 'Number of deaths', \n  title = 'Number of death in Norway \\n2000 - 2023'\n)\nq1 &lt;- q1 + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 12), \n  axis.text.x = element_text(angle = 45, vjust = 0.5) \n)\n# q1\n\n# per 100k\nq2 &lt;- ggplot(d, aes(x = year, y = deaths_vs_pop_per_100k))\nq2 &lt;- q2 + geom_line()\nq2 &lt;- q2 + geom_point(size = 2)\nq2 &lt;- q2 + geom_vline(xintercept = 2019.5, color = \"red\", lty = 2)\nq2 &lt;- q2 + theme_bw()\nq2 &lt;- q2 + scale_x_continuous(breaks = seq(2000, 2023, 2))\nq2 &lt;- q2 + labs(\n  x = 'Year', \n  y = 'Number of deaths', \n  title = 'Number of death in Norway \\n(per 100k population) 2000 - 2023'\n)\nq2 &lt;- q2 + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 12), \n  axis.text.x = element_text(angle = 45, vjust = 0.5) \n)\n# plot side by side\nq1 + q2"
  },
  {
    "objectID": "inference/intervals.html#model-mortality-using-2000-2019-data",
    "href": "inference/intervals.html#model-mortality-using-2000-2019-data",
    "title": "Intervals",
    "section": "Model mortality using 2000-2019 data",
    "text": "Model mortality using 2000-2019 data\n\n# take pre 2019 data\ndt &lt;- d[year &lt;= 2019, .(year, deaths_vs_pop_per_100k)]\n\n# prediction \ndnew &lt;- data.frame(year = c(2020, 2021, 2022, 2023))\n\n\nLinear regression with lm\n\nm_linear &lt;- lm(deaths_vs_pop_per_100k ~ year, \n               data = dt)\n\n# summary(m_linear)\n\n# produce two intervals\npred_freq_pi &lt;- predict(m_linear, newdata = dnew, interval = 'prediction')\npred_freq_ci &lt;- predict(m_linear, newdata = dnew, interval = 'confidence')\n\n\nVerify from formula\n\n# verify with formula\nn &lt;- nrow(dt)\n\n# option 1\nfitted_val &lt;- m_linear$fitted.values\nmse &lt;- sum((dt$deaths_vs_pop_per_100k - fitted_val)^2)/(n-2)\nmse\n\n[1] 4.383279\n\n# option 2\nsum((m_linear$residuals)^2)/(n-2)\n\n[1] 4.383279\n\n# option 3\nsummary(m_linear)$sigma^2\n\n[1] 4.383279\n\n# option 4\ndvmisc::get_mse(m_linear)\n\n[1] 4.383279\n\n# t-val\ntval &lt;- qt(p=0.975, df=n-2)\nmean_x &lt;- mean(dt$year)\n\n# sum(xi - xbar)^2\nssx &lt;- sum((dt$year - mean_x)^2)\n\nsd_confint &lt;- sqrt(mse * (1/20+ ((dnew$year - mean_x)^2)/ssx))\nsd_predint &lt;- sqrt(mse * (1 + 1/20+ ((dnew$year - mean_x)^2)/ssx))\n\n\n# point prediction\nb0 &lt;- coef(m_linear)[1]\nb &lt;- coef(m_linear)[2]\nprednew &lt;- b0 + b*dnew$year\nprednew\n\n[1] 85.68773 82.50765 79.32757 76.14749\n\n# prediction interval\npredint_linear &lt;- data.frame(fit = prednew, \n                             lwr = prednew - tval*sd_predint, \n                             upr = prednew + tval*sd_predint)\n\npredint_linear\n\n       fit      lwr      upr\n1 85.68773 80.83777 90.53770\n2 82.50765 77.59214 87.42316\n3 79.32757 74.34154 84.31360\n4 76.14749 71.08617 81.20880\n\npred_freq_pi # compare with result from lm\n\n       fit      lwr      upr\n1 85.68773 80.83777 90.53770\n2 82.50765 77.59214 87.42316\n3 79.32757 74.34154 84.31360\n4 76.14749 71.08617 81.20880\n\n# confidence interval\nconfint_linear &lt;- data.frame(fit = prednew, \n                             lwr = prednew - tval*sd_confint, \n                             upr = prednew + tval*sd_confint)\n\nconfint_linear\n\n       fit      lwr      upr\n1 85.68773 83.64447 87.73100\n2 82.50765 80.31334 84.70196\n3 79.32757 76.97954 81.67560\n4 76.14749 73.64355 78.65142\n\npred_freq_ci # compare with result from lm\n\n       fit      lwr      upr\n1 85.68773 83.64447 87.73100\n2 82.50765 80.31334 84.70196\n3 79.32757 76.97954 81.67560\n4 76.14749 73.64355 78.65142\n\n\n\n\n\nLinear regression with rstanarm\n\nm_bayes &lt;- rstanarm::stan_glm(\n  deaths_vs_pop_per_100k ~ year, \n  data = dt, \n  family = gaussian,\n  iter = 2000,\n  chains = 8,\n  refresh = 0\n)\n\nm_bayes \n\nstan_glm\n family:       gaussian [identity]\n formula:      deaths_vs_pop_per_100k ~ year\n observations: 20\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 6510.0  169.5\nyear          -3.2    0.1\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.2    0.4   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nsims &lt;- as.matrix(m_bayes)\nmedian &lt;- apply(sims, 2, median)\nmedian\nmad_sd &lt;- apply(sims, 2, mad) \n# median absolute deviation (similar to sd)\nmad_sd\n\n\nCredible interval\n\n# credible interval about the fit\ncred &lt;- rstanarm::posterior_interval(m_bayes, prob = 0.95)\ncred\n\n                   2.5%       97.5%\n(Intercept) 6147.110403 6852.327354\nyear          -3.350749   -2.999700\nsigma          1.614450    3.220209\n\n# equivalent to\nsims &lt;- as.matrix(m_bayes)\napply(sims, 2, quantile, probs = c(0.025, 0.5, 0.975))\n\n       parameters\n        (Intercept)      year    sigma\n  2.5%     6147.110 -3.350749 1.614450\n  50%      6510.045 -3.180305 2.194318\n  97.5%    6852.327 -2.999700 3.220209\n\n\n\n\n\nPoint prediction\n\n# point predict\n# uses median from the posterior sim\ny_point_pred &lt;- predict(m_bayes, newdata = dnew)\ny_point_pred\n\n       1        2        3        4 \n85.69091 82.51177 79.33263 76.15349 \n\na_hat &lt;- coef(m_bayes)[1] # median\nb_hat &lt;- coef(m_bayes)[2] # median\na_hat + b_hat*dnew$year\n\n[1] 85.82895 82.64865 79.46834 76.28804\n\n\n\n\nUncertainty of linear predictor\nThe uncertainty of linear predictor, \\(a + bx\\) is propagated through the uncertainty in \\(a\\) and \\(b\\), respectively. For now the error term is not included.\nrstanarm::posterior_linpred is equivalent to using each pairs of \\(a, b\\) from the posterior distribution to compute the point predictions.\n\n# linear predictor with uncertainty via a's and b's\ny_linpred &lt;- rstanarm::posterior_linpred(m_bayes, newdata = dnew)\nhead(y_linpred)\n\n          \niterations        1        2        3        4\n      [1,] 86.76040 83.68034 80.60028 77.52022\n      [2,] 86.33213 83.24274 80.15334 77.06394\n      [3,] 86.24299 83.12216 80.00133 76.88050\n      [4,] 85.26344 82.07579 78.88813 75.70048\n      [5,] 85.49074 82.23835 78.98596 75.73357\n      [6,] 85.28163 82.01827 78.75492 75.49156\n\n# focus on one year: 2023\nhist(y_linpred[, 4], main = 'Predictions for 2023')\n\n\n\n\n\n\n\nhead(y_linpred[, 4])\n\n[1] 77.52022 77.06394 76.88050 75.70048 75.73357 75.49156\n\ny_linpred_byhand &lt;- sims[,1] + dnew$year[4] * sims[,2]\ny_linpred_byhand[1:6]\n\n[1] 77.52022 77.06394 76.88050 75.70048 75.73357 75.49156\n\n\n\n\nPosterior predictive distribution\nWith PPD, include the additional uncertainty in \\(\\sigma\\). This is equivalent to using the linear predictor from above, plus a random draw from rnorm(1, 0, sigma).\nDue to randomness in the error, we can not get the exact same results. But they should be close enough.\n\n# posterior predictive dist (with sigma)\ny_postpred &lt;- rstanarm::posterior_predict(m_bayes, newdata = dnew)\nhead(y_postpred)\n\n            1        2        3        4\n[1,] 85.38304 80.76660 80.77002 78.29299\n[2,] 83.58124 83.70593 83.34616 76.07834\n[3,] 86.75044 85.02622 78.82639 76.81196\n[4,] 86.80538 78.61904 81.45173 78.71479\n[5,] 83.38782 82.76790 79.21554 75.24868\n[6,] 90.54799 80.21181 83.45230 75.14096\n\n# by hand\n# focus on one year: 2023\nn_sim &lt;- nrow(sims)\ny_postpred_byhand &lt;- y_linpred_byhand + rnorm(n_sim, 0, sims[,3])\npar(mfrow = c(1,2))\nhist(y_postpred_byhand, main = 'PPD for 2023 (linpred + error)')\nhist(y_postpred[,4], main = 'PPD for 2023 (post_predict)')\n\n\n\n\n\n\n\ny_postpred[,4] |&gt; summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  66.36   74.44   76.12   76.15   77.85   87.16 \n\ny_postpred_byhand |&gt; summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  65.10   74.51   76.23   76.21   77.89   87.72"
  },
  {
    "objectID": "inference/intervals.html#prediction-for-2020-2023-mortality",
    "href": "inference/intervals.html#prediction-for-2020-2023-mortality",
    "title": "Intervals",
    "section": "Prediction for 2020-2023 mortality",
    "text": "Prediction for 2020-2023 mortality\nHere we choose to use rstanarm::posterior_predict().\n\npred_all &lt;- rstanarm::posterior_predict(m_bayes, d)\n\npred_all_quantile &lt;- apply(pred_all, 2, quantile, \n                    probs = c(0.025, 0.5, 0.975)) |&gt; \n  t()  |&gt; \n  as.data.frame()\npred_all_quantile[21:24,]\n\n       2.5%      50%    97.5%\n21 80.76780 85.74303 90.58276\n22 77.46361 82.48300 87.47500\n23 74.24409 79.29768 84.40856\n24 70.99240 76.20945 81.30611\n\n\nWe can compare with the frequentist prediction interval, and we see that they are very close.\n\npredint_linear\n\n       fit      lwr      upr\n1 85.68773 80.83777 90.53770\n2 82.50765 77.59214 87.42316\n3 79.32757 74.34154 84.31360\n4 76.14749 71.08617 81.20880"
  },
  {
    "objectID": "inference/intervals.html#visualize-mortality-prediction",
    "href": "inference/intervals.html#visualize-mortality-prediction",
    "title": "Intervals",
    "section": "Visualize mortality prediction",
    "text": "Visualize mortality prediction\n\n\nShow the code\npd &lt;- copy(d)\n# head(pd)\npd &lt;- cbind(pd, pred_all_quantile)\n\nsetnames(pd, old = '2.5%', new = 'exp_death_per100k_025')\nsetnames(pd, old = '50%', new = 'exp_death_per100k_50')\nsetnames(pd, old = '97.5%', new = 'exp_death_per100k_975')\n\n# also compute the expected death overall\n\npd[, exp_death_025 := exp_death_per100k_025 * pop_jan1_n/100000]\npd[, exp_death_50 := exp_death_per100k_50 * pop_jan1_n/100000]\npd[, exp_death_975 := exp_death_per100k_975 * pop_jan1_n/100000]\n\npd[, alert := fcase(\n  deaths_vs_pop_per_100k &gt; exp_death_per100k_975, \"Higher than expected\",\n  default = \"Expected\"\n)]\n\npd[, type := fcase(\n  year &lt;= 2019, paste0(\"Baseline (2000-2019)\"),\n  default = \"Pandemic years (2020-2023)\"\n)]\n\n\n# make plot\nq &lt;- ggplot(pd, aes(x = year))\nq &lt;- q + geom_ribbon(mapping = aes(ymin = exp_death_per100k_025, \n                                   ymax = exp_death_per100k_975), \n                     alpha = 0.3)\nq &lt;- q + geom_line(mapping = aes(y = exp_death_per100k_50, lty = type), linewidth = 1)\nq &lt;- q + geom_point(mapping = aes(y = deaths_vs_pop_per_100k, color = alert), size = 3)\nq &lt;- q + geom_vline(xintercept = 2019.5, color = \"red\", lty = 2)\nq &lt;- q + expand_limits(y=0)\nq &lt;- q + scale_y_continuous(\"Number of death per 100k\", expand = expansion(mult = c(0, 0.1)))\nq &lt;- q + scale_x_continuous(\"Year\", breaks = seq(2000, 2023, 2))\nq &lt;- q + scale_linetype_discrete(NULL)\nq &lt;- q + scale_color_brewer(NULL, palette = \"Set1\", direction = -1)\nq &lt;- q + theme_bw()\nq &lt;- q + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15), \n  axis.text.x = element_text(angle = 45, vjust = 0.5) \n)\nq &lt;- q + theme(legend.position = \"bottom\", legend.direction = \"horizontal\")\nq &lt;- q + theme(legend.box = \"horizontal\", legend.margin = margin(2, 2, 2, 2))\nq"
  },
  {
    "objectID": "inference/rwd_vendor_engagement.html",
    "href": "inference/rwd_vendor_engagement.html",
    "title": "RWD EHR Vendor Engagement",
    "section": "",
    "text": "Vendor engagement (VE) in the context of Real-world data and Electronic Health Records (EHR/EMR) is the collaboration between various healthcare organizations (such as hospitals, clinics and medical practices) and vendors that provide EHR systems. VE is essential for successful EHR implementation and ongoing use in healthcare organizations. This partnership ensures that EHR systems meet the needs of healthcare providers and improves patient care.\nA few EHR vendors:\n\nEpicCare\nOracle Health EHR\nathenahealth\neClinicalWorks"
  },
  {
    "objectID": "inference/rwd_vendor_engagement.html#what-is-vendor-engagement",
    "href": "inference/rwd_vendor_engagement.html#what-is-vendor-engagement",
    "title": "RWD EHR Vendor Engagement",
    "section": "",
    "text": "Vendor engagement (VE) in the context of Real-world data and Electronic Health Records (EHR/EMR) is the collaboration between various healthcare organizations (such as hospitals, clinics and medical practices) and vendors that provide EHR systems. VE is essential for successful EHR implementation and ongoing use in healthcare organizations. This partnership ensures that EHR systems meet the needs of healthcare providers and improves patient care.\nA few EHR vendors:\n\nEpicCare\nOracle Health EHR\nathenahealth\neClinicalWorks"
  },
  {
    "objectID": "inference/rwd_vendor_engagement.html#who-is-involed",
    "href": "inference/rwd_vendor_engagement.html#who-is-involed",
    "title": "RWD EHR Vendor Engagement",
    "section": "Who is involed",
    "text": "Who is involed"
  },
  {
    "objectID": "inference/rwd_vendor_engagement.html#how-is-it-implemented",
    "href": "inference/rwd_vendor_engagement.html#how-is-it-implemented",
    "title": "RWD EHR Vendor Engagement",
    "section": "How is it implemented",
    "text": "How is it implemented\n\nSelection process\nHealthcare orgaisations engage with EHR vendors to select EHR systems that best fits their needs. This involves evaluating the products, features, pricing and support services.\n\n\nImplementation\nThis process innvolves data migration, software installation, customization to meet specific workflow requirements, and staff training. The customization could be to support specialized clinical workflows, integrate with other softwaree solutions or implementing additional features and modules.\n\n\nAfter implementation\nVE continues after the EHR system is implemented. Healthcare organizations rely on vendors for ongoing technical support, software updates and maintenance to ensure that the systems function smoothly and remains compliant with regulatory requirements.\nFeedback and collaboration between healthcare organizations and the vendor is crucial. Feedbacks on usability, functionality and performance of the EHR system can inform future updates and improvements for the vendor.\n\n\nRegulation and compliance\nRegulatory compliance in the context of electronic health records (EHRs) typically includes adherence to laws and regulations aimed at protecting patient privacy, ensuring data security, and promoting the interoperability of health information. Some of the key regulations that healthcare organizations and EHR vendors need to comply with include:\nHealth Insurance Portability and Accountability Act (HIPAA): HIPAA sets the standard for protecting sensitive patient data. It includes the Privacy Rule, which governs the use and disclosure of protected health information (PHI), and the Security Rule, which outlines security standards for protecting electronic PHI (ePHI). HIPAA also includes the Breach Notification Rule, which requires covered entities to notify individuals affected by breaches of their PHI.\nHITECH Act: The Health Information Technology for Economic and Clinical Health (HITECH) Act promotes the adoption and meaningful use of health information technology, including EHRs. It introduced incentives for healthcare providers to adopt EHRs and strengthened HIPAA’s privacy and security requirements, including increased penalties for non-compliance.\nMeaningful Use (now Promoting Interoperability) Program: The Centers for Medicare & Medicaid Services (CMS) previously administered the Meaningful Use program, which incentivized eligible healthcare providers to adopt and demonstrate meaningful use of certified EHR technology. This program has transitioned to the Promoting Interoperability program, which focuses on promoting the exchange of health information and improving interoperability.\nFDA Regulations: The U.S. Food and Drug Administration (FDA) regulates certain types of EHR software that meet the definition of a medical device. EHR vendors must comply with FDA regulations, particularly if their software includes features that are considered medical devices."
  },
  {
    "objectID": "inference/case_study_ctn.html",
    "href": "inference/case_study_ctn.html",
    "title": "Case study: CTN",
    "section": "",
    "text": "Task overview see here\nQuestions of interest (examples)\nArticle describing the process: Odom 2023"
  },
  {
    "objectID": "inference/rwd_vendor_engagement.html#implemention",
    "href": "inference/rwd_vendor_engagement.html#implemention",
    "title": "RWD EHR Vendor Engagement",
    "section": "Implemention",
    "text": "Implemention\n\nSelection process\nHealthcare organisations engage with EHR vendors to select EHR systems that best fits their needs. This involves evaluating the products, features, pricing and support services.\n\n\nImplementation\nThis process involves data migration, software installation, customization to meet specific workflow requirements, and staff training. The customization could be to support specialized clinical workflows, integrate with other software solutions or implementing additional features and modules.\n\n\nAfter implementation\nVE continues after the EHR system is implemented. Healthcare organizations rely on vendors for ongoing technical support, software updates and maintenance to ensure that the systems function smoothly and remains compliant with regulatory requirements.\nFeedback and collaboration between healthcare organizations and the vendor is crucial. Feedbacks on usability, functionality and performance of the EHR system can inform future updates and improvements for the vendor.\n\n\nRegulation and compliance\nRegulatory compliance in the context of electronic health records (EHRs) typically includes adherence to laws and regulations aimed at protecting patient privacy, ensuring data security, and promoting the interoperability of health information. Some of the key regulations that healthcare organizations and EHR vendors need to comply with include:\nHealth Insurance Portability and Accountability Act (HIPAA): HIPAA sets the standard for protecting sensitive patient data. It includes the Privacy Rule, which governs the use and disclosure of protected health information (PHI), and the Security Rule, which outlines security standards for protecting electronic PHI (ePHI). HIPAA also includes the Breach Notification Rule, which requires covered entities to notify individuals affected by breaches of their PHI.\nHITECH Act: The Health Information Technology for Economic and Clinical Health (HITECH) Act promotes the adoption and meaningful use of health information technology, including EHRs. It introduced incentives for healthcare providers to adopt EHRs and strengthened HIPAA’s privacy and security requirements, including increased penalties for non-compliance.\nMeaningful Use (now Promoting Interoperability) Program: The Centers for Medicare & Medicaid Services (CMS) previously administered the Meaningful Use program, which incentivized eligible healthcare providers to adopt and demonstrate meaningful use of certified EHR technology. This program has transitioned to the Promoting Interoperability program, which focuses on promoting the exchange of health information and improving interoperability.\nFDA Regulations: The U.S. Food and Drug Administration (FDA) regulates certain types of EHR software that meet the definition of a medical device. EHR vendors must comply with FDA regulations, particularly if their software includes features that are considered medical devices."
  },
  {
    "objectID": "inference/case_study_ctn.html#ctn-data",
    "href": "inference/case_study_ctn.html#ctn-data",
    "title": "Case study: CTN",
    "section": "CTN data",
    "text": "CTN data\n\nStudy protocol\nThree studies\n\nCTN 30(Protocol): prescription opiate abuse treatment study. Randomized, outpatient study, whether adding additional drug counseling improves outcome. Two phases.\n\nTx0: Bup/Nx (buprenorphine/naloxone) + SMM (standard medical management)\nTx1: Bup/Nx + EMM (enhanced medical management)\nphase 1 n=653, phase 2 n=360\nprimary finding: patients are likely to reduce opioid use during Bup/Nx treatment, however unsuccessful outcome is highly likely even after 12 weeks if tapering off treatment.\n\nCTN 27(Protocol, Saxon 2013): Bup/Nx vs MET (methadone) on liver function. Randomized, open-label, multi-center, phase 4 study.\nCTN 51 (Lee 2018): Bup/Nx vs XR-NTX naltrexone, effectiveness of pharmacotheerapeutic aids to recovery who completed detoxification and achieved short term abstinence. n=570\n\nXR-NTX (n=283): induction hurdle, higher rate of 24-week relapse, fewer successfully initiated XR-NTX (204 of 283) than Bup/Nx (270 of 287)\nfor those inducted successfully, relapse rate is similar\nrelapse: 4 consecutive weeks of any non-study opioid use (UDS or report), or 7 consecutive days of self-reported use.\n\n\nIf want to know which subject belongs to which study, use CreateProtocolHistory() or everybody data to have project ID.\nOpioids: Oxymorphone, Opium, Fentanyl, Hydromorphone, Codeine, Suboxone, Tramadol, Morphine, Buprenorphine, Hydrocodone, Opioid, Methadone, Oxycodone, Heroin\n\n\nTimes\n\nday 0: consent signed\nday of randomization (dr)\nday of receiving first dose of study drug (dd)\ninduction delay: dr - dd (in terms of days, not weeks)\nany day before dd is pre-treatment, even if participants are assigned to a treatment arm\n\npre-tx (baseline) / treatment period is defined by the first non-zero dose for the patient.\n\n\n\n\nEndpoints (CTNote)\nAs of CTNote, there are three types of outcomes\n\nabstinence\nuse reduction\nrelapse\n\n\n\nRisk factors\ndatasets info\n\n\nData details\n\nctn0094data\nctn0094extra\n\n\nlibrary(public.ctn0094data)\nlibrary(public.ctn0094extra)\n\n# visit_imp &lt;- public.ctn0094extra::derived_visitImputed\n# eth &lt;- public.ctn0094extra::derived_raceEthnicity\n\n#visit_imp\n#eth\n\n\ninduct_delay &lt;- public.ctn0094extra::derived_inductDelay\ninduct_delay\n\n# A tibble: 2,492 × 3\n     who treatment            inductDelay\n   &lt;int&gt; &lt;fct&gt;                      &lt;dbl&gt;\n 1     2 Outpatient BUP + EMM           0\n 2     3 Inpatient BUP                 NA\n 3     4 Inpatient NR-NTX               0\n 4     6 Outpatient BUP + SMM           0\n 5     7 Inpatient NR-NTX               1\n 6     9 Inpatient NR-NTX              NA\n 7    10 Outpatient BUP                 0\n 8    11 Methadone                      0\n 9    12 Outpatient BUP                 0\n10    13 Outpatient BUP                 0\n# ℹ 2,482 more rows\n\n\nWeekly pattern data. Week 1 starts the day after randomisation or signed consent, depending whether patients are randomized or not. Negative time stamps suggest prior randomization (or consent).\n\npattern_o &lt;- public.ctn0094extra::derived_weeklyOpioidPattern\npattern_o\n\n# A tibble: 3,560 × 8\n     who startWeek randWeek1 randWeek2 endWeek Baseline Phase_1          Phase_2\n   &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;  \n 1     1        -4        NA        NA      15 _____    ooooooooooooooo  \"\"     \n 2     2        -5         0        NA      14 ____++   ----oo-o-o-o+o   \"\"     \n 3     3        -6         0        NA      23 _______  o-ooo-ooooooooo… \"\"     \n 4     4        -4         0        NA      24 ____-    ---------------… \"\"     \n 5     5        -4        NA        NA      15 _____    ooooooooooooooo  \"\"     \n 6     6        -6         0        NA      14 ____+_+  -ooooooooooooo   \"\"     \n 7     7        -5         0        NA      24 _____+   ----ooooooooooo… \"\"     \n 8     8        -4        NA        NA      25 _____    ooooooooooooooo… \"\"     \n 9     9        -6         0        NA      22 ____+__  ooooooooooooooo… \"\"     \n10    10        -7         0        NA      22 _____+_- --o--*++o-+++++… \"\"     \n# ℹ 3,550 more rows\n\n\n\npattern_t &lt;- public.ctn0094extra::derived_weeklyTLFBPattern\npattern_t\n\n# A tibble: 3,560 × 8\n     who startWeek randWeek1 randWeek2 endWeek Baseline Phase_1          Phase_2\n   &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;  \n 1     1        -4        NA        NA      14 _____    oooooooooooooo   \"\"     \n 2     2        -5         0        NA      14 ++++++   -----------*+-   \"\"     \n 3     3        -6         0        NA      23 ++++*--  ----*----------… \"\"     \n 4     4        -4         0        NA      24 +++--    -------------*-… \"\"     \n 5     5        -4        NA        NA      14 _____    oooooooooooooo   \"\"     \n 6     6        -6         0        NA      14 +++++++  --------------   \"\"     \n 7     7        -5         0        NA      24 _+++--   ---------------… \"\"     \n 8     8        -4        NA        NA      24 _____    ooooooooooooooo… \"\"     \n 9     9        -6         0        NA      22 +++**--  ---------------… \"\"     \n10    10        -7         0        NA      22 --*+++++ ---------++-+-*… \"\"     \n# ℹ 3,550 more rows\n\n\nOutcomes (relapse)\n\nout &lt;- CTNote::outcomesCTN0094\nout$ctn0094_relapse_event |&gt; table() # relapse\n\n\n   0    1 \n 619 2941"
  },
  {
    "objectID": "inference/case_study_ctn_internal.html",
    "href": "inference/case_study_ctn_internal.html",
    "title": "Case study CTN (internal notes)",
    "section": "",
    "text": "What do I wish to get out of this analysis\n\na new dataset that is complex enough to test out methodology, potentially causal inference, since this is a RCT dataset\nlearn more about the novel ways to present data\ntry out AI for generating ideas\nwrite about it in at least 2 blogposts\n\nMy practical aim is to do ONE analysis only, and present the result in an innovative way.\n\n\nVisualization would be my priority, and presenting non-standard data is a key. This suggests that adding interactivity would be relevant here.\nLimited relevant knowledge means that my analysis wouldn’t standout."
  },
  {
    "objectID": "inference/case_study_ctn_internal.html#analysis-plan",
    "href": "inference/case_study_ctn_internal.html#analysis-plan",
    "title": "Case study CTN (internal notes)",
    "section": "",
    "text": "check"
  },
  {
    "objectID": "inference/case_study_ctn_internal.html#motivation",
    "href": "inference/case_study_ctn_internal.html#motivation",
    "title": "Case study CTN (internal notes)",
    "section": "",
    "text": "What do I wish to get out of this analysis\n\na new dataset that is complex enough to test out methodology, potentially causal inference, since this is a RCT dataset\nlearn more about the novel ways to present data\ntry out AI for generating ideas\nwrite about it in at least 2 blogposts\n\nMy practical aim is to do ONE analysis only, and present the result in an innovative way.\n\n\nVisualization would be my priority, and presenting non-standard data is a key. This suggests that adding interactivity would be relevant here.\nLimited relevant knowledge means that my analysis wouldn’t standout."
  },
  {
    "objectID": "inference/case_study_ctn_internal.html#plan",
    "href": "inference/case_study_ctn_internal.html#plan",
    "title": "Case study CTN (internal notes)",
    "section": "Plan",
    "text": "Plan\nThe deadline for completion is 2024.5.1, however I can only spare 7 full days to work on it.\n\nResearch stage: narrow down the scope\nOutcome: abstinence, use reduction, relapse\nRisk factors\n\n\nAnalysis stage: do one analysis\nPossibly focus on CTN51 study, examine endpoints including relapse (since it is also documented in a paper).\n\n\nVisualization: interactivity\nPossibly not standard visualization (KM plot)"
  },
  {
    "objectID": "inference/case_study_ctn_internal.html#log",
    "href": "inference/case_study_ctn_internal.html#log",
    "title": "Case study CTN (internal notes)",
    "section": "Log",
    "text": "Log\nStage I\n\n3.25 Initialize project, understand symbolic drug use data\n3.26 Read through vignette for the data derivation. Read data description for 3 data sources, try to understand the data generating process.\n3.27 Ask chatgpt for some background information on opioid use disorder. Merge datasets into wide format, then ask chatgpt to give some analysis suggestion.\n\nStage II\n\n3.28 Narrowed down scope, decide to create a visual tool for summarizing ICE in RCT. Read papers, identify important types of ICE and find relevant information.\n\nDecide to use lollipop plot for individual patient vis, possibly enhanced by augmented text, add interactivity to the plot using ggiraph\nadd some kind of grouping to multiple patients"
  },
  {
    "objectID": "inference/case_study_ctn_internal.html#background-information",
    "href": "inference/case_study_ctn_internal.html#background-information",
    "title": "Case study CTN (internal notes)",
    "section": "Background information",
    "text": "Background information\nOpioid epidemic in the US: driven by overprescription of opioid pain medications, proliferation of illicit opioids such as heroin, and emergence of highly potent synthetic opioids (e.g. fentanyl)\nTreatment for OUD involves a combination of pharmacotherapy (e.g. methadone, buprenorphine), behavioral therapies, support services. Medication-assisted treatment (MAT) is effective.\nChallenges: limited availability of treatment providers and resources, socialeconomic factors\nRisk factors for relapse\n\nbiological\n\npredisposition, family history of substance abuse\nco-occuring mental health disorders: depression, anxiety, post-traumatic stress disorder PTSD\nphysical health issues: chronic pain or other\n\npsycological\n\ncraving and withdrawl symptoms during detoxification\npoor coping skills, inability to deal with stress, negative emotions\nlow self-esteem\n\nsocial\n\npeer pressure, acquaintances who use opioid\nfamily dynamics, dysfunctional family relationships, lack of support\nsocial isolation\n\nenvironmental\n\navailability of opioids (prescription or illicit)\nstressful environments, poverty\n\ntreatment related\n\ninadequate treatment, lack of comprehensive care\npremature discontinuation\nlack of engagement\n\n\nRisk factors for drop-out\n\nseverity of addiction\npsychological\nsocial supoort (lack of)\nstigma and shame\ntreatment accessibility and affordability"
  },
  {
    "objectID": "inference/case_study_ctn_internal.html#analysis-proposal",
    "href": "inference/case_study_ctn_internal.html#analysis-proposal",
    "title": "Case study CTN (internal notes)",
    "section": "Analysis proposal",
    "text": "Analysis proposal\nChatgpt has suggested a few ideas for analysis:\n\nregression\nsurvival analysis\npropensity score matching\nmachine learning (rf, svm, gb), combined with feature importance analysis\nclustering (k-means, hierarchical)"
  },
  {
    "objectID": "inference/estimands_ice.html",
    "href": "inference/estimands_ice.html",
    "title": "Estimands, intercurrent events",
    "section": "",
    "text": "Key question: how to present the data visually?\nWhat are considered as important, what are not?\nITT (intention to treat): include the data after rescue medicine. Includes dropouts\nPP (Per-protocol): exclude patients taking rescue medicine, only analyse the complete cases\nTrial estimands: trial treatment effect depends on how events occur after treatment initiation.\nTreatment effect for a given outcome.\nFive core attributes\n\npopulation\ntreatment conditions\nendpoint\nsummary measure\nstrategies to handle each type of intercurrent event\n\n\nIntercurrent event\nPost-baseline events (post randomisation in RCT) that affects the interpretation of outcome.\nTwo categories:\n\ntreatment-modifying events, affects the receipt of assigned treatment. E.g. early discontinuation, use of rescue, wrong dose, wrong type (placebo for example).\ntruncating events, e.g. death, amputation of limb when the limb is relevant for the research question.\n\nStrategies (multiple can be used for different ICE in the same study)\n\ntreatment policy strategy: treat as it is, ignore ICE\ncomposite strategy: modifies the endpoint value, defined by the investigator\nwhile-on-treatment (while alive): before intercurrent event data is used.\nhypothetical strategy\nprincipal stratum strategy: redefine the population"
  },
  {
    "objectID": "inference/case_study_ctn51.html",
    "href": "inference/case_study_ctn51.html",
    "title": "Case study: CTN-51",
    "section": "",
    "text": "CTN 51 (Lee 2018): Bup/Nx vs XR-NTX naltrexone, effectiveness of pharmacotheerapeutic aids to recovery who completed detoxification and achieved short term abstinence. n=570\n\nXR-NTX (n=283): induction hurdle, higher rate of 24-week relapse, fewer successfully initiated XR-NTX (204 of 283) than Bup/Nx (270 of 287)\nfor those inducted successfully, relapse rate is similar\nrelapse: 4 consecutive weeks of any non-study opioid use (UDS or report), or 7 consecutive days of self-reported use.\n\nReproduce some of the results, try to understand the events involved after initiation\nKey numbers\n\nrelapse is 4 consecutive weeks of non-study opioid use or 7 days of self-reported use\nITT (n=570), 24 weeks relapse greater for NTX\npre-protocol (n=474), similar relapse\nrelapse rate is between 57% to 65%\n\nThings to understand\n\ninitiation hurdle"
  },
  {
    "objectID": "inference/case_study_ctn51.html#study-information",
    "href": "inference/case_study_ctn51.html#study-information",
    "title": "Case study: CTN-51",
    "section": "",
    "text": "CTN 51 (Lee 2018): Bup/Nx vs XR-NTX naltrexone, effectiveness of pharmacotheerapeutic aids to recovery who completed detoxification and achieved short term abstinence. n=570\n\nXR-NTX (n=283): induction hurdle, higher rate of 24-week relapse, fewer successfully initiated XR-NTX (204 of 283) than Bup/Nx (270 of 287)\nfor those inducted successfully, relapse rate is similar\nrelapse: 4 consecutive weeks of any non-study opioid use (UDS or report), or 7 consecutive days of self-reported use.\n\nReproduce some of the results, try to understand the events involved after initiation\nKey numbers\n\nrelapse is 4 consecutive weeks of non-study opioid use or 7 days of self-reported use\nITT (n=570), 24 weeks relapse greater for NTX\npre-protocol (n=474), similar relapse\nrelapse rate is between 57% to 65%\n\nThings to understand\n\ninitiation hurdle"
  },
  {
    "objectID": "inference/case_study_ctn51.html#datasets-used",
    "href": "inference/case_study_ctn51.html#datasets-used",
    "title": "Case study: CTN-51",
    "section": "Datasets used",
    "text": "Datasets used\n\nlibrary(public.ctn0094data)\nlibrary(public.ctn0094extra)\nlibrary(CTNote)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(stringr)\n\nout &lt;- CTNote::outcomesCTN0094\n\n# take c51 \n\neverybody$project |&gt; table() # this includes all eligible\n\n\n  27   30   51 \n1920  868  772 \n\nout_c51 &lt;- left_join(everybody, out) |&gt; \n  filter(project == 51)\n\nJoining with `by = join_by(who)`\n\nout_c51 |&gt; head()\n\n# A tibble: 6 × 64\n    who project usePatternUDS             fiellin2006_abs kosten1993_isAbs\n  &lt;dbl&gt; &lt;fct&gt;   &lt;chr&gt;                               &lt;dbl&gt; &lt;lgl&gt;           \n1     3 51      o-ooo-ooooooooooooooooo                 2 FALSE           \n2     4 51      -------------------o-o-o               21 TRUE            \n3     7 51      ----oooooooooooooooooooo                4 TRUE            \n4     9 51      oooooooooooooooooooooo                  0 FALSE           \n5    18 51      ooooooooooooooooooooooooo               0 FALSE           \n6    20 51      ooooooooooooooooooooooooo               0 FALSE           \n# ℹ 59 more variables: krupitsky2011A_isAbs &lt;lgl&gt;, krupitsky2011B_abs &lt;dbl&gt;,\n#   ling1998_isAbs &lt;lgl&gt;, lofwall2018_isAbs &lt;lgl&gt;, mokri2016_abs_time &lt;dbl&gt;,\n#   mokri2016_abs_event &lt;dbl&gt;, schottenfeld2005_abs &lt;dbl&gt;,\n#   schottenfeld2008A_abs_time &lt;dbl&gt;, schottenfeld2008A_abs_event &lt;dbl&gt;,\n#   schottenfeld2008B_abs &lt;dbl&gt;, shufman1994_absN_time &lt;dbl&gt;,\n#   shufman1994_absN_event &lt;dbl&gt;, weissLingCTN0030_isAbs &lt;lgl&gt;,\n#   comer2006_red &lt;dbl&gt;, eissenberg1997_isAbs &lt;lgl&gt;, fiellin2006_red &lt;dbl&gt;, …\n\n\n\nTreatment and outcome related\nOutcome includes pattern and class\nRandomization\ntreatment: time is study day, amount is dose or 1 (injection)"
  },
  {
    "objectID": "programming/git_setup.html",
    "href": "programming/git_setup.html",
    "title": "Set up version control",
    "section": "",
    "text": "Recently I have switched to a new MacBook M2 machine. The most important task is to set up version control, and synchronize important GitHub repositories.\n\nInstall Git\nCheck if you have git\nwhich git\ngit --version\ngit config\nI had to install git. I decided to do it with Homebrew.\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nAfter a few minutes, configure homebrew.\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; /Users/chizhang/.zprofile\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nbrew help\nIf the last line runs, it suggests that homebrew is correctly installed. Now try to install git with homebrew.\nbrew install git\n\n\nConfigure git\nAfter download, try to run the configuration again.\ngit config\ngit --version\nSince this is a new computer, need to configure the user name and email again. I made the names exactly the same as my old computer. To get the configuration from the other computer, I used git config --list.\ngit config --global user.name 'MYNAME'\ngit config --global user.name 'MYEMAIL'\ngit config --global credential.helper osxkeychain\n\n\nNew SSH key\nI followed the following steps: Checking for existing SSH keys\nFirst check whether I have existing ssh keys.\nls -al ~/.ssh\nNone returned, then I need to create a new one.\nGenerating a new SSH key\nssh-keygen -t ed25519 -C \"MYEMAIL\"\nNeed to enter passphrase.\nAdd SSH key to the ssh-agent\neval \"$(ssh-agent -s)\"\nopen ~/.ssh/config\nIf it does not exist, create a new one.\ntouch ~/.ssh/config\nnano ~/.ssh/config\nWrite the following in the config file. Pay attention to typos!\nHost github.com\n  AddKeysToAgent yes\n  UseKeychain yes\n  IdentityFile ~/.ssh/id_ed25519\nAdd the private key to the ssh-agent.\nssh-add --apple-use-keychain ~/.ssh/id_ed25519\n\n\nAdd SSH key to GitHub Account\nCopy the public SSH key to my clipboard.\npbcopy &lt; ~/.ssh/id_ed25519.pub\nGo on GitHub, go to Settings -&gt; SSH and GPG keys -&gt; New SSH Key\nPaste the public key."
  },
  {
    "objectID": "inference/rwd_statistics_overview.html",
    "href": "inference/rwd_statistics_overview.html",
    "title": "RWD statistical considerations",
    "section": "",
    "text": "In addition to data quality, missing, …\nThe key is the bias\nSelection bias (p67)\nInformation bias (p68)\nExamples of confounders\nAspects to consider: PROTECT framework (Fang 2019)"
  },
  {
    "objectID": "inference/rwd_statistics_overview.html#motivation-why-rwd-requires-special-treatment",
    "href": "inference/rwd_statistics_overview.html#motivation-why-rwd-requires-special-treatment",
    "title": "RWD statistical considerations",
    "section": "",
    "text": "In addition to data quality, missing, …\nThe key is the bias\nSelection bias (p67)\nInformation bias (p68)\nExamples of confounders\nAspects to consider: PROTECT framework (Fang 2019)"
  },
  {
    "objectID": "inference/rwd_statistics_overview.html#methods",
    "href": "inference/rwd_statistics_overview.html#methods",
    "title": "RWD statistical considerations",
    "section": "Methods",
    "text": "Methods\n\nStratification methods\nmatching, propensity score (p71)\n\n\nSensitivity analysis\np72\n\n\nMissing data\nOthers: G-methods (generalized)"
  },
  {
    "objectID": "programming/test_slides.html",
    "href": "programming/test_slides.html",
    "title": "Quarto Presentations",
    "section": "",
    "text": "This presentation will show you examples of what you can do with Quarto and Reveal.js, including:\n\nPresenting code and LaTeX equations\nIncluding computations in slide output\nImage, video, and iframe backgrounds\nFancy transitions and animations\nPrinting to PDF"
  },
  {
    "objectID": "programming/text.html",
    "href": "programming/text.html",
    "title": "Processing text and characters",
    "section": "",
    "text": "# split by space, put in columns\nstringr::str_split_fixed('Metronidazol Injection', '\\\\s+',2)\n\n     [,1]           [,2]       \n[1,] \"Metronidazol\" \"Injection\"\n\n# a vector\nv &lt;- c('Metronidazol Injection', 'Gentamicin Injection')\nstringr::str_split_fixed(v, '\\\\s+',2)\n\n     [,1]           [,2]       \n[1,] \"Metronidazol\" \"Injection\"\n[2,] \"Gentamicin\"   \"Injection\""
  },
  {
    "objectID": "programming/text.html#patterns",
    "href": "programming/text.html#patterns",
    "title": "Processing text and characters",
    "section": "",
    "text": "# split by space, put in columns\nstringr::str_split_fixed('Metronidazol Injection', '\\\\s+',2)\n\n     [,1]           [,2]       \n[1,] \"Metronidazol\" \"Injection\"\n\n# a vector\nv &lt;- c('Metronidazol Injection', 'Gentamicin Injection')\nstringr::str_split_fixed(v, '\\\\s+',2)\n\n     [,1]           [,2]       \n[1,] \"Metronidazol\" \"Injection\"\n[2,] \"Gentamicin\"   \"Injection\""
  },
  {
    "objectID": "programming/text.html#special-characters",
    "href": "programming/text.html#special-characters",
    "title": "Processing text and characters",
    "section": "Special characters",
    "text": "Special characters\n\nc('\\u00E6', '\\u00F8', '\\u00E5')\n\n[1] \"æ\" \"ø\" \"å\"\n\nc('\\u00C6', '\\u00D8', '\\u00C5')\n\n[1] \"Æ\" \"Ø\" \"Å\"\n\n# combine with other characters\n'Injeksjons\\u00E6ske'\n\n[1] \"Injeksjonsæske\""
  },
  {
    "objectID": "programming/df_manipulation.html",
    "href": "programming/df_manipulation.html",
    "title": "Data manipulation with data.table",
    "section": "",
    "text": "Code\n# make some fake data\nid &lt;- c(\n  rep(1, 3), \n  rep(2, 2), \n  rep(3, 6), \n  rep(4, 1)\n)\n\nab_types &lt;- c(\n  'ampicillin', 'gentamicin', 'cefalotin', 'metronidazol'\n)\nab &lt;- sample(ab_types, size = length(id), replace = T)\n\ndt &lt;- data.table::data.table(\n  id = id, ab = ab\n)\ndt\n\n\n       id         ab\n    &lt;num&gt;     &lt;char&gt;\n 1:     1 gentamicin\n 2:     1 ampicillin\n 3:     1  cefalotin\n 4:     2 gentamicin\n 5:     2 ampicillin\n 6:     3 ampicillin\n 7:     3  cefalotin\n 8:     3 ampicillin\n 9:     3 ampicillin\n10:     3 gentamicin\n11:     3  cefalotin\n12:     4 ampicillin\n\n\n\n\nRemove a column temporarily\n\ndt[, !c('id')]\n\n            ab\n        &lt;char&gt;\n 1: gentamicin\n 2: ampicillin\n 3:  cefalotin\n 4: gentamicin\n 5: ampicillin\n 6: ampicillin\n 7:  cefalotin\n 8: ampicillin\n 9: ampicillin\n10: gentamicin\n11:  cefalotin\n12: ampicillin\n\n\n\n\n\nGet number of records per person\n\ndt[, .N, by = id]\n\n      id     N\n   &lt;num&gt; &lt;int&gt;\n1:     1     3\n2:     2     2\n3:     3     6\n4:     4     1\n\n\nNumber of unique records\n\n# if use lapply, the results will be a list in the column\ndt[, .(n_uab = sapply(.SD, function(x){length(unique(x))})), \n   .SDcols = 'ab', \n   by = id]\n\n      id n_uab\n   &lt;num&gt; &lt;int&gt;\n1:     1     3\n2:     2     2\n3:     3     3\n4:     4     1"
  },
  {
    "objectID": "programming/df_manipulation.html#data.table-manipulation",
    "href": "programming/df_manipulation.html#data.table-manipulation",
    "title": "Data manipulation with data.table",
    "section": "",
    "text": "Code\n# make some fake data\nid &lt;- c(\n  rep(1, 3), \n  rep(2, 2), \n  rep(3, 6), \n  rep(4, 1)\n)\n\nab_types &lt;- c(\n  'ampicillin', 'gentamicin', 'cefalotin', 'metronidazol'\n)\nab &lt;- sample(ab_types, size = length(id), replace = T)\n\ndt &lt;- data.table::data.table(\n  id = id, ab = ab\n)\ndt\n\n\n       id         ab\n    &lt;num&gt;     &lt;char&gt;\n 1:     1 gentamicin\n 2:     1 ampicillin\n 3:     1  cefalotin\n 4:     2 gentamicin\n 5:     2 ampicillin\n 6:     3 ampicillin\n 7:     3  cefalotin\n 8:     3 ampicillin\n 9:     3 ampicillin\n10:     3 gentamicin\n11:     3  cefalotin\n12:     4 ampicillin\n\n\n\n\nRemove a column temporarily\n\ndt[, !c('id')]\n\n            ab\n        &lt;char&gt;\n 1: gentamicin\n 2: ampicillin\n 3:  cefalotin\n 4: gentamicin\n 5: ampicillin\n 6: ampicillin\n 7:  cefalotin\n 8: ampicillin\n 9: ampicillin\n10: gentamicin\n11:  cefalotin\n12: ampicillin\n\n\n\n\n\nGet number of records per person\n\ndt[, .N, by = id]\n\n      id     N\n   &lt;num&gt; &lt;int&gt;\n1:     1     3\n2:     2     2\n3:     3     6\n4:     4     1\n\n\nNumber of unique records\n\n# if use lapply, the results will be a list in the column\ndt[, .(n_uab = sapply(.SD, function(x){length(unique(x))})), \n   .SDcols = 'ab', \n   by = id]\n\n      id n_uab\n   &lt;num&gt; &lt;int&gt;\n1:     1     3\n2:     2     2\n3:     3     3\n4:     4     1"
  },
  {
    "objectID": "inference/weighting.html",
    "href": "inference/weighting.html",
    "title": "Weighting",
    "section": "",
    "text": "Exact matching:\n\nperfect covariate balance; \\(F(X_i|T_i = 1) = F(X_i|T_i=0)\\)\ninfeasible when covariate is continuous, and when there are many covariates.\n\nProbability of receiving treatment, \\(\\pi(X_i) = P(T_i = 1 | X_i)\\)\nMatching based on distance measures\n\nMahalanobis distance\nEstimated propensity score, \\(D(X_i, X_j) = |P(T_i = 1|X_i) - P(T_j=1 | X_j)|\\)\n\nCheck covariate balance\n\nideally compare joint distribution of all covariates\npractically check lower-dimensional summaries (standardized mean difference, variance ratio, empirical CDF difference)\n\nBalance test\nMatching would reduce number of observations\nMatchIt package\nPS matching is ONE of the many matching techniques that uses PS as the difference."
  },
  {
    "objectID": "inference/weighting.html#matching",
    "href": "inference/weighting.html#matching",
    "title": "Weighting",
    "section": "",
    "text": "Exact matching:\n\nperfect covariate balance; \\(F(X_i|T_i = 1) = F(X_i|T_i=0)\\)\ninfeasible when covariate is continuous, and when there are many covariates.\n\nProbability of receiving treatment, \\(\\pi(X_i) = P(T_i = 1 | X_i)\\)\nMatching based on distance measures\n\nMahalanobis distance\nEstimated propensity score, \\(D(X_i, X_j) = |P(T_i = 1|X_i) - P(T_j=1 | X_j)|\\)\n\nCheck covariate balance\n\nideally compare joint distribution of all covariates\npractically check lower-dimensional summaries (standardized mean difference, variance ratio, empirical CDF difference)\n\nBalance test\nMatching would reduce number of observations\nMatchIt package\nPS matching is ONE of the many matching techniques that uses PS as the difference."
  },
  {
    "objectID": "inference/weighting.html#weighting",
    "href": "inference/weighting.html#weighting",
    "title": "Weighting",
    "section": "Weighting",
    "text": "Weighting\nWeighting can be viewed as a generalization of matching\nUsed in weighting\nATE (population) \\(w_{ATE} = \\frac{Z_i}{p_i} + \\frac{1-Z_i}{1-p_i}\\)\nATT \\(w_{ATT} = \\frac{p_i Z_i}{p_i} + \\frac{p_i(1-Z_i)}{1-p_i}\\)\nATC \\(w_{ATC} = \\frac{(1 - p_i) Z_i}{p_i} + \\frac{(1 - p_i)(1-Z_i)}{1-p_i}\\)\n\nInverse probability weighting IPW\n(In my use case it is for survey sampling, rather than causal inference. My project does not have treatment group, and do not care about treatment effects. )"
  },
  {
    "objectID": "inference/weighting.html#propensity-score",
    "href": "inference/weighting.html#propensity-score",
    "title": "Weighting",
    "section": "Propensity score",
    "text": "Propensity score\n(Rosenbaum and Rubin, 1983)\n\nin observational studies, conditioning on propensity scores can lead to unbiased estimates of the exposure effect\ngiven that there are no unmeasured confounders\nevery subject has a non-zero probability of receiving exposure\n\nFit a logistic regression:\n\nbinary outcome: exposure (1,0)\ncovariates: all but exposure\n\nPredict the values (probability), they are the propensity scores.\nPS can also be estimated using other methods that produce probabilities, not just logistic regression: random forest, lasso logistic regression etc."
  },
  {
    "objectID": "method/index.html",
    "href": "method/index.html",
    "title": "Inference and models",
    "section": "",
    "text": "Different settings to apply methods.\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nReal-world Data, Real-world Evidence\n\n\nRWD, RWE \n\n\n\n\nGenomics in Drug Discovery\n\n\nUse of machine learning techniques \n\n\n\n\nAntibiotics\n\n\nBackground of antimicrobial drugs and resistance \n\n\n\n\nRWD EHR Vendor Engagement\n\n\nOverview of vendor engagement \n\n\n\n\nNutritional Epidemiology\n\n\nAbout Food \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "method/index.html#settings",
    "href": "method/index.html#settings",
    "title": "Inference and models",
    "section": "",
    "text": "Different settings to apply methods.\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nReal-world Data, Real-world Evidence\n\n\nRWD, RWE \n\n\n\n\nGenomics in Drug Discovery\n\n\nUse of machine learning techniques \n\n\n\n\nAntibiotics\n\n\nBackground of antimicrobial drugs and resistance \n\n\n\n\nRWD EHR Vendor Engagement\n\n\nOverview of vendor engagement \n\n\n\n\nNutritional Epidemiology\n\n\nAbout Food \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "method/index.html#study-design",
    "href": "method/index.html#study-design",
    "title": "Inference and models",
    "section": "Study design",
    "text": "Study design\n\nSurvey\nClinical trial design\n\nPhase I, II, III\nadaptive design\n\nSample size calculation\n\ncomparing a few groups (visualization TBD)\nregression (LR, GLM)\nmore advanced model (e.g. GLMM)\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nClinical trial design: overview\n\n\nNotes related to clinical trial design. \n\n\n\n\nSample size (part I)\n\n\nOverview, mean and proportion comparison \n\n\n\n\nSample size (part II)\n\n\nRegression \n\n\n\n\nAdaptive design: overview\n\n\nIntro to adaptive design \n\n\n\n\nSurvey, stratification\n\n\nSurvey sampling \n\n\n\n\nObservational study design\n\n\nCohort, case control and related metrics \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "method/index.html#inference",
    "href": "method/index.html#inference",
    "title": "Inference and models",
    "section": "Inference",
    "text": "Inference\nNotes on causal inference and other related topics.\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nOverview: causal inference\n\n\nCollider, confounder, mediator and M-bias \n\n\n\n\nWeighting\n\n\nPropensity score, matching, weighting \n\n\n\n\nTarget trial emulation\n\n\nTTE \n\n\n\n\nG-Computation\n\n\nG-Computation \n\n\n\n\nMissing data and imputation\n\n\nOverview of multiple imputation \n\n\n\n\nMultiple imputation in R\n\n\nMICE, regression, PMM \n\n\n\n\nIntervals\n\n\nConfidence, credible and prediction intervals \n\n\n\n\nNotes from book: What If (Part 1)\n\n\nCausal inference notes: chapter 1 to 10 \n\n\n\n\nNotes from book: What if (Part x)\n\n\nInstrumental variables \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "method/index.html#models",
    "href": "method/index.html#models",
    "title": "Inference and models",
    "section": "Models",
    "text": "Models\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nRegression\n\n\nLinear, logistic, Cox proportional hazard \n\n\n\n\nMixed models for repeted measurements\n\n\n\n\n\n\n\nSurvival\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "method/index.html#case-studies",
    "href": "method/index.html#case-studies",
    "title": "Inference and models",
    "section": "Case studies",
    "text": "Case studies\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nLength of hospital stay: Part I\n\n\nPart 1: EDA \n\n\n\n\nLength of hospital stay: Part II\n\n\nPart 2: time-to-event analysis \n\n\n\n\nLinear regression example: prestige\n\n\nLinear regression \n\n\n\n\nLogistic regression example: lung\n\n\nLogistic regression \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "method/rwd_rwe.html",
    "href": "method/rwd_rwe.html",
    "title": "Real-world Data, Real-world Evidence",
    "section": "",
    "text": "RWD is any data collected outside clinical trial setting, can be combined with clinical trials.\n(The general benefits and disadvantages of RWD is coherent with EHR data)\nTherapheutic areas: oncology, rare diseases, infectious diseases among others\nTraditionally regarded as inferior evidence: lack of randomization, limited information on potential relevant prognostic factors.\nFDA: (2016) 21st Century Cures Act, evaluate the use of RWD in support of regulatory approvals and post-approval safety studies.\nEMA (2017): HMA/EMA Joint Big Data Task Force, establish a roadmap for the use of RWD in regulatory assessments\nChallenges of using RCT\nFocus: whether RWD can be trusted to reliably measure treatment effects of new drugs, causal relationship. The main difference between RCT and RWD is the confounding bias."
  },
  {
    "objectID": "method/rwd_rwe.html#use-cases-for-rwd",
    "href": "method/rwd_rwe.html#use-cases-for-rwd",
    "title": "Real-world Data, Real-world Evidence",
    "section": "Use cases for RWD",
    "text": "Use cases for RWD\nA few examples\n\ncharacerise health conditions, interventions, care pathways and patient outcomes\npatient-reported outcomes, quality of life\nestimate economic burden\nestimate test accuracy or reproducibility of biomarker test results\n\nRather than using RWD to replace RCT, there are a few ways to improve RCT in smaller populations. See Wieseler 2023\n\nRWD in oncology\nChallenging to incorporate RWD in regulatory evidence, treatment decisions and efficacy results are dependent on clinical characteristics that are not normally observed in RWD sources:\n\ndisease staging\nperformance status\nmutation tests\n…"
  },
  {
    "objectID": "method/genomics.html",
    "href": "method/genomics.html",
    "title": "Genomics in Drug Discovery",
    "section": "",
    "text": "Why do we need precision medicine? Late-stage failures cost the most, and small improvements in failure rates at early stage yield largest savings - use better targets.\nAs of 2016, 10-15% targets have genetic data; increased to 50%, expect 13-15% cost reduction.\nNot all genes are targets, and not all targets are genes.\nFTO (fat mass and obesity gene). Can search FTO in clinical trial gov website.\nOverall nearly 60% are pursued by multiple companies, 26% by more than 5 companies: once a drug is made a target, redundancy is high. But before a target is proven, high diversity and novelty."
  },
  {
    "objectID": "method/genomics.html#machine-learning",
    "href": "method/genomics.html#machine-learning",
    "title": "Genomics in Drug Discovery",
    "section": "Machine learning",
    "text": "Machine learning\nGenomics data alone are insufficient for therapeutic development. How they interact with other types of data such as compounds, proteins, EHR, images, texts etc need to be investigated.\nTarget discovery: identify the molecure that can be targeted by a drug to produce a therapeutic effect, such as inhibition, to block the disease process.\nTherapeutic discovery: design potent therapeutic agents to modulate the target and block disease pathway. ML can be used to predict drug response in cell lines. Drug combination screening.\nDuring clinical studies, ML can help characterize patient groups and identify eligible patients from gene expression data and EHRs.\nDuring post-market studies, mining EHR and other RWD to provid additional evidence, such as patients’ drug response given different patient characteristics.\n\nSupervised learning\nRegression and classification, e.g. \n\ndrug sensitivity prediction\ngene expression signitures that predict clinical trial success\n\n\n\nUnsupervised learning\nClustering, e.g. \n\nfeature reduction in single-cell data to identify cell types\ncell types and biomarkers from single-cell RNA data\n\nExample: drug sensitivity predictive model. Identify biomarkers and build drug sensitivity predictive models using preclinical data, then apply to patients in early-stage clinical trials. Once validated, the model can be used for patient stratification and disease indicaation selection to support clinical development of a drug.\n(Example from (Vamathevan et al. 2019))"
  },
  {
    "objectID": "method/g_computation.html",
    "href": "method/g_computation.html",
    "title": "G-Computation",
    "section": "",
    "text": "G-computation\n\nFit a model for y ~ x + z, where z is all covariates\nCreate a duplicate for each level of x\nSet the value of x to a single value for each cloned dataset: x = 1 for one, x = 0 for the other\npredict\ncalculate estimate, mean(x_1) - mean(x_0)\n\nAdvantages\n\nflexible\nprecise (compared to propensity-score based methods)\nbasis of other important models (e.g. TMLE)"
  },
  {
    "objectID": "method/sample_size_2.html",
    "href": "method/sample_size_2.html",
    "title": "Sample size (part II)",
    "section": "",
    "text": "(This is the part II on sample size calculation)\n\nCorrelation\nCorrelation coefficient is used for effect size measure. Usse 0.1, 0.3, 0.5 to represent small, medium and large sizes.\n\n# correlation coeff\npwr::pwr.r.test(r = 0.3, sig.level = 0.05, power = 0.8, alternative = 'two.sided')\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 84.07364\n              r = 0.3\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\nLinear regression (F-test)\nF-test for linear regression is testinng whether \\(R^2\\) is greater than zero (one-sided). \\(R^2\\) is the explained variance by using the predictors, \\(R^2 = 0.3\\) means that 30% of the variance are explained by the model.\nCohen’s f2, based on \\(R^2\\), goodness of fit (\\(f2 = R^2/(1-R^2)\\)). use 0.02, 0.15, 0.35 to represent small, medium and large effect sizes.\n\nu: number of predictors\nv: n-u-1\nas a result, sample size n = v+u+1\n\n\n# effect size f2 = 0.15; use u=3 predictors\npwr::pwr.f2.test(u = 3, f2 = 0.3, sig.level = 0.05, power = 0.8)\n\n\n     Multiple regression power calculation \n\n              u = 3\n              v = 36.47078\n             f2 = 0.3\n      sig.level = 0.05\n          power = 0.8\n\n\nHere v = 73, sample size is 73+3+1 = 77.\nAlternatively, can use pwrss::pwrss.f.reg(). The parameter is r2 rather than f2 (but can also use f2).\n\npwrss::pwrss.f.reg(r2 = 0.3, k = 0.3, power = 0.8, alpha = 0.05)\n\n Linear Regression (F test) \n R-squared Deviation from 0 (zero) \n H0: r2 = 0 \n HA: r2 &gt; 0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 14 \n ------------------------------ \n Numerator degrees of freedom = 0.3 \n Denominator degrees of freedom = 12.415 \n Non-centrality parameter = 5.878 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n\n# should be equivalent to\n# pwr::pwr.f2.test(u = 3, f2 = 0.3/0.7, sig.level = 0.05, power = 0.8)\n\n\n\nGLM\n\nResources\n\nSample size calculation in clinical trial using R.\nPark et al. 2023. https://doi.org/10.7602/jmis.2023.26.1.9\nBulus, M (2023) pwrss: Statistical Power and Sample Size Calculation Tools. R package version 0.3.1. https://CRAN.R-project.org/package=pwrss. Vignette documentation"
  },
  {
    "objectID": "method/adaptive_design.html",
    "href": "method/adaptive_design.html",
    "title": "Adaptive design: overview",
    "section": "",
    "text": "Resources\n\nR package rpact and tutorial\nR package gsDesign and tutorial\n\n\nSequential design\nNumber of patient isn’t set in advance. A good group sequential design can reduce the sample size needed, while keeping the desired statistical power and controlling the overall type I error.\nGSD includes pre-determined number of stages (interim, final). Each stage specified by\n\nsample size\ncritical values\nstopping criterion to support or reject null hypothesis\n\nTBC\n\n\nBasket and umbrella trials\n\n\nInterview questions\n\nCan you describe what an adaptive clinical trial design is and give an example of how it can be applied in oncology?\nWhat are the advantages and disadvantages of adaptive trial designs compared to traditional fixed designs?\nHow would you adjust a trial protocol if interim analysis shows a promising treatment effect?"
  },
  {
    "objectID": "method/intervals.html",
    "href": "method/intervals.html",
    "title": "Intervals",
    "section": "",
    "text": "Key difference: confidence and credible intervals: about the (unknown) parameter; prediction interval: about individual (unseen) observations."
  },
  {
    "objectID": "method/intervals.html#confidence-vs-credible-interval",
    "href": "method/intervals.html#confidence-vs-credible-interval",
    "title": "Intervals",
    "section": "Confidence vs credible interval",
    "text": "Confidence vs credible interval\nConfidence interval (frequentist), about the unknown but fixed parameter. That means, the parameter is not treated as a random variable so does not have a probability distribution. CI is random because it is based on your sample.\nCredible interval (Bayesian), associated with posterior distribution of the parameter. The parameter is treated as a random variable hence has a probability distribution."
  },
  {
    "objectID": "method/intervals.html#prediction-intereval",
    "href": "method/intervals.html#prediction-intereval",
    "title": "Intervals",
    "section": "Prediction intereval",
    "text": "Prediction intereval\nIn a regression model, you might want to know both confidence and prediction intervals.\n\nCI for mean value of \\(y\\) when \\(x =0\\), the mean response (e.g. growth of GDP), this is a parameter, an average\nPI for \\(y\\) when \\(x=0\\), this is an individual observation.\n\n\nIn simple linear regression\nStandard deviation for linear predictor \\(\\alpha + \\beta x\\) is\n\\(\\hat{\\sigma}_{linpred} = \\hat{\\sigma} \\sqrt{\\frac{1}{n} + \\frac{(x_{new} - \\bar{x})^2}{\\sum(x_i - \\bar{x})}}\\)\nConfidence interval\n\\(\\hat{y_{new}} \\pm t_{1-\\frac{\\alpha}{2}, n-2} \\times \\sqrt{\\hat{\\sigma}^2 (\\frac{1}{n} + \\frac{(x_{new} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2})}\\)\nStandard deviation for the predicted value \\(\\alpha + \\beta x + \\epsilon\\) is\n\\(\\hat{\\sigma}_{prediction} = \\hat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(x_{new} - \\bar{x})^2}{\\sum(x_i - \\bar{x})}}\\)\nPrediction interval (frequentist)\n\\(\\hat{y_{new}} \\pm t_{1-\\frac{\\alpha}{2}, n-2} \\times \\sqrt{\\hat{\\sigma}^2 (1 + \\frac{1}{n} + \\frac{(x_{new} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2})}\\)"
  },
  {
    "objectID": "method/intervals.html#posterior-predictive-distribution",
    "href": "method/intervals.html#posterior-predictive-distribution",
    "title": "Intervals",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\nDifference between posterior distribution and posterior predictive distribution PPD\n\nposterior dist \\(p(\\theta|x) = c \\times p(x|\\theta)p(\\theta)\\), depends on the parameter \\(\\theta\\)\nPPD does not depend on \\(\\theta\\) as it is integrated out, for unobserved \\(x^*\\),\n\n\\(p(x^*|x) = \\int_{\\Theta} c \\times p(x^*, \\theta|x) d\\theta = \\int_{\\Theta} c \\times p(x^*|\\theta)p(\\theta|x) d\\theta\\)\nPD is part of PPD formulation.\n\nPD explains the unknown parameter (treated as a random variable), conditional on the evidence observed (data).\nPPD is the distribution for the future predicted data based on the data you have already seen."
  },
  {
    "objectID": "method/intervals.html#explore-the-data",
    "href": "method/intervals.html#explore-the-data",
    "title": "Intervals",
    "section": "Explore the data",
    "text": "Explore the data\n\n\nShow the code\nq1 &lt;- ggplot(d, aes(x = year, y = deaths_n, group))\nq1 &lt;- q1 + geom_line()\nq1 &lt;- q1 + geom_point(size = 2)\nq1 &lt;- q1 + geom_vline(xintercept = 2019.5, color = \"red\", lty = 2)\nq1 &lt;- q1 + theme_bw()\nq1 &lt;- q1 + scale_x_continuous(breaks = seq(2000, 2023, 2))\nq1 &lt;- q1 + labs(\n  x = 'Year', \n  y = 'Number of deaths', \n  title = 'Number of death in Norway \\n2000 - 2023'\n)\nq1 &lt;- q1 + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 12), \n  axis.text.x = element_text(angle = 45, vjust = 0.5) \n)\n# q1\n\n# per 100k\nq2 &lt;- ggplot(d, aes(x = year, y = deaths_vs_pop_per_100k))\nq2 &lt;- q2 + geom_line()\nq2 &lt;- q2 + geom_point(size = 2)\nq2 &lt;- q2 + geom_vline(xintercept = 2019.5, color = \"red\", lty = 2)\nq2 &lt;- q2 + theme_bw()\nq2 &lt;- q2 + scale_x_continuous(breaks = seq(2000, 2023, 2))\nq2 &lt;- q2 + labs(\n  x = 'Year', \n  y = 'Number of deaths', \n  title = 'Number of death in Norway \\n(per 100k population) 2000 - 2023'\n)\nq2 &lt;- q2 + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 12), \n  axis.text.x = element_text(angle = 45, vjust = 0.5) \n)\n# plot side by side\nq1 + q2"
  },
  {
    "objectID": "method/intervals.html#model-mortality-using-2000-2019-data",
    "href": "method/intervals.html#model-mortality-using-2000-2019-data",
    "title": "Intervals",
    "section": "Model mortality using 2000-2019 data",
    "text": "Model mortality using 2000-2019 data\n\n# take pre 2019 data\ndt &lt;- d[year &lt;= 2019, .(year, deaths_vs_pop_per_100k)]\n\n# prediction \ndnew &lt;- data.frame(year = c(2020, 2021, 2022, 2023))\n\n\nLinear regression with lm\n\nm_linear &lt;- lm(deaths_vs_pop_per_100k ~ year, \n               data = dt)\n\n# summary(m_linear)\n\n# produce two intervals\npred_freq_pi &lt;- predict(m_linear, newdata = dnew, interval = 'prediction')\npred_freq_ci &lt;- predict(m_linear, newdata = dnew, interval = 'confidence')\n\n\nVerify from formula\n\n# verify with formula\nn &lt;- nrow(dt)\n\n# option 1\nfitted_val &lt;- m_linear$fitted.values\nmse &lt;- sum((dt$deaths_vs_pop_per_100k - fitted_val)^2)/(n-2)\nmse\n\n[1] 4.383279\n\n# option 2\nsum((m_linear$residuals)^2)/(n-2)\n\n[1] 4.383279\n\n# option 3\nsummary(m_linear)$sigma^2\n\n[1] 4.383279\n\n# option 4\ndvmisc::get_mse(m_linear)\n\n[1] 4.383279\n\n# t-val\ntval &lt;- qt(p=0.975, df=n-2)\nmean_x &lt;- mean(dt$year)\n\n# sum(xi - xbar)^2\nssx &lt;- sum((dt$year - mean_x)^2)\n\nsd_confint &lt;- sqrt(mse * (1/20+ ((dnew$year - mean_x)^2)/ssx))\nsd_predint &lt;- sqrt(mse * (1 + 1/20+ ((dnew$year - mean_x)^2)/ssx))\n\n\n# point prediction\nb0 &lt;- coef(m_linear)[1]\nb &lt;- coef(m_linear)[2]\nprednew &lt;- b0 + b*dnew$year\nprednew\n\n[1] 85.68773 82.50765 79.32757 76.14749\n\n# prediction interval\npredint_linear &lt;- data.frame(fit = prednew, \n                             lwr = prednew - tval*sd_predint, \n                             upr = prednew + tval*sd_predint)\n\npredint_linear\n\n       fit      lwr      upr\n1 85.68773 80.83777 90.53770\n2 82.50765 77.59214 87.42316\n3 79.32757 74.34154 84.31360\n4 76.14749 71.08617 81.20880\n\npred_freq_pi # compare with result from lm\n\n       fit      lwr      upr\n1 85.68773 80.83777 90.53770\n2 82.50765 77.59214 87.42316\n3 79.32757 74.34154 84.31360\n4 76.14749 71.08617 81.20880\n\n# confidence interval\nconfint_linear &lt;- data.frame(fit = prednew, \n                             lwr = prednew - tval*sd_confint, \n                             upr = prednew + tval*sd_confint)\n\nconfint_linear\n\n       fit      lwr      upr\n1 85.68773 83.64447 87.73100\n2 82.50765 80.31334 84.70196\n3 79.32757 76.97954 81.67560\n4 76.14749 73.64355 78.65142\n\npred_freq_ci # compare with result from lm\n\n       fit      lwr      upr\n1 85.68773 83.64447 87.73100\n2 82.50765 80.31334 84.70196\n3 79.32757 76.97954 81.67560\n4 76.14749 73.64355 78.65142\n\n\n\n\n\nLinear regression with rstanarm\n\nm_bayes &lt;- rstanarm::stan_glm(\n  deaths_vs_pop_per_100k ~ year, \n  data = dt, \n  family = gaussian,\n  iter = 2000,\n  chains = 8,\n  refresh = 0\n)\n\nm_bayes \n\nstan_glm\n family:       gaussian [identity]\n formula:      deaths_vs_pop_per_100k ~ year\n observations: 20\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 6510.2  164.3\nyear          -3.2    0.1\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.2    0.4   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nsims &lt;- as.matrix(m_bayes)\nmedian &lt;- apply(sims, 2, median)\nmedian\nmad_sd &lt;- apply(sims, 2, mad) \n# median absolute deviation (similar to sd)\nmad_sd\n\n\nCredible interval\n\n# credible interval about the fit\ncred &lt;- rstanarm::posterior_interval(m_bayes, prob = 0.95)\ncred\n\n                   2.5%       97.5%\n(Intercept) 6150.268265 6856.275580\nyear          -3.352702   -3.001280\nsigma          1.618884    3.204802\n\n# equivalent to\nsims &lt;- as.matrix(m_bayes)\napply(sims, 2, quantile, probs = c(0.025, 0.5, 0.975))\n\n       parameters\n        (Intercept)      year    sigma\n  2.5%     6150.268 -3.352702 1.618884\n  50%      6510.156 -3.180501 2.191324\n  97.5%    6856.276 -3.001280 3.204802\n\n\n\n\n\nPoint prediction\n\n# point predict\n# uses median from the posterior sim\ny_point_pred &lt;- predict(m_bayes, newdata = dnew)\ny_point_pred\n\n       1        2        3        4 \n85.68812 82.50853 79.32894 76.14936 \n\na_hat &lt;- coef(m_bayes)[1] # median\nb_hat &lt;- coef(m_bayes)[2] # median\na_hat + b_hat*dnew$year\n\n[1] 85.5449 82.3644 79.1839 76.0034\n\n\n\n\nUncertainty of linear predictor\nThe uncertainty of linear predictor, \\(a + bx\\) is propagated through the uncertainty in \\(a\\) and \\(b\\), respectively. For now the error term is not included.\nrstanarm::posterior_linpred is equivalent to using each pairs of \\(a, b\\) from the posterior distribution to compute the point predictions.\n\n# linear predictor with uncertainty via a's and b's\ny_linpred &lt;- rstanarm::posterior_linpred(m_bayes, newdata = dnew)\nhead(y_linpred)\n\n          \niterations        1        2        3        4\n      [1,] 85.62550 82.43075 79.23600 76.04125\n      [2,] 84.61548 81.30348 77.99149 74.67949\n      [3,] 86.04317 82.84212 79.64108 76.44004\n      [4,] 86.44035 83.32712 80.21388 77.10065\n      [5,] 86.76571 83.67927 80.59283 77.50639\n      [6,] 84.50811 81.22552 77.94292 74.66033\n\n# focus on one year: 2023\nhist(y_linpred[, 4], main = 'Predictions for 2023')\n\n\n\n\n\n\n\nhead(y_linpred[, 4])\n\n[1] 76.04125 74.67949 76.44004 77.10065 77.50639 74.66033\n\ny_linpred_byhand &lt;- sims[,1] + dnew$year[4] * sims[,2]\ny_linpred_byhand[1:6]\n\n[1] 76.04125 74.67949 76.44004 77.10065 77.50639 74.66033\n\n\n\n\nPosterior predictive distribution\nWith PPD, include the additional uncertainty in \\(\\sigma\\). This is equivalent to using the linear predictor from above, plus a random draw from rnorm(1, 0, sigma).\nDue to randomness in the error, we can not get the exact same results. But they should be close enough.\n\n# posterior predictive dist (with sigma)\ny_postpred &lt;- rstanarm::posterior_predict(m_bayes, newdata = dnew)\nhead(y_postpred)\n\n            1        2        3        4\n[1,] 88.41858 84.78954 76.95068 78.02747\n[2,] 85.12834 77.97064 78.59530 76.28800\n[3,] 89.37966 83.29759 80.74936 77.26655\n[4,] 84.08476 80.31791 78.68230 82.60028\n[5,] 85.33956 84.10050 78.01238 79.37299\n[6,] 86.85800 77.55464 77.54367 72.66941\n\n# by hand\n# focus on one year: 2023\nn_sim &lt;- nrow(sims)\ny_postpred_byhand &lt;- y_linpred_byhand + rnorm(n_sim, 0, sims[,3])\npar(mfrow = c(1,2))\nhist(y_postpred_byhand, main = 'PPD for 2023 (linpred + error)')\nhist(y_postpred[,4], main = 'PPD for 2023 (post_predict)')\n\n\n\n\n\n\n\ny_postpred[,4] |&gt; summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  65.74   74.44   76.11   76.14   77.83   87.68 \n\ny_postpred_byhand |&gt; summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  66.57   74.45   76.12   76.14   77.86   86.27"
  },
  {
    "objectID": "method/intervals.html#prediction-for-2020-2023-mortality",
    "href": "method/intervals.html#prediction-for-2020-2023-mortality",
    "title": "Intervals",
    "section": "Prediction for 2020-2023 mortality",
    "text": "Prediction for 2020-2023 mortality\nHere we choose to use rstanarm::posterior_predict().\n\npred_all &lt;- rstanarm::posterior_predict(m_bayes, d)\n\npred_all_quantile &lt;- apply(pred_all, 2, quantile, \n                    probs = c(0.025, 0.5, 0.975)) |&gt; \n  t()  |&gt; \n  as.data.frame()\npred_all_quantile[21:24,]\n\n       2.5%      50%    97.5%\n21 80.65036 85.68474 90.65783\n22 77.33989 82.43464 87.41621\n23 74.22532 79.36985 84.39894\n24 70.88991 76.14560 81.31778\n\n\nWe can compare with the frequentist prediction interval, and we see that they are very close.\n\npredint_linear\n\n       fit      lwr      upr\n1 85.68773 80.83777 90.53770\n2 82.50765 77.59214 87.42316\n3 79.32757 74.34154 84.31360\n4 76.14749 71.08617 81.20880"
  },
  {
    "objectID": "method/intervals.html#visualize-mortality-prediction",
    "href": "method/intervals.html#visualize-mortality-prediction",
    "title": "Intervals",
    "section": "Visualize mortality prediction",
    "text": "Visualize mortality prediction\n\n\nShow the code\npd &lt;- copy(d)\n# head(pd)\npd &lt;- cbind(pd, pred_all_quantile)\n\nsetnames(pd, old = '2.5%', new = 'exp_death_per100k_025')\nsetnames(pd, old = '50%', new = 'exp_death_per100k_50')\nsetnames(pd, old = '97.5%', new = 'exp_death_per100k_975')\n\n# also compute the expected death overall\n\npd[, exp_death_025 := exp_death_per100k_025 * pop_jan1_n/100000]\npd[, exp_death_50 := exp_death_per100k_50 * pop_jan1_n/100000]\npd[, exp_death_975 := exp_death_per100k_975 * pop_jan1_n/100000]\n\npd[, alert := fcase(\n  deaths_vs_pop_per_100k &gt; exp_death_per100k_975, \"Higher than expected\",\n  default = \"Expected\"\n)]\n\npd[, type := fcase(\n  year &lt;= 2019, paste0(\"Baseline (2000-2019)\"),\n  default = \"Pandemic years (2020-2023)\"\n)]\n\n\n# make plot\nq &lt;- ggplot(pd, aes(x = year))\nq &lt;- q + geom_ribbon(mapping = aes(ymin = exp_death_per100k_025, \n                                   ymax = exp_death_per100k_975), \n                     alpha = 0.3)\nq &lt;- q + geom_line(mapping = aes(y = exp_death_per100k_50, lty = type), linewidth = 1)\nq &lt;- q + geom_point(mapping = aes(y = deaths_vs_pop_per_100k, color = alert), size = 3)\nq &lt;- q + geom_vline(xintercept = 2019.5, color = \"red\", lty = 2)\nq &lt;- q + expand_limits(y=0)\nq &lt;- q + scale_y_continuous(\"Number of death per 100k\", expand = expansion(mult = c(0, 0.1)))\nq &lt;- q + scale_x_continuous(\"Year\", breaks = seq(2000, 2023, 2))\nq &lt;- q + scale_linetype_discrete(NULL)\nq &lt;- q + scale_color_brewer(NULL, palette = \"Set1\", direction = -1)\nq &lt;- q + theme_bw()\nq &lt;- q + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15), \n  axis.text.x = element_text(angle = 45, vjust = 0.5) \n)\nq &lt;- q + theme(legend.position = \"bottom\", legend.direction = \"horizontal\")\nq &lt;- q + theme(legend.box = \"horizontal\", legend.margin = margin(2, 2, 2, 2))\nq"
  },
  {
    "objectID": "method/rwd_vendor_engagement.html",
    "href": "method/rwd_vendor_engagement.html",
    "title": "RWD EHR Vendor Engagement",
    "section": "",
    "text": "Vendor engagement (VE) in the context of Real-world data and Electronic Health Records (EHR/EMR) is the collaboration between various healthcare organizations (such as hospitals, clinics and medical practices) and vendors that provide EHR systems. VE is essential for successful EHR implementation and ongoing use in healthcare organizations. This partnership ensures that EHR systems meet the needs of healthcare providers and improves patient care.\nA few EHR vendors:\n\nEpicCare\nOracle Health EHR\nathenahealth\neClinicalWorks"
  },
  {
    "objectID": "method/rwd_vendor_engagement.html#what-is-vendor-engagement",
    "href": "method/rwd_vendor_engagement.html#what-is-vendor-engagement",
    "title": "RWD EHR Vendor Engagement",
    "section": "",
    "text": "Vendor engagement (VE) in the context of Real-world data and Electronic Health Records (EHR/EMR) is the collaboration between various healthcare organizations (such as hospitals, clinics and medical practices) and vendors that provide EHR systems. VE is essential for successful EHR implementation and ongoing use in healthcare organizations. This partnership ensures that EHR systems meet the needs of healthcare providers and improves patient care.\nA few EHR vendors:\n\nEpicCare\nOracle Health EHR\nathenahealth\neClinicalWorks"
  },
  {
    "objectID": "method/rwd_vendor_engagement.html#implemention",
    "href": "method/rwd_vendor_engagement.html#implemention",
    "title": "RWD EHR Vendor Engagement",
    "section": "Implemention",
    "text": "Implemention\n\nSelection process\nHealthcare organisations engage with EHR vendors to select EHR systems that best fits their needs. This involves evaluating the products, features, pricing and support services.\n\n\nImplementation\nThis process involves data migration, software installation, customization to meet specific workflow requirements, and staff training. The customization could be to support specialized clinical workflows, integrate with other software solutions or implementing additional features and modules.\n\n\nAfter implementation\nVE continues after the EHR system is implemented. Healthcare organizations rely on vendors for ongoing technical support, software updates and maintenance to ensure that the systems function smoothly and remains compliant with regulatory requirements.\nFeedback and collaboration between healthcare organizations and the vendor is crucial. Feedbacks on usability, functionality and performance of the EHR system can inform future updates and improvements for the vendor.\n\n\nRegulation and compliance\nRegulatory compliance in the context of electronic health records (EHRs) typically includes adherence to laws and regulations aimed at protecting patient privacy, ensuring data security, and promoting the interoperability of health information. Some of the key regulations that healthcare organizations and EHR vendors need to comply with include:\nHealth Insurance Portability and Accountability Act (HIPAA): HIPAA sets the standard for protecting sensitive patient data. It includes the Privacy Rule, which governs the use and disclosure of protected health information (PHI), and the Security Rule, which outlines security standards for protecting electronic PHI (ePHI). HIPAA also includes the Breach Notification Rule, which requires covered entities to notify individuals affected by breaches of their PHI.\nHITECH Act: The Health Information Technology for Economic and Clinical Health (HITECH) Act promotes the adoption and meaningful use of health information technology, including EHRs. It introduced incentives for healthcare providers to adopt EHRs and strengthened HIPAA’s privacy and security requirements, including increased penalties for non-compliance.\nMeaningful Use (now Promoting Interoperability) Program: The Centers for Medicare & Medicaid Services (CMS) previously administered the Meaningful Use program, which incentivized eligible healthcare providers to adopt and demonstrate meaningful use of certified EHR technology. This program has transitioned to the Promoting Interoperability program, which focuses on promoting the exchange of health information and improving interoperability.\nFDA Regulations: The U.S. Food and Drug Administration (FDA) regulates certain types of EHR software that meet the definition of a medical device. EHR vendors must comply with FDA regulations, particularly if their software includes features that are considered medical devices."
  },
  {
    "objectID": "method/rct_design_overview.html",
    "href": "method/rct_design_overview.html",
    "title": "Clinical trial design: overview",
    "section": "",
    "text": "Considerations: choice of comparator and trial outcome measures (due to the grrowing number of treatmenet options, standard care would change over time), annd definition of target patient population (e.g. molecular profiling makes it possible to identify smaller subgroups of patient with defined tumor type)."
  },
  {
    "objectID": "method/rct_design_overview.html#intention-to-treat-itt",
    "href": "method/rct_design_overview.html#intention-to-treat-itt",
    "title": "Clinical trial design: overview",
    "section": "Intention to treat ITT",
    "text": "Intention to treat ITT\nCross-overs after rz: some patients might have a treatment (yes or no) beyond what they were assigned to, e.g. refuse surgery or medical treatment\nNon-adherence during followup: some in treatment group refuse of can not tolerate certain treatment; while some in placebo group require medication or take on their own"
  },
  {
    "objectID": "method/rct_design_overview.html#subgroup-analysis",
    "href": "method/rct_design_overview.html#subgroup-analysis",
    "title": "Clinical trial design: overview",
    "section": "Subgroup analysis",
    "text": "Subgroup analysis\nStratified analysis: estimate treatment effect separately in subgroups. Does not tell difference across different subgroups\nTest for interaction: use of main effect and interaction.\nIssue of multiple testing when doing a series of analyses\n\ninflate sample size to plan for subgroup analysis\nreport number of subgroup analyses performed\npossibly adjust for multiple comparisons\nreport CI instead of just p-values\n\n\nResources\n\nCoursera course Design and interpretation of clinical trials by Johns Hopkins University\nBook Fast Facts: Clinical trialss inn oncology: The fundamentals"
  },
  {
    "objectID": "method/survival.html",
    "href": "method/survival.html",
    "title": "Survival",
    "section": "",
    "text": "Links\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html\nhttps://www.danieldsjoberg.com/ggsurvfit/\njmpost: combines survival analysis, mixed effect model https://genentech.github.io/jmpost/main/"
  },
  {
    "objectID": "method/weighting.html",
    "href": "method/weighting.html",
    "title": "General techniques",
    "section": "",
    "text": "Exact matching:\n\nperfect covariate balance; \\(F(X_i|T_i = 1) = F(X_i|T_i=0)\\)\ninfeasible when covariate is continuous, and when there are many covariates.\n\nProbability of receiving treatment, \\(\\pi(X_i) = P(T_i = 1 | X_i)\\)\nMatching based on distance measures\n\nMahalanobis distance\nEstimated propensity score, \\(D(X_i, X_j) = |P(T_i = 1|X_i) - P(T_j=1 | X_j)|\\)\n\nCheck covariate balance\n\nideally compare joint distribution of all covariates\npractically check lower-dimensional summaries (standardized mean difference, variance ratio, empirical CDF difference)\n\nBalance test\nMatching would reduce number of observations\nMatchIt package\nPS matching is ONE of the many matching techniques that uses PS as the difference."
  },
  {
    "objectID": "method/weighting.html#matching",
    "href": "method/weighting.html#matching",
    "title": "General techniques",
    "section": "",
    "text": "Exact matching:\n\nperfect covariate balance; \\(F(X_i|T_i = 1) = F(X_i|T_i=0)\\)\ninfeasible when covariate is continuous, and when there are many covariates.\n\nProbability of receiving treatment, \\(\\pi(X_i) = P(T_i = 1 | X_i)\\)\nMatching based on distance measures\n\nMahalanobis distance\nEstimated propensity score, \\(D(X_i, X_j) = |P(T_i = 1|X_i) - P(T_j=1 | X_j)|\\)\n\nCheck covariate balance\n\nideally compare joint distribution of all covariates\npractically check lower-dimensional summaries (standardized mean difference, variance ratio, empirical CDF difference)\n\nBalance test\nMatching would reduce number of observations\nMatchIt package\nPS matching is ONE of the many matching techniques that uses PS as the difference."
  },
  {
    "objectID": "method/weighting.html#weighting",
    "href": "method/weighting.html#weighting",
    "title": "General techniques",
    "section": "Weighting",
    "text": "Weighting\nWeighting can be viewed as a generalization of matching\nUsed in weighting\nATE (population) \\(w_{ATE} = \\frac{Z_i}{p_i} + \\frac{1-Z_i}{1-p_i}\\)\nATT \\(w_{ATT} = \\frac{p_i Z_i}{p_i} + \\frac{p_i(1-Z_i)}{1-p_i}\\)\nATC \\(w_{ATC} = \\frac{(1 - p_i) Z_i}{p_i} + \\frac{(1 - p_i)(1-Z_i)}{1-p_i}\\)\n\nInverse probability weighting IPW\n(In my use case it is for survey sampling, rather than causal inference. My project does not have treatment group, and do not care about treatment effects. )"
  },
  {
    "objectID": "method/weighting.html#propensity-score",
    "href": "method/weighting.html#propensity-score",
    "title": "General techniques",
    "section": "Propensity score",
    "text": "Propensity score\n(Rosenbaum and Rubin, 1983)\n\nin observational studies, conditioning on propensity scores can lead to unbiased estimates of the exposure effect\ngiven that there are no unmeasured confounders\nevery subject has a non-zero probability of receiving exposure\n\nFit a logistic regression:\n\nbinary outcome: exposure (1,0)\ncovariates: all but exposure\n\nPredict the values (probability), they are the propensity scores.\nPS can also be estimated using other methods that produce probabilities, not just logistic regression: random forest, lasso logistic regression etc.\nLinks\nhttps://stats.stackexchange.com/questions/492218/should-the-choice-of-propensity-score-matching-versus-weighting-depend-on-the-de\nhttps://stats.stackexchange.com/questions/553853/understanding-propensity-score-matching?rq=1\nhttps://aetion.com/evidence-hub/understanding-propensity-score-weighting-methods-rwe/"
  },
  {
    "objectID": "method/covariate_adjustment.html",
    "href": "method/covariate_adjustment.html",
    "title": "Covariate adjustment",
    "section": "",
    "text": "https://jbetz-jhu.github.io/CovariateAdjustmentTutorial/\nBaseline covariates are variables measured prior to randomization, expected to have strong association with outcome. Potential confounding occurs when the distribution of baseline covariates between treatment groups are imbalanced.\nTo address confounding:\n\ndesign stage: stratified randomization to reduce imbalance\nanalysis stage: covariate adjustment\n\nBenefits:\n\nreduce sample size\nimprove precision - smaller CI, higher power\nsome CA do not depend on a correctly specified model\n\nRecent FDA guidance requires distinction between conditional and marginal treatment effects. These two coincide in linear models, but not in non-linear models (e.g. binary, ordinal, count, time-to-event outcomes)."
  },
  {
    "objectID": "method/case_study_ctn51.html",
    "href": "method/case_study_ctn51.html",
    "title": "Case study: CTN-51",
    "section": "",
    "text": "CTN 51 (Lee 2018): Bup/Nx vs XR-NTX naltrexone, effectiveness of pharmacotheerapeutic aids to recovery who completed detoxification and achieved short term abstinence. n=570\n\nXR-NTX (n=283): induction hurdle, higher rate of 24-week relapse, fewer successfully initiated XR-NTX (204 of 283) than Bup/Nx (270 of 287)\nfor those inducted successfully, relapse rate is similar\nrelapse: 4 consecutive weeks of any non-study opioid use (UDS or report), or 7 consecutive days of self-reported use.\n\nReproduce some of the results, try to understand the events involved after initiation\nKey numbers\n\nrelapse is 4 consecutive weeks of non-study opioid use or 7 days of self-reported use\nITT (n=570), 24 weeks relapse greater for NTX\npre-protocol (n=474), similar relapse\nrelapse rate is between 57% to 65%\n\nThings to understand\n\ninitiation hurdle"
  },
  {
    "objectID": "method/case_study_ctn51.html#study-information",
    "href": "method/case_study_ctn51.html#study-information",
    "title": "Case study: CTN-51",
    "section": "",
    "text": "CTN 51 (Lee 2018): Bup/Nx vs XR-NTX naltrexone, effectiveness of pharmacotheerapeutic aids to recovery who completed detoxification and achieved short term abstinence. n=570\n\nXR-NTX (n=283): induction hurdle, higher rate of 24-week relapse, fewer successfully initiated XR-NTX (204 of 283) than Bup/Nx (270 of 287)\nfor those inducted successfully, relapse rate is similar\nrelapse: 4 consecutive weeks of any non-study opioid use (UDS or report), or 7 consecutive days of self-reported use.\n\nReproduce some of the results, try to understand the events involved after initiation\nKey numbers\n\nrelapse is 4 consecutive weeks of non-study opioid use or 7 days of self-reported use\nITT (n=570), 24 weeks relapse greater for NTX\npre-protocol (n=474), similar relapse\nrelapse rate is between 57% to 65%\n\nThings to understand\n\ninitiation hurdle"
  },
  {
    "objectID": "method/case_study_ctn51.html#datasets-used",
    "href": "method/case_study_ctn51.html#datasets-used",
    "title": "Case study: CTN-51",
    "section": "Datasets used",
    "text": "Datasets used\n\nlibrary(public.ctn0094data)\nlibrary(public.ctn0094extra)\nlibrary(CTNote)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(stringr)\n\nout &lt;- CTNote::outcomesCTN0094\n\n# take c51 \n\neverybody$project |&gt; table() # this includes all eligible\n\n\n  27   30   51 \n1920  868  772 \n\nout_c51 &lt;- left_join(everybody, out) |&gt; \n  filter(project == 51)\n\nJoining with `by = join_by(who)`\n\nout_c51 |&gt; head()\n\n# A tibble: 6 × 64\n    who project usePatternUDS             fiellin2006_abs kosten1993_isAbs\n  &lt;dbl&gt; &lt;fct&gt;   &lt;chr&gt;                               &lt;dbl&gt; &lt;lgl&gt;           \n1     3 51      o-ooo-ooooooooooooooooo                 2 FALSE           \n2     4 51      -------------------o-o-o               21 TRUE            \n3     7 51      ----oooooooooooooooooooo                4 TRUE            \n4     9 51      oooooooooooooooooooooo                  0 FALSE           \n5    18 51      ooooooooooooooooooooooooo               0 FALSE           \n6    20 51      ooooooooooooooooooooooooo               0 FALSE           \n# ℹ 59 more variables: krupitsky2011A_isAbs &lt;lgl&gt;, krupitsky2011B_abs &lt;dbl&gt;,\n#   ling1998_isAbs &lt;lgl&gt;, lofwall2018_isAbs &lt;lgl&gt;, mokri2016_abs_time &lt;dbl&gt;,\n#   mokri2016_abs_event &lt;dbl&gt;, schottenfeld2005_abs &lt;dbl&gt;,\n#   schottenfeld2008A_abs_time &lt;dbl&gt;, schottenfeld2008A_abs_event &lt;dbl&gt;,\n#   schottenfeld2008B_abs &lt;dbl&gt;, shufman1994_absN_time &lt;dbl&gt;,\n#   shufman1994_absN_event &lt;dbl&gt;, weissLingCTN0030_isAbs &lt;lgl&gt;,\n#   comer2006_red &lt;dbl&gt;, eissenberg1997_isAbs &lt;lgl&gt;, fiellin2006_red &lt;dbl&gt;, …\n\n\n\nTreatment and outcome related\nOutcome includes pattern and class\nRandomization\ntreatment: time is study day, amount is dose or 1 (injection)"
  },
  {
    "objectID": "method/imputation_1_overview.html",
    "href": "method/imputation_1_overview.html",
    "title": "Missing data and imputation",
    "section": "",
    "text": "Missing data can occur in various situations:\n\nunit non-response: individuals decline to participate\nitem non-response: some questions or measurements left out by participating individuals\nloss of follow-up: drop outs\n\nFor unit non-response and loss of follow-up, weighting the data could help. Need to know the response rate, characteristics of the non-responders and how the respondent differ.\nSolutions (to item non-response)\n\ncomplete case analysis (listwise deletion)\nsingle imputation\n\nmean imputation\nconditional imputation: make a prediction, plus minus some noise\n\nmultiple imputation: repeat single imputation M times, produce M complete datasets, use Rubin’s rules to pool the estimates.\n\n\n\nThree categories of missing mechanism. Example: disease status, level of exposure, age. Exposure is missing for some subjects.\n\nMCAR: any two individuals have the same probability of having missing value for exposure. This is unlikely.\nMAR: any two individuaals with the same disease status and age have the same chance of having the exposure missing\nMNAR: chance of having missing exposure depends on the value of exposure\n\nData could also have special missing structures: multi-level, longitudinal and repeated measurements. In RCT a specific technique reference based imputation can be applied."
  },
  {
    "objectID": "method/imputation_1_overview.html#overview",
    "href": "method/imputation_1_overview.html#overview",
    "title": "Missing data and imputation",
    "section": "",
    "text": "Missing data can occur in various situations:\n\nunit non-response: individuals decline to participate\nitem non-response: some questions or measurements left out by participating individuals\nloss of follow-up: drop outs\n\nFor unit non-response and loss of follow-up, weighting the data could help. Need to know the response rate, characteristics of the non-responders and how the respondent differ.\nSolutions (to item non-response)\n\ncomplete case analysis (listwise deletion)\nsingle imputation\n\nmean imputation\nconditional imputation: make a prediction, plus minus some noise\n\nmultiple imputation: repeat single imputation M times, produce M complete datasets, use Rubin’s rules to pool the estimates.\n\n\n\nThree categories of missing mechanism. Example: disease status, level of exposure, age. Exposure is missing for some subjects.\n\nMCAR: any two individuals have the same probability of having missing value for exposure. This is unlikely.\nMAR: any two individuaals with the same disease status and age have the same chance of having the exposure missing\nMNAR: chance of having missing exposure depends on the value of exposure\n\nData could also have special missing structures: multi-level, longitudinal and repeated measurements. In RCT a specific technique reference based imputation can be applied."
  },
  {
    "objectID": "method/imputation_1_overview.html#consideration",
    "href": "method/imputation_1_overview.html#consideration",
    "title": "Missing data and imputation",
    "section": "Consideration",
    "text": "Consideration\nNeed to account for the missinng data process, preserve the relations in the data and uncertainty in the relations.\n\nMAR assumption whether plausible. (FCS can handle both MAR anad MNAR)\nform of imputation model: structure and error distribution\npredictors, as many relevant as possible, including interactions\nthe order in which to impute\nset up starting imputation and number of iteration\ndecide number of imputed datasets"
  },
  {
    "objectID": "method/survey.html",
    "href": "method/survey.html",
    "title": "Survey, stratification",
    "section": "",
    "text": "Survey sampling is a bit different from RCT"
  },
  {
    "objectID": "method/survey.html#terms",
    "href": "method/survey.html#terms",
    "title": "Survey, stratification",
    "section": "Terms",
    "text": "Terms\n\nWeights\nProbability weight: inverse of probability of being included.\n\nN/n\nN: number of elements in the population; n: in the sample\nif population has 10 and 3 sampled at random, probability weight is 10/3 = 3.33\n\nSampling weight: a probability weight that could have other corrections (e.g. unit non-response, calibration, trimming etc)\n\n\nStrata\nStratification breaks up the population into groups. Each element in the population must belong to only one strata.\nTypically you need two or more PSU in each stratum.\nPurpose of stratification: reduce standard error of the estimates\nPSU: Primary sampling unit\n\n\nPost-stratification\nStratify data AFTER data is collected, to ensure data is representative of the target population.\nExamples\n\nmale n=20, y1 = 180\nfemale n=80, y2 = 120\noverall mean: 132, from \\((20*180 + 80*120)/100\\)\n\nThis would be underestimating due to over-representation of female.\nAdjustment:\n\nin the population, proportion is 0.5 and 0.5\nmean would be \\(\\bar{y_{st}} = 0.5*180 + 0.5*120 = 150\\)\n\nThis is the post stratification estimator.\n\n\nCalibration\nUse inverse probability weights to adjust sample.\n\n\nHorvitz-Thompson estimator\nA method to estimate mean of population in a stratified sample, by applying IPW to account for the difference in sampling distribution between the collected data and target population."
  },
  {
    "objectID": "method/survey.html#horvitz-thompson-estimator",
    "href": "method/survey.html#horvitz-thompson-estimator",
    "title": "Survey, stratification",
    "section": "Horvitz-Thompson estimator",
    "text": "Horvitz-Thompson estimator\nA method to estimate mean of population in a stratified sample, by applying IPW to account for the difference in sampling distribution between the collected data and target population."
  },
  {
    "objectID": "method/survey.html#inverse-probability-weighting",
    "href": "method/survey.html#inverse-probability-weighting",
    "title": "Survey, stratification",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\nCrude example of how it would affect the mean estimate\n\ndf &lt;- data.frame(\n  v = c(100, 100, 200), # values\n  w = c(1, 1, 1), \n  popw = c(0.35, 0.5, 0.15), # assume population proportion\n  invw = c(1/0.35, 1/0.5, 1/0.15), \n  w2 = c(0.3, 0.2, 0.5), # observed prop \n  invw2 = c(0.3/0.35, 0.2/0.5, 0.5/0.15) # use obs/pop\n)\n\ndf\n\n    v w popw     invw  w2     invw2\n1 100 1 0.35 2.857143 0.3 0.8571429\n2 100 1 0.50 2.000000 0.2 0.4000000\n3 200 1 0.15 6.666667 0.5 3.3333333\n\ncrude_mean &lt;- mean(df$v)\ncrude_mean\n\n[1] 133.3333\n\nweighted.mean(df$v, w = df$w)\n\n[1] 133.3333\n\n# use inverse probability\nweighted.mean(df$v, w = df$invw)\n\n[1] 157.8512\n\n(sum(df$v * 1/df$popw))/(sum(1/df$popw))\n\n[1] 157.8512\n\n# mean 2, need to use weight\nweighted.mean(df$v, w = df$w2)\n\n[1] 150\n\n# different obs prop\nweighted.mean(df$v, w = df$invw2)\n\n[1] 172.6141"
  },
  {
    "objectID": "method/estimands_ice.html",
    "href": "method/estimands_ice.html",
    "title": "Estimands, intercurrent events",
    "section": "",
    "text": "Key question: how to present the data visually?\nWhat are considered as important, what are not?\nITT (intention to treat): include the data after rescue medicine. Includes dropouts\nPP (Per-protocol): exclude patients taking rescue medicine, only analyse the complete cases\nTrial estimands: trial treatment effect depends on how events occur after treatment initiation.\nTreatment effect for a given outcome.\nFive core attributes\n\npopulation\ntreatment conditions\nendpoint\nsummary measure\nstrategies to handle each type of intercurrent event\n\n\nIntercurrent event\nPost-baseline events (post randomisation in RCT) that affects the interpretation of outcome.\nTwo categories:\n\ntreatment-modifying events, affects the receipt of assigned treatment. E.g. early discontinuation, use of rescue, wrong dose, wrong type (placebo for example).\ntruncating events, e.g. death, amputation of limb when the limb is relevant for the research question.\n\nStrategies (multiple can be used for different ICE in the same study)\n\ntreatment policy strategy: treat as it is, ignore ICE\ncomposite strategy: modifies the endpoint value, defined by the investigator\nwhile-on-treatment (while alive): before intercurrent event data is used.\nhypothetical strategy\nprincipal stratum strategy: redefine the population"
  },
  {
    "objectID": "method/imputation_2.html",
    "href": "method/imputation_2.html",
    "title": "Multiple imputation in R",
    "section": "",
    "text": "Here I use the small dataset nhanes included in mice package. It has 25 rows, and three out of four variables have missings.\nThe original NHANES data is a large national level survey, some are publicly available via R package nhanes.\nlibrary(mice)\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n# load example dataset from mice\nhead(nhanes)\n\n  age  bmi hyp chl\n1   1   NA  NA  NA\n2   2 22.7   1 187\n3   1   NA   1 187\n4   3   NA  NA  NA\n5   1 20.4   1 113\n6   3   NA  NA 184\n\nsummary(nhanes)\n\n      age            bmi             hyp             chl       \n Min.   :1.00   Min.   :20.40   Min.   :1.000   Min.   :113.0  \n 1st Qu.:1.00   1st Qu.:22.65   1st Qu.:1.000   1st Qu.:185.0  \n Median :2.00   Median :26.75   Median :1.000   Median :187.0  \n Mean   :1.76   Mean   :26.56   Mean   :1.235   Mean   :191.4  \n 3rd Qu.:2.00   3rd Qu.:28.93   3rd Qu.:1.000   3rd Qu.:212.0  \n Max.   :3.00   Max.   :35.30   Max.   :2.000   Max.   :284.0  \n                NA's   :9       NA's   :8       NA's   :10\nExamine missing pattern with md.pattern(data).\n# 27 missing in total\n# by col: 8 for hyp, 9 for bmi, 10 for chl\n# by row: n missing numbers\n\nmd.pattern(nhanes)\n\n\n\n\n\n\n\n\n   age hyp bmi chl   \n13   1   1   1   1  0\n3    1   1   1   0  1\n1    1   1   0   1  1\n1    1   0   0   1  2\n7    1   0   0   0  3\n     0   8   9  10 27"
  },
  {
    "objectID": "method/imputation_2.html#imputation-without-model",
    "href": "method/imputation_2.html#imputation-without-model",
    "title": "Multiple imputation in R",
    "section": "Imputation without model",
    "text": "Imputation without model\n\nImpute with mean\n\n# only run once, since it's just the mean\nimp &lt;- mice(data = nhanes, \n            method = 'mean', \n            m = 1, \n            maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimp # this is not the imputed value\n\nClass: mids\nNumber of multiple imputations:  1 \nImputation methods:\n   age    bmi    hyp    chl \n    \"\" \"mean\" \"mean\" \"mean\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n# check imputed values for bmi: same as mean(nhanes$bmi, na.rm=T)\nimp$imp$bmi\n\n         1\n1  26.5625\n3  26.5625\n4  26.5625\n6  26.5625\n10 26.5625\n11 26.5625\n12 26.5625\n16 26.5625\n21 26.5625\n\n# impute the dataset\nnhanes_imp &lt;- complete(imp)\nnhanes_imp\n\n   age     bmi      hyp   chl\n1    1 26.5625 1.235294 191.4\n2    2 22.7000 1.000000 187.0\n3    1 26.5625 1.000000 187.0\n4    3 26.5625 1.235294 191.4\n5    1 20.4000 1.000000 113.0\n6    3 26.5625 1.235294 184.0\n7    1 22.5000 1.000000 118.0\n8    1 30.1000 1.000000 187.0\n9    2 22.0000 1.000000 238.0\n10   2 26.5625 1.235294 191.4\n11   1 26.5625 1.235294 191.4\n12   2 26.5625 1.235294 191.4\n13   3 21.7000 1.000000 206.0\n14   2 28.7000 2.000000 204.0\n15   1 29.6000 1.000000 191.4\n16   1 26.5625 1.235294 191.4\n17   3 27.2000 2.000000 284.0\n18   2 26.3000 2.000000 199.0\n19   1 35.3000 1.000000 218.0\n20   3 25.5000 2.000000 191.4\n21   1 26.5625 1.235294 191.4\n22   1 33.2000 1.000000 229.0\n23   1 27.5000 1.000000 131.0\n24   3 24.9000 1.000000 191.4\n25   2 27.4000 1.000000 186.0\n\n\n\n\nImpute by sampling\n\nimps &lt;- mice(data = nhanes, \n            method = 'sample', \n            m = 1, \n            maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimps$imp$bmi\n\n      1\n1  27.4\n3  22.0\n4  27.5\n6  27.2\n10 22.0\n11 25.5\n12 27.2\n16 24.9\n21 22.0"
  },
  {
    "objectID": "method/imputation_2.html#imputation-with-regression",
    "href": "method/imputation_2.html#imputation-with-regression",
    "title": "Multiple imputation in R",
    "section": "Imputation with regression",
    "text": "Imputation with regression\nRegression methods (continuous, normal outcome) are implemented in mice with methods starting with norm.\n\nLinear regression without parameter uncertainty, mice.impute.norm.nob\nLinear regression through prediction, mice.impute.norm.predict\nBayesian linear regression, mice.impute.norm\nLinear regression bootstrap, mice.impute.norm.boot\n\n\nExample: Regression without parameter uncertainty\nWe can generate two imputed datasets by setting m=2.\nThere is a certain level of randomness, so would be a good idea to set seed.\n\nset.seed(1)\nimpr0 &lt;- mice(nhanes, method = 'norm.nob', m=2, maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  1   2  bmi  hyp  chl\n\nimpr0\n\nClass: mids\nNumber of multiple imputations:  2 \nImputation methods:\n       age        bmi        hyp        chl \n        \"\" \"norm.nob\" \"norm.nob\" \"norm.nob\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\nnhanes_impr0 &lt;- complete(impr0) # by default, returns the first imputation\nnhanes_impr0\n\n   age      bmi       hyp      chl\n1    1 35.53430 1.2503841 256.6153\n2    2 22.70000 1.0000000 187.0000\n3    1 27.31412 1.0000000 187.0000\n4    3 25.31243 2.3880837 267.1435\n5    1 20.40000 1.0000000 113.0000\n6    3 17.94547 1.5855064 184.0000\n7    1 22.50000 1.0000000 118.0000\n8    1 30.10000 1.0000000 187.0000\n9    2 22.00000 1.0000000 238.0000\n10   2 26.99782 1.0810473 206.9927\n11   1 32.71511 0.7819353 213.7222\n12   2 27.65399 0.7904680 209.6716\n13   3 21.70000 1.0000000 206.0000\n14   2 28.70000 2.0000000 204.0000\n15   1 29.60000 1.0000000 252.1596\n16   1 27.47980 0.6071353 145.9557\n17   3 27.20000 2.0000000 284.0000\n18   2 26.30000 2.0000000 199.0000\n19   1 35.30000 1.0000000 218.0000\n20   3 25.50000 2.0000000 245.7884\n21   1 35.12809 0.5807116 232.4652\n22   1 33.20000 1.0000000 229.0000\n23   1 27.50000 1.0000000 131.0000\n24   3 24.90000 1.0000000 268.3929\n25   2 27.40000 1.0000000 186.0000\n\n\nWhen we have two imputed datasets, we can check the values for each of the variables. For example, extract bmi variable from the imputed data imp,\n\n# two imputed datasets (m=2)\nimpr0$imp$bmi\n\n          1        2\n1  35.53430 32.26078\n3  27.31412 22.55473\n4  25.31243 14.90410\n6  17.94547 22.59196\n10 26.99782 25.08534\n11 32.71511 27.71485\n12 27.65399 25.76286\n16 27.47980 30.34985\n21 35.12809 29.89142\n\n\nWe can also specify which imputed dataset to use as our complete data. Set index to 0 (action = 0) returns the original dataset with missing values.\nHere we check which of the imputed data is being used as the completed dataset. First take a note of the row IDs (based on bmi, for example). Then we generate completed dataset.\n\nif no action argument is set, then it returns the first imputation by default\naction=0 corresponds to the original data with missing values\n\n\n# check which imputed data is used for the final result, take note of row id\nid_missing &lt;- which(is.na(nhanes$bmi))\nid_missing\n\n[1]  1  3  4  6 10 11 12 16 21\n\nnhanes_impr0_action0 &lt;- complete(impr0, action = 0) \nnhanes_impr0_action0[id_missing, ] # original data with missing bmi\n\n   age bmi hyp chl\n1    1  NA  NA  NA\n3    1  NA   1 187\n4    3  NA  NA  NA\n6    3  NA  NA 184\n10   2  NA  NA  NA\n11   1  NA  NA  NA\n12   2  NA  NA  NA\n16   1  NA  NA  NA\n21   1  NA  NA  NA\n\nnhanes_impr0_action1 &lt;- complete(impr0, action = 1) \nnhanes_impr0_action1[id_missing, ] # using first imputation\n\n   age      bmi       hyp      chl\n1    1 35.53430 1.2503841 256.6153\n3    1 27.31412 1.0000000 187.0000\n4    3 25.31243 2.3880837 267.1435\n6    3 17.94547 1.5855064 184.0000\n10   2 26.99782 1.0810473 206.9927\n11   1 32.71511 0.7819353 213.7222\n12   2 27.65399 0.7904680 209.6716\n16   1 27.47980 0.6071353 145.9557\n21   1 35.12809 0.5807116 232.4652\n\nnhanes_impr0_action2 &lt;- complete(impr0, action = 2) \nnhanes_impr0_action2[id_missing, ] # using second imputation\n\n   age      bmi       hyp      chl\n1    1 32.26078 0.4616324 228.0022\n3    1 22.55473 1.0000000 187.0000\n4    3 14.90410 1.4558818 212.7958\n6    3 22.59196 1.7664882 184.0000\n10   2 25.08534 1.2940549 201.5872\n11   1 27.71485 0.9410698 169.2427\n12   2 25.76286 1.3570093 168.5961\n16   1 30.34985 0.6878971 163.7262\n21   1 29.89142 1.0452062 212.9144\n\n\n\n\nOther imputation by linear regression\nOther various of imputaton via linear regression can be implemented simply by changing the method argument.\n\nLinear regression through prediction, mice.impute.norm.predict\nBayesian linear regression, mice.impute.norm\nLinear regression bootstrap, mice.impute.norm.boot\n\n\nimpr &lt;- mice(nhanes, method = 'norm.predict', m=1, maxit=1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimpr$imp$bmi\n\n          1\n1  28.33396\n3  28.33396\n4  22.75613\n6  21.17519\n10 27.19573\n11 29.12443\n12 26.26576\n16 30.28688\n21 28.33396\n\n\nBayesian linear regression\n\nimpb &lt;- mice(nhanes, method = 'norm', m=1, maxit=1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimpb$imp$bmi\n\n          1\n1  33.82959\n3  28.98754\n4  20.88810\n6  19.11391\n10 27.32990\n11 29.44117\n12 22.68062\n16 32.13267\n21 22.03164\n\n# nhanes_impb &lt;- complete(impb)\n\nBootstrap\n\nimpbt &lt;- mice(nhanes, method = 'norm.boot', m=1, maxit=1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimpbt$imp$bmi\n\n          1\n1  24.19248\n3  28.77464\n4  22.42321\n6  23.47542\n10 21.95529\n11 23.12703\n12 25.84230\n16 27.68216\n21 26.43770"
  },
  {
    "objectID": "method/imputation_2.html#predictive-mean-matching-pmm",
    "href": "method/imputation_2.html#predictive-mean-matching-pmm",
    "title": "Multiple imputation in R",
    "section": "Predictive Mean Matching PMM",
    "text": "Predictive Mean Matching PMM\nThe idea behind PMM is as follow.\n\nwith complete data, estimate a linear regression of Y (some missing) on Z (no missing), results in coefficients \\(\\beta\\).\ndraw \\(\\beta^*\\) from the posterior predictive distribution of \\(\\beta\\) (multivariate normal with mean b and covariance matrix of b).\ngenerate predicted values for \\(Y_{hat}\\) (complete cases) and \\(Y_{star}\\) (missing)\nfor each \\(Y_{star}\\), identify a few cases (7,12) whose predicted values \\(Y_{hat}\\) are close to the predicted \\(Y_{star}\\) (10 in the illustration below)\nrandomly draw one value from the observed \\(Y\\) from the doner cases (6, 11).\n\n\n\n\n\n\nAssumption for PMM: distribution of missing is the same as obsereved data of the candidates that produce the closest values to the predicted value by the missing entry.\nPMM is robust to transformation, less vulnerable to model misspecification.\nImplementation in mice:\n\nPredictive mean matching, mice.impute.pmm\nWeighted predictive mean matching, mice.impute.midastouch\nMultivariate predictive mean matching, mice.impute.mpmm\n\n\nimp_pmm &lt;- mice(nhanes, method = 'pmm', m=1, maxit=10)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n  6   1  bmi  hyp  chl\n  7   1  bmi  hyp  chl\n  8   1  bmi  hyp  chl\n  9   1  bmi  hyp  chl\n  10   1  bmi  hyp  chl\n\nimp_pmm\n\nClass: mids\nNumber of multiple imputations:  1 \nImputation methods:\n  age   bmi   hyp   chl \n   \"\" \"pmm\" \"pmm\" \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n# imputations for bmi\nimp_pmm$imp$bmi\n\n      1\n1  35.3\n3  27.2\n4  27.4\n6  22.5\n10 26.3\n11 22.5\n12 26.3\n16 33.2\n21 35.3\n\n\n\nimp_pmms &lt;- mice(nhanes, method = 'midastouch', m=1, maxit=10)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n  6   1  bmi  hyp  chl\n  7   1  bmi  hyp  chl\n  8   1  bmi  hyp  chl\n  9   1  bmi  hyp  chl\n  10   1  bmi  hyp  chl\n\nimp_pmm\n\nClass: mids\nNumber of multiple imputations:  1 \nImputation methods:\n  age   bmi   hyp   chl \n   \"\" \"pmm\" \"pmm\" \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\nimp_pmms$imp$bmi\n\n      1\n1  22.5\n3  30.1\n4  27.2\n6  27.4\n10 27.4\n11 30.1\n12 28.7\n16 22.5\n21 27.2"
  },
  {
    "objectID": "method/case_study_liggetid_1.html",
    "href": "method/case_study_liggetid_1.html",
    "title": "Length of hospital stay: Part I",
    "section": "",
    "text": "The data liggetid was collected at the Geriatric Department at Ullevål Sykehus. This dataset has been used for teaching at University of Oslo, MF9130E. The course material can be accessed here.\nWe will focus on the following variables:\nlos &lt;- readRDS('data/los.rds')\nhead(los, 3)\n\n  admission_year age    sex admission_from stroke los\n1           1987  81 female           home      0  13\n2           1987  96 female           home      0  17\n3           1987  79 female           home      0   6\nSome variables need to be recoded and put into factor to display nicely.\nShow the code\n# library(dplyr)\nlibrary(ggplot2)\n\n# remove NA\nlos &lt;- dplyr::filter(los, !is.na(sex) & !is.na(stroke) & !is.na(admission_from))\n\n# code admission from with text\n# unique(los$admission_from)\nlos$admission_from &lt;- factor(los$admission_from, \n                            levels = c('home', 'div_surgery', \n                                       'div_medicine', 'div_other', \n                                       'other_hospital', 'nursing_home'), \n                            labels = c('home', 'div_surgery', \n                                       'div_medicine', 'div_other', \n                                       'other_hospital', 'nursing_home'))\n\n# code admission year with text\nlos$admission_year &lt;- factor(los$admission_year,\n                            levels = c(1981:1987),\n                            labels = as.character(1981:1987))\n\nlos$stroke &lt;- factor(los$stroke, \n                          levels = c(0, 1), \n                          labels = c('no','yes'))"
  },
  {
    "objectID": "method/case_study_liggetid_1.html#visualization",
    "href": "method/case_study_liggetid_1.html#visualization",
    "title": "Length of hospital stay: Part I",
    "section": "Visualization",
    "text": "Visualization\n\nLOS vs age and sex\n\n\nShow the code\nplt_scat2 &lt;- ggplot(data = los, \n                    mapping = aes(x = age, y = los, shape = sex, color = sex))\nplt_scat2 &lt;- plt_scat2 + geom_point(size = 2, alpha = 0.7)\n# customize\nplt_scat2 &lt;- plt_scat2 + labs(\n  x = 'Age', \n  y = 'Length of hosptial stay (days)', \n  title = 'Length of stay versus age'\n)\nplt_scat2 &lt;- plt_scat2 + theme_bw() # make white background\n# change text size\nplt_scat2 &lt;- plt_scat2 + theme(\n  axis.text = element_text(size = 12),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15)\n)\n# change color\nplt_scat2 &lt;- plt_scat2 + scale_color_brewer(palette = 'Set1')\nplt_scat2\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggridges)\nplt_ridge &lt;- ggplot(data = los, \n                    mapping = aes(x = los, y = admission_year, fill = sex))\nplt_ridge &lt;- plt_ridge + geom_density_ridges(alpha = 0.6) \nplt_ridge &lt;- plt_ridge + theme_ridges()\nplt_ridge &lt;- plt_ridge + labs(\n  x = 'Length of hosptial stay (days)', \n  y = 'Admission year', \n  title = 'Length of stay in each year, for each gender'\n)\n# change color\nplt_ridge &lt;- plt_ridge + scale_fill_brewer(palette = 'Set1')\nplt_ridge\n\n\nPicking joint bandwidth of 32.7\n\n\n\n\n\n\n\n\n\n\n\nLOS vs year of admission\n\n\nShow the code\nplt_box &lt;- ggplot(data = los, \n                  mapping = aes(x = admission_year, y = los, fill = sex))\nplt_box &lt;- plt_box + geom_boxplot(outlier.size = 1)\n# plt_box &lt;- plt_box + facet_wrap( ~ sex)\nplt_box &lt;- plt_box + coord_flip()\n\n# customize\nplt_box &lt;- plt_box + theme_bw() # make white background\nplt_box &lt;- plt_box + labs(\n  x = 'Admission year', \n  y = 'Length of hosptial stay (days)', \n  title = 'Length of stay in each year, both men and women'\n)\nplt_box &lt;- plt_box + theme(\n  axis.text = element_text(size = 12),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15), \n  strip.text = element_text(size = 12)\n)\n\nplt_box &lt;- plt_box + scale_fill_brewer(palette = 'Set1')\nplt_box \n\n\n\n\n\n\n\n\n\n\n\nLOS vs types of admission\n\n\nShow the code\nplt_box2 &lt;- ggplot(data = los, \n                   mapping = aes(x = admission_year, y = los, fill = sex))\nplt_box2 &lt;- plt_box2 + geom_boxplot(outlier.size = 0.8)\nplt_box2 &lt;- plt_box2 + facet_wrap( ~ admission_from)\n\n\n# customize\nplt_box2 &lt;- plt_box2 + theme_bw() # make white background\nplt_box2 &lt;- plt_box2 + labs(\n  x = 'Admission year', \n  y = 'Length of hosptial stay (days)', \n  title = 'Length of stay in each year, each type of admission'\n)\nplt_box2 &lt;- plt_box2 + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15), \n  strip.text = element_text(size = 12), \n  axis.text.x = element_text(angle = 45) # more readable\n)\n\nplt_box2 &lt;- plt_box2 + scale_fill_brewer(palette = 'Set1')\nplt_box2"
  },
  {
    "objectID": "method/rwd_statistics_overview.html",
    "href": "method/rwd_statistics_overview.html",
    "title": "RWD statistical considerations",
    "section": "",
    "text": "In addition to data quality, missing, …\nThe key is the bias\nSelection bias (p67)\nInformation bias (p68)\nExamples of confounders\nAspects to consider: PROTECT framework (Fang 2019)"
  },
  {
    "objectID": "method/rwd_statistics_overview.html#motivation-why-rwd-requires-special-treatment",
    "href": "method/rwd_statistics_overview.html#motivation-why-rwd-requires-special-treatment",
    "title": "RWD statistical considerations",
    "section": "",
    "text": "In addition to data quality, missing, …\nThe key is the bias\nSelection bias (p67)\nInformation bias (p68)\nExamples of confounders\nAspects to consider: PROTECT framework (Fang 2019)"
  },
  {
    "objectID": "method/rwd_statistics_overview.html#methods",
    "href": "method/rwd_statistics_overview.html#methods",
    "title": "RWD statistical considerations",
    "section": "Methods",
    "text": "Methods\n\nStratification methods\nmatching, propensity score (p71)\n\n\nSensitivity analysis\np72\n\n\nMissing data\nOthers: G-methods (generalized)"
  },
  {
    "objectID": "method/antibiotics.html",
    "href": "method/antibiotics.html",
    "title": "Antibiotics",
    "section": "",
    "text": "Drug-resistant diseases kill 700k people (Plackett2020) yearly, however fewer new antibiotic drugs are reaching the market.\nDevelopment cost: 1.5 billion USD (for one drug, based on 2017 study) while revenue is 45 million USD.\n\nnot enough demand: physicians prescribe less, treatment cycle is short compared to chronic diseases\nprice is low\ndevelopment is difficult\n\nhttps://www.nature.com/articles/s41429-023-00629-8"
  },
  {
    "objectID": "method/antibiotics.html#economics-and-production",
    "href": "method/antibiotics.html#economics-and-production",
    "title": "Antibiotics",
    "section": "",
    "text": "Drug-resistant diseases kill 700k people (Plackett2020) yearly, however fewer new antibiotic drugs are reaching the market.\nDevelopment cost: 1.5 billion USD (for one drug, based on 2017 study) while revenue is 45 million USD.\n\nnot enough demand: physicians prescribe less, treatment cycle is short compared to chronic diseases\nprice is low\ndevelopment is difficult\n\nhttps://www.nature.com/articles/s41429-023-00629-8"
  },
  {
    "objectID": "method/antibiotics.html#antibiotics-stewardship",
    "href": "method/antibiotics.html#antibiotics-stewardship",
    "title": "Antibiotics",
    "section": "Antibiotics stewardship",
    "text": "Antibiotics stewardship\nCoursera course: Antibiotics stewardship\nAntimicrobials are the 2nd most frequently prescribed claass of pharmaceuticals. As much as 50% of antibiotic use is inappropriate.\nFive D’s: (if it is the right) drug, dose, delivery, deescalation, duration.\nImpact of inappropriate AB use: poor patient outcomes (adverse reactions, organ toxicity, AB resistance, increased mortality); excess costs (drug acquisition cost, complication management, prolonged hospital stays, costs associated with AB resistance)\n\nPrinciples\nProphylactic to prevent infection, preemptive to abort infection, empiric to provide initial control in absence of knowledge of its etiology, definitive to cure infectiou of a knownn etiology or its antimicrobial susceptibility\nEmpiric use is very common - e.g. community and hospital acquired pneumonia, sepsis.\n\n\nPKPD\nPharmacokinetics (PK): what the body does to the drug - absorption, distribution, metabolism, elimination\nPharmacodynamics (PD): what the drug does to the body / target organism - measured drug concentration and antimicrobial effect (e.g. adverse effect, safety)\nMIC: minimum inhibitory concentration, from no visible growth to 99.9% bacteria kill (MBC)\nPharmacodynamic measures\n\nCmax, AB peak concentration\nCmin, AB trough concentration\nCmax / MIC, AUC / MIC, T&gt;MIC (on concentration time curve)\n\nConcentration-dependent AB classes: large, infrequent doses\n\naminoglycosides (e.g. gentamicin, tobramycin, aminkacin for life-threatening nosocomial infections)\nfluoroquinolonse (e.g. norfloxacin, ciprofloxacin, levofloxacin)\npolymyxin\n\nTime-dependent AB classes (beta-lactam): optimize the duration\n\npenicillins\ncephalosporins\ncarbapenems\nmacrolides"
  },
  {
    "objectID": "method/antibiotics.html#research-on-antibiotics-use",
    "href": "method/antibiotics.html#research-on-antibiotics-use",
    "title": "Antibiotics",
    "section": "Research on antibiotics use",
    "text": "Research on antibiotics use\n(relevant to my research)\nhttps://jamanetwork.com/journals/jamanetworkopen/fullarticle/2814214\nhttps://jamanetwork.com/journals/jamanetworkopen/fullarticle/2814216\nhttps://pubmed.ncbi.nlm.nih.gov/37760690/"
  },
  {
    "objectID": "method/sample_size_1.html",
    "href": "method/sample_size_1.html",
    "title": "Sample size (part I)",
    "section": "",
    "text": "(This is the part I on sample size calculation)\nSample size calculation is to determine the smallest number of subjects required, to detect a clinical meaningful effect. Why not recruiting as many as possible? Too expensive; or unethical (i.e. more people will be having potentially harmful or futile treatments)."
  },
  {
    "objectID": "method/sample_size_1.html#relevant-concepts",
    "href": "method/sample_size_1.html#relevant-concepts",
    "title": "Sample size (part I)",
    "section": "Relevant concepts",
    "text": "Relevant concepts\nStudy design\n\nparallel: group 1 TxA, group 2 TxB\ncrossover: requires fewer ssample than parallel; but requires wash-out period. Group 1 TxA -&gt; TxB; group 2 TxB -&gt; TxA\n\nTests\n\n\\(\\mu_T, \\mu_s\\): mean of new Tx or standard procedure\n\\(\\delta\\): minimum clinically important difference\n\\(\\delta_{NI}\\): non-inferiority margin\n\n\n\n\n\n\n\n\n\nTest for\nH0\nH1\n\n\n\n\nEquality\n\\(\\mu_T - \\mu_s = 0\\)\n\\(\\mu_T - \\mu_s \\neq 0\\)\n\n\nEquivalence\n\\(|\\mu_T - \\mu_s| \\geq 0\\)\n\\(|\\mu_T - \\mu_s| &lt; 0\\)\n\n\nSuperiority\n\\(\\mu_T - \\mu_s \\geq 0\\)\n\\(\\mu_T - \\mu_s &lt; 0\\)\n\n\nNon-inferiority\n\\(\\mu_T - \\mu_s \\leq -\\delta_{NI}\\)\n\\(\\mu_T - \\mu_s \\geq -\\delta_{NI}\\)\n\n\n\nErrors\n\nType I error, significance level \\(\\alpha\\). P(reject H0 |H0). Usually set to 0.05\nType II error \\(\\beta\\). P(not reject H0 |H1).\nPower, \\(1 - \\beta\\). P(reject H0 |H1). Usually set to 80% or 90%\n\nPrimary outcome\n\ncan be categorical or continuous\nMinimal meaningful detecable difference MD: the smallest difference to be considered as clinically meaningful in the primary outcome\n\nDropout rate: need to be adjusted.\nAllocation ratio: unequal sample size.\nEffect size (Cohen’s d, f) should be found in the literature. In general,\n\nvery small, d = 0.01\nsmall, d = 0.2\nmedium, d = 0.5\nlarge, d = 0.8\nvery large, d = 1.2\nhuge, d = 2"
  },
  {
    "objectID": "method/sample_size_1.html#two-sample-t-test",
    "href": "method/sample_size_1.html#two-sample-t-test",
    "title": "Sample size (part I)",
    "section": "Two sample t-test",
    "text": "Two sample t-test\nNote that this result is for one group: in total it’s times two.\n\n# effect size: 0.5\npwr::pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8, type = 'two.sample', alternative = 'two.sided')\n\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group"
  },
  {
    "objectID": "method/sample_size_1.html#anova",
    "href": "method/sample_size_1.html#anova",
    "title": "Sample size (part I)",
    "section": "ANOVA",
    "text": "ANOVA\nResult is for each group.\n\n# k: number of groups; f: effect ssize\npwr::pwr.anova.test(k = 3, f = 0.25, sig.level = 0.05, power = 0.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.3966\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group"
  },
  {
    "objectID": "method/sample_size_1.html#proportions",
    "href": "method/sample_size_1.html#proportions",
    "title": "Sample size (part I)",
    "section": "Proportions",
    "text": "Proportions\nCohen’s h is used as the effect size, \\(h = 2arcsin(\\sqrt{p_1} - 2arcsin(\\sqrt{p_2}))\\). Use 0.2, 0.5, 0.8 for small, medium and large effect sizes.\n\n# one group\npwr::pwr.p.test(h = 0.5, sig.level = 0.05, power = 0.8, alternative = 'two.sided')\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.5\n              n = 31.39544\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n# two groups\npwr::pwr.2p.test(h = 0.5, sig.level = 0.05, power = 0.8, alternative = 'two.sided')\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.5\n              n = 62.79088\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes"
  },
  {
    "objectID": "method/sample_size_1.html#chi-square-test",
    "href": "method/sample_size_1.html#chi-square-test",
    "title": "Sample size (part I)",
    "section": "Chi-square test",
    "text": "Chi-square test\nCohen’s w. Use \\(l, k\\) to compute degrees of freedom.\n\n# k: number of groups; f: effect ssize\npwr::pwr.chisq.test(w = 0.3, df = (2-1)*(3-1), sig.level = 0.05, power = 0.8)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 107.0521\n             df = 2\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations"
  },
  {
    "objectID": "method/sample_size_1.html#exact-test",
    "href": "method/sample_size_1.html#exact-test",
    "title": "Sample size (part I)",
    "section": "Exact test",
    "text": "Exact test\nNeed to specify the proportion in each group (control, treatment). Allocation ratio 1:1\n\nexact2x2::ss2x2(p0 = 0.2, p1 = 0.8, n1.over.n0 = 1, sig.level = 0.05, power = 0.8, \n                approx = F, print.steps = T, paired = F)\n\n[1] \"starting calculation at n0= 11  n1= 11\"\n[1] \"n0=11 n1=11 power=0.734302912043505\"\n[1] \"n0=19 n1=19 power=0.962100966603327\"\n[1] \"n0=15 n1=15 power=0.872315260457242\"\n[1] \"n0=13 n1=13 power=0.868827534112504\"\n[1] \"n0=12 n1=12 power=0.811527612034704\"\n\n\n\n     Power for Fisher's Exact Test \n\n          power = 0.8115276\n             n0 = 12\n             n1 = 12\n             p0 = 0.2\n             p1 = 0.8\n      sig.level = 0.05\n    alternative = two.sided\n  nullOddsRatio = 1\n\nNOTE: errbound= 1e-06"
  },
  {
    "objectID": "method/overview_ci.html",
    "href": "method/overview_ci.html",
    "title": "Overview: causal inference",
    "section": "",
    "text": "\\(Y^{a=1}, Y^{a=0}\\) are the potential outcomes under treatment 1 and 0. They are random variables. Treatment A has causal effect if \\(Y^{a=1} \\neq Y^{a=0}\\).\nFor individual \\(i\\), \\(Y_i^{a=1}, Y_i^{a=0}\\) are deterministic.\nIn reality we do not observe both potential outcomes for an individual, since we only have ONE outcome. We observe \\(Y\\) and \\(A\\). For a population, average treatment effect (ATE) can be estimate"
  },
  {
    "objectID": "method/overview_ci.html#effects",
    "href": "method/overview_ci.html#effects",
    "title": "Overview: causal inference",
    "section": "",
    "text": "\\(Y^{a=1}, Y^{a=0}\\) are the potential outcomes under treatment 1 and 0. They are random variables. Treatment A has causal effect if \\(Y^{a=1} \\neq Y^{a=0}\\).\nFor individual \\(i\\), \\(Y_i^{a=1}, Y_i^{a=0}\\) are deterministic.\nIn reality we do not observe both potential outcomes for an individual, since we only have ONE outcome. We observe \\(Y\\) and \\(A\\). For a population, average treatment effect (ATE) can be estimate"
  },
  {
    "objectID": "method/overview_ci.html#estimands",
    "href": "method/overview_ci.html#estimands",
    "title": "Overview: causal inference",
    "section": "Estimands",
    "text": "Estimands\nGreifer, N., & Stuart, E. A. (2021). Choosing the estimand when matching or weighting in observational studies. arXiv preprint arXiv:2106.10577.\n\n\n\n\n\nATE: average treatment effect in the population\n\\(E[Y(1) - Y(0)]\\)\nATT: average treatment effect among the treated\n\\(E[Y(1) - Y(0) | Z = 1]\\)\nATC: average treatment effect among the controls\n\\(E[Y(1) - Y(0) | Z = 0]\\)\nATM: average treatment effect among the matched"
  },
  {
    "objectID": "method/overview_ci.html#graphical-representation",
    "href": "method/overview_ci.html#graphical-representation",
    "title": "Overview: causal inference",
    "section": "Graphical representation",
    "text": "Graphical representation\n\n\n\n\n\n\n\n\nData generation\nCorrect causal model\nCorrect causal effect\n\n\n\n\nCollider\nY ~ X\n1\n\n\nConfounder\nY ~ X; Z\n0.5\n\n\nMediator\nDirect effect: Y ~ X; Z. Total effect: Y ~ X\nDirect: 0; total: 1\n\n\nM-Bias\nY ~ X\n1"
  },
  {
    "objectID": "method/overview_ci.html#selection-bias",
    "href": "method/overview_ci.html#selection-bias",
    "title": "Overview: causal inference",
    "section": "Selection bias",
    "text": "Selection bias\nThis bias is the result of selecting a common effect of 2 other variables (collider): a treatment, an outcome.\n\nnon-response, missing data\nself-selection, volunteer bias\nselection affected by treatment before study started\n\nA form of lack of exchangeability between the treated and untreated.\nCorrect for selection bias: IP weighting"
  },
  {
    "objectID": "method/mixed_models.html",
    "href": "method/mixed_models.html",
    "title": "Mixed models for repeted measurements",
    "section": "",
    "text": "Resources:"
  },
  {
    "objectID": "method/case_study_ctn.html",
    "href": "method/case_study_ctn.html",
    "title": "Case study: CTN",
    "section": "",
    "text": "Task overview see here\nQuestions of interest (examples)\nArticle describing the process: Odom 2023"
  },
  {
    "objectID": "method/case_study_ctn.html#ctn-data",
    "href": "method/case_study_ctn.html#ctn-data",
    "title": "Case study: CTN",
    "section": "CTN data",
    "text": "CTN data\n\nStudy protocol\nThree studies\n\nCTN 30(Protocol): prescription opiate abuse treatment study. Randomized, outpatient study, whether adding additional drug counseling improves outcome. Two phases.\n\nTx0: Bup/Nx (buprenorphine/naloxone) + SMM (standard medical management)\nTx1: Bup/Nx + EMM (enhanced medical management)\nphase 1 n=653, phase 2 n=360\nprimary finding: patients are likely to reduce opioid use during Bup/Nx treatment, however unsuccessful outcome is highly likely even after 12 weeks if tapering off treatment.\n\nCTN 27(Protocol, Saxon 2013): Bup/Nx vs MET (methadone) on liver function. Randomized, open-label, multi-center, phase 4 study.\nCTN 51 (Lee 2018): Bup/Nx vs XR-NTX naltrexone, effectiveness of pharmacotheerapeutic aids to recovery who completed detoxification and achieved short term abstinence. n=570\n\nXR-NTX (n=283): induction hurdle, higher rate of 24-week relapse, fewer successfully initiated XR-NTX (204 of 283) than Bup/Nx (270 of 287)\nfor those inducted successfully, relapse rate is similar\nrelapse: 4 consecutive weeks of any non-study opioid use (UDS or report), or 7 consecutive days of self-reported use.\n\n\nIf want to know which subject belongs to which study, use CreateProtocolHistory() or everybody data to have project ID.\nOpioids: Oxymorphone, Opium, Fentanyl, Hydromorphone, Codeine, Suboxone, Tramadol, Morphine, Buprenorphine, Hydrocodone, Opioid, Methadone, Oxycodone, Heroin\n\n\nTimes\n\nday 0: consent signed\nday of randomization (dr)\nday of receiving first dose of study drug (dd)\ninduction delay: dr - dd (in terms of days, not weeks)\nany day before dd is pre-treatment, even if participants are assigned to a treatment arm\n\npre-tx (baseline) / treatment period is defined by the first non-zero dose for the patient.\n\n\n\n\nEndpoints (CTNote)\nAs of CTNote, there are three types of outcomes\n\nabstinence\nuse reduction\nrelapse\n\n\n\nRisk factors\ndatasets info\n\n\nData details\n\nctn0094data\nctn0094extra\n\n\nlibrary(public.ctn0094data)\nlibrary(public.ctn0094extra)\n\n# visit_imp &lt;- public.ctn0094extra::derived_visitImputed\n# eth &lt;- public.ctn0094extra::derived_raceEthnicity\n\n#visit_imp\n#eth\n\n\ninduct_delay &lt;- public.ctn0094extra::derived_inductDelay\ninduct_delay\n\n# A tibble: 2,492 × 3\n     who treatment            inductDelay\n   &lt;int&gt; &lt;fct&gt;                      &lt;dbl&gt;\n 1     2 Outpatient BUP + EMM           0\n 2     3 Inpatient BUP                 NA\n 3     4 Inpatient NR-NTX               0\n 4     6 Outpatient BUP + SMM           0\n 5     7 Inpatient NR-NTX               1\n 6     9 Inpatient NR-NTX              NA\n 7    10 Outpatient BUP                 0\n 8    11 Methadone                      0\n 9    12 Outpatient BUP                 0\n10    13 Outpatient BUP                 0\n# ℹ 2,482 more rows\n\n\nWeekly pattern data. Week 1 starts the day after randomisation or signed consent, depending whether patients are randomized or not. Negative time stamps suggest prior randomization (or consent).\n\npattern_o &lt;- public.ctn0094extra::derived_weeklyOpioidPattern\npattern_o\n\n# A tibble: 3,560 × 8\n     who startWeek randWeek1 randWeek2 endWeek Baseline Phase_1          Phase_2\n   &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;  \n 1     1        -4        NA        NA      15 _____    ooooooooooooooo  \"\"     \n 2     2        -5         0        NA      14 ____++   ----oo-o-o-o+o   \"\"     \n 3     3        -6         0        NA      23 _______  o-ooo-ooooooooo… \"\"     \n 4     4        -4         0        NA      24 ____-    ---------------… \"\"     \n 5     5        -4        NA        NA      15 _____    ooooooooooooooo  \"\"     \n 6     6        -6         0        NA      14 ____+_+  -ooooooooooooo   \"\"     \n 7     7        -5         0        NA      24 _____+   ----ooooooooooo… \"\"     \n 8     8        -4        NA        NA      25 _____    ooooooooooooooo… \"\"     \n 9     9        -6         0        NA      22 ____+__  ooooooooooooooo… \"\"     \n10    10        -7         0        NA      22 _____+_- --o--*++o-+++++… \"\"     \n# ℹ 3,550 more rows\n\n\n\npattern_t &lt;- public.ctn0094extra::derived_weeklyTLFBPattern\npattern_t\n\n# A tibble: 3,560 × 8\n     who startWeek randWeek1 randWeek2 endWeek Baseline Phase_1          Phase_2\n   &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;  \n 1     1        -4        NA        NA      14 _____    oooooooooooooo   \"\"     \n 2     2        -5         0        NA      14 ++++++   -----------*+-   \"\"     \n 3     3        -6         0        NA      23 ++++*--  ----*----------… \"\"     \n 4     4        -4         0        NA      24 +++--    -------------*-… \"\"     \n 5     5        -4        NA        NA      14 _____    oooooooooooooo   \"\"     \n 6     6        -6         0        NA      14 +++++++  --------------   \"\"     \n 7     7        -5         0        NA      24 _+++--   ---------------… \"\"     \n 8     8        -4        NA        NA      24 _____    ooooooooooooooo… \"\"     \n 9     9        -6         0        NA      22 +++**--  ---------------… \"\"     \n10    10        -7         0        NA      22 --*+++++ ---------++-+-*… \"\"     \n# ℹ 3,550 more rows\n\n\nOutcomes (relapse)\n\nout &lt;- CTNote::outcomesCTN0094\nout$ctn0094_relapse_event |&gt; table() # relapse\n\n\n   0    1 \n 619 2941"
  },
  {
    "objectID": "method/mixed_models.html#practical-strategy",
    "href": "method/mixed_models.html#practical-strategy",
    "title": "Mixed models for repeted measurements",
    "section": "Practical strategy",
    "text": "Practical strategy\n(Assuming it’s normal continuous outcome)\n\nUnderstand the design\n\nrepeated measures?\nany intervention?\nhow many levels? nesting? clustering?\n\n\n\nModel building\nStart with linear model without any random effect.\n\n\nMixed modeling in R\n\nLinear mixed models: nlme::lme, lme4::mner, brms::brm\ngeneralized linear mixed models (GLMM): lme4::glmer, glmmTMB; brms::brm for Bayesian\nnonlinear mixed models: nlme::nlme, lme4::nlmer; brms::brm\n\n\n\n\n\n\n\n\n\nequation\nformula\nmeaning\n\n\n\n\n\\(B_0 + B_1X_i + e_i\\)\n\nno random effect\n\n\n\\((B_0+b_{g,0} + B_1X_i + e_i)\\)\nx+(1|group)\nrandom group intercept\n\n\n\n(x|group)\nrandom slopt of x within group, with correlated intercept\n\n\n\n(1+x|group)\n\n\n\n\n(0+x | group)\nrandom slop of x within group, no variation in intercept\n\n\n\n(-1+x | group)\n\n\n\n\n(1| group) + (0+x|group)\nuncorrelated random intercept and random slopt within group\n\n\n\n\n\nRandom intercept\nEach group has different intercelpt, but the slope is the same\n\\[gpa = (b_0 + effect_{student} + b_{occasion} * occasion + e)\\]"
  },
  {
    "objectID": "method/nut_epi.html",
    "href": "method/nut_epi.html",
    "title": "Nutritional Epidemiology",
    "section": "",
    "text": "Reading notes from Chapter 29, Modern Epidemiology\n\nNutritional epidemiology: determine relationships between dietary factors and occurrence of specific disease.\nNutrition: could be nutrients, natural constituents of food, chemicals created in the cooking and preservation of food, or noninfectious food contaminants.\nIn NE, exposure are aspects of diet, very complex set of variables. Measurements of dietary intake is key\n\nStudy designs\nCase control studies\n\nconsistent valid results are difficult to obtain, due to methodological bias\n\nselection bias, appropriate control group is difficult. Hospital based studies use patients from another disease, or from population; low participation rate, higher level of general health consciousness\nrecall bias\n\n\nExperimental studies\n\ntrials must be of long duration\ncompliance with the treatment diet is likely to decrease, particularly if treatment involves a real change in food intake, comparison group may adopt diet of the treatment group if the treatment is beneficial\nhigher health consciousness are likely to participate, hence those with highest potential risk are underrepresented\n\n\n\nMeasurements\nIn human diets, intake of different components tend to be correlated. All individuals are exposed (e.g. fat, fiber, vitamin A), hence dietary exposures can rarely be ‘present’ or ‘absent’. Continuous range, with limited range of variation between persons with a common culture or geographic location.\nDiet can be described in terms of\n\nfoods\nfood groups\noverall dietary patterns\n\n\nUsing food\nMeasurement of total intake of a nutrient provides the most powerful test of a hypothesis, especially if many foods each contributes only modestly to intake of that nutrient.\nUse of food to represent diet has advantage when suspicion exists, but no specific hypothesis is formulated.\nFoods are not fully represented by their nutrient composition: milk vs yogurt produce different physiologic effects\nAnalyses based on foods (rather than nutrients) are more directly related with dietary recommendations.\nReciprocal relations emerge, promoting use of nutrient\n\ndark bread eaters tend to not eat white bread\nmargarine users tend to not use butter\n\n\n\nUsing food group\nFiber from grains, fiber from fruit and vegetables may have different association with risk of disease\n\n\nTime dimension\nDiet may be important during childhood, even though disease occurs decades later\nDiet may also act as a late-stage promoting or inhibiting factor, hence intake near the time before diagnosis may be important.\nDiets of individuals tend to be correlated from year to year, with decreasing correlation over longer intervals\n\n\n\nDietary assessment\nThree approaches:\n\nfood intake\nbiochemical measurements of blood or body tissues\nmeasurement of body dimensions\n\nShort-term recall (24h)\n\nrequires 10-20 minutes\nno training or literacy and minimal effort for the participants\nvalidity needs to be assessed\nmost serious limitation: highly variable from day to day, only very commonly eaten food can be studied with this method\n\nDietary records, food diaries\n\ntypical 3-7 days\nburdern on the subject, only for those literate and highly motivated"
  },
  {
    "objectID": "method/survival.html#basics",
    "href": "method/survival.html#basics",
    "title": "Survival",
    "section": "Basics",
    "text": "Basics\nObserved time \\(Y_i = min(T_i, C_i)\\)\n\n\\(T_i\\) is event time\n\\(C_i\\) is censoring time\n\nEvent indicator \\(\\delta_i = 1\\) if event observed (\\(T_i &lt;= C_i\\)), 0 else.\nProbability that a subject surves beyond given specific time:\n\\[S(t) = Pr(T&gt;t) = 1 - F(t)\\] where\n\n\\(S(t)\\) is the survival function\n\\(F(t) = Pr(T &lt;= t)\\) is the cumulative distribution function\n\nSurvival probability at a certain time t is a conditional probability of surviving beyond that time, given that an individual has survived just prior to that time. This can be estimated as the number of patients who are alive (without loss of follow-up), divided by the number of patients who were alive just prior to that time.\n\nLeft and right censor\n3 Time points: diagnosis, intervention or study time 0 (baseline), recording of outcome\n\nRight censor: endpoint not observed, survived until at least recording time\nLeft censor: diagnosis time unknown. We might care about the time from diagnosis to outcome, rather than baseline to outcome."
  },
  {
    "objectID": "method/survival.html#competing-risks",
    "href": "method/survival.html#competing-risks",
    "title": "Survival",
    "section": "Competing risks",
    "text": "Competing risks\nSubdistribution hazards: modifies the traditional hazard function to account for probability of NOT experiencing the event of interest due to competing risks\nCumulative incidence function: used to estimate the probability of experiencing each type of event over time\nAalen-Johansen estimators \\(\\hat{P}(T&lt;=t, X_T = j), j = 1, 2\\) add up to 1 minus Kaplan-Meier estimator \\(\\hat{P}(T&gt;t)\\)\nIndependence assumption: assuming competing events are mutually exclusive\n\nNonparametric estimation\nCause-specific hazards \\(\\alpha_{0j}(t), j = 1,2\\) are key quantities of the competing risk model, its specification suffice to generate competing risk data (except censoring).\nNelson-Aalen estimator of the cumulative cause-specific hazards \\(A_{0j}(t) = \\int_0^t \\alpha_{0j}(u) du, j = 1,2\\)"
  },
  {
    "objectID": "method/survival.html#multi-state-modeling",
    "href": "method/survival.html#multi-state-modeling",
    "title": "Survival",
    "section": "Multi-state modeling",
    "text": "Multi-state modeling\nMulti-state modeling is not only for time-to-event data. Competing risk is a special case for MS.\nModels the transition between states: healthy to diseased, diseased to death, healthy to death etc\nTransition probabilities\nTransition hazards\nState occupancy probabilities\nUse package msm, mstate, etc\n\nUse msm\nTime of transition is unknown. The time in the dataset is observed state\nTransition structure: defined by a matrix with \\(r, s\\) entry. For example, for transitions allowed below\n\n1 to 2\n2 to 3\n3 to 4\n\nthe transition matrix would be\n\nQ &lt;- rbind(c(0, 1, 0, 0), \n           c(0, 0, 1, 0), \n           c(0, 0, 0, 1),\n           c(0, 0, 0, 0))\n\nTransitions should only be allowed in adjacent states in continuous time. For example, even if we observe 1 -&gt; 3 without seeing the state 2, we know that patients have been through 1 -&gt; 2 -&gt; 3. The transition matrix should NOT be selected based on what we observe in the data!\nMean sojourn time (waiting time): \\(-1/q_{rr}\\) is the expected time to stay in the same state (hence the \\(rr\\))."
  },
  {
    "objectID": "method/book_whatif_part1.html",
    "href": "method/book_whatif_part1.html",
    "title": "Notes from book: What If (Part 1)",
    "section": "",
    "text": "Objective for reading the first 10 chapters: build an overview of the concepts at a high level, so that details can fit in the big picture more easily."
  },
  {
    "objectID": "method/book_whatif_part1.html#chapter-1---3",
    "href": "method/book_whatif_part1.html#chapter-1---3",
    "title": "Notes from book: What If (Part 1)",
    "section": "Chapter 1 - 3",
    "text": "Chapter 1 - 3\n(Notes are written on paper)\nKeywords:\n\nexchangeability, conditional exchangeability\nmarginally randomized experiment, conditional and unconditional randomization\nthe equivalence of causal risk difference (or ratio) to associational risk difference\neffect, and difference between standardization and IP weighting"
  },
  {
    "objectID": "method/book_whatif_part1.html#chapter-4-effect-modification",
    "href": "method/book_whatif_part1.html#chapter-4-effect-modification",
    "title": "Notes from book: What If (Part 1)",
    "section": "Chapter 4 Effect Modification",
    "text": "Chapter 4 Effect Modification\nA null average causal effect in the population does not imply a null average causal effect in a particular subset of the population, such as males and females.\nEffect modifier: V is a modifier of the effect of A on Y, when the average causal effect on A on Y varies across levels of V.\nStratification is the natural way to identify effect modification. Effect modification is also called effect heterogeneity across strata of V.\nWhy care about it? If V modifies the effect of A on Y, then the average effect will differ between populations with different prevalence of V, e.g. the effect (size and direction) is more pronounced in the larger group. Also, it helps to identify the group that would benefit the most from an intervention.\nWhen counterfactual outcomes \\(Y^{a=1}, Y^{a=0}\\) are unavailable, only observed outcomes are available: the use of stratification to detect effect modification will depend on the study design.\n\nin an ideal marginally randomized experiment with unconditional randomization, the causal RD is the associational RD. Simply carry out a stratified analysis.\n\n\nSummary\n\nIP weighting and standardisation can be used to compute either marginal or conditional effects;\nstratification / restriction and matching can only be used to compute conditional effects in certain subsets of the population.\n\nAll four approaches require exchangeability and positivity.\nWhen there is no effect modification, the effect measures (RR, RD) computed via these four approches will be equal."
  },
  {
    "objectID": "method/book_whatif_part1.html#chapter-6-graphical-representation",
    "href": "method/book_whatif_part1.html#chapter-6-graphical-representation",
    "title": "Notes from book: What If (Part 1)",
    "section": "Chapter 6 Graphical representation",
    "text": "Chapter 6 Graphical representation\nDAG: directed acyclic graphs. No cycles: a variable can not cause itself.\nCausal diagram guides the data analysis. The opposite direction is called ‘causal discovery’.\nFor marginally randomized experiment, variable L is not in the DAG as L does not affect the assignment. Example: aspirin use A has preventative causal effect on risk of heart disease Y.\n\n\n\n\nflowchart LR\n  A[A] --&gt; Y{Y}\n\n\n\n\n\nFor conditionally randomized experiment, L is a confounder so needs to be in the DAG (has arrow to both A and Y), as L affects assignment. Example: carrying a lighter A has no causal effect on risk of lung cancer Y; cigarette smoking L has a causal effect on both carrying a lung cancer Y and carrying a light A. If one examines the association between Y and A, there is an association linked by L.\n\n\n\n\nflowchart LR\n  L(L) --&gt; A[A]\n  A[A] --&gt; Y{Y}\n  L(L) --&gt; Y{Y}\n\n\n\n\n\n\n\n\n\nflowchart LR\n  L(L) --&gt; A[A]\n  L(L) --&gt; Y{Y}\n\n\nDAG when L is a confounder (common cause)\n\n\n\nExample: genetic haplotype A has no causal effect on becoming cigarette smoker Y. Both the haplotype A and cigarette smoking Y has a causal effect on the risk of heart disease L. L here is a collider. A and Y are independent.\n\n\n\n\nflowchart LR\n  A[A] --&gt; L(L)\n  Y{Y} --&gt; L(L)\n\n\nDAG when L is a collider (common effect)\n\n\n\n\n\n\n\n\n\nMarginally association or not\n\n\n\nTwo variables are marginally associated if\n\none causes the other; or\nthey share common causes (linked by common causes)\n\nOtherwise they are marginally independent (collider, linked by common effect). Note that if now you condition on levels of the collider, it opens the path and brings in association.\n\n\n\nConditional independence\nd-separation: two variable are d-separated if all paths between them are blocked. d stands for directional.\n\n\n\n\nflowchart LR\n  A[Aspirin] --&gt; B(Platelet aggregation)\n  B(Platelet aggregation) --&gt; Y{Heart disease} \n\n\n\n\n\nA is marginally associated with Y. A affects Y only through B; hence knowing levels of B is enough to predict Y, there no information added by knowing A. A and Y are conditionally independent given B.\n\n\n\n\nflowchart LR\n  L(Smoker) --&gt; A[Carrying a lighter]\n  L(Smoker) --&gt; Y{Lung cancer}\n\n\n\n\n\nL here is a confounder. A and Y are marginally associated, however A and Y are conditionally independent given L (i.e. within each level of L). For non-smokers (L=0), knowing that he carries a lighter or not does not help predict the risk of lung cancers; as we assume people carrying lighter tend to be smokers and we already know the smoking status, no information needed for whether they carry a lighter (opposite direction).\n\n\n\n\nflowchart LR\n  A[haplotype] --&gt; L(heart disease)\n  Y{smoker} --&gt; L(heart diseas)\n\n\n\n\n\nThe path from A to Y is blocked by collider L, however A and Y will be conditionally associated within levels of their common effect L. Whether two variables (causes) are associated can not be influenced by an event in the future (effect), but they generally become associated once stratifying on the common effect. This is related to selection bias."
  },
  {
    "objectID": "method/book_whatif_part1.html#chapter-7-confounding",
    "href": "method/book_whatif_part1.html#chapter-7-confounding",
    "title": "Notes from book: What If (Part 1)",
    "section": "Chapter 7 Confounding",
    "text": "Chapter 7 Confounding\nConfounding is the main shortcoming of observational studies.\nBackdoor path: a non causal path between A and Y. \\(A \\leftarrow L \\rightarrow Y\\) that links A to Y through their common cause L is a backdoor path.\n\n\n\n\nflowchart LR\n  L(L) --&gt; A[A]\n  A[A] --&gt; Y{Y}\n  L(L) --&gt; Y{Y}\n\n\n\n\n\n\nWhen exchangeability holds, meaning that \\(Y^a\\) is independent from A, all individuals have the same probability of receiving treatment as in a marginally randomized experiment, the average causal effect \\(E[Y^{a=1}] - E[Y^{a=10}]\\) is equivalent to \\(E[Y|A=1] - E[Y|A=0]\\).\nWhen exchangeability does not hold, but conditional exchangeability holds (conditional on L), need to adjust for L via standardization or IP weighting to get the population causal effect.\n\n### Backdoor criterion\nMinimal adjustment sets\n\nConfounding adjustment\nAdjustment or controlling for: any technique that removes the effect of the variables we are not interested in.\n\nG-methods (generalized): standardization, IP weighting, g-estimation\nStratification based methods: stratification, matching"
  },
  {
    "objectID": "method/book_whatif_iv.html",
    "href": "method/book_whatif_iv.html",
    "title": "Notes from book: What if (Part x)",
    "section": "",
    "text": "Instrumental variable estimation is one way that does not rely on the assumptions where all variables to adjust for confounding and selection bias to be identified and correctly measured.\n\n\n\n\nflowchart LR\n  Z[Z] --&gt; A[A]\n  A[A] --&gt; Y{Y}\n  U(U) --&gt; A[A]\n  U(U) --&gt; Y{Y}\n\n\n\n\n\n\nZ is the random assignment indicator (1 treatment 0 placebo)\nA is the treatment (1 yes 0 no), not all adhere to original assignment\nY is the outcome\nU is the unmeasured variables that affect both treatment adherence and outcome\n\n\nConditions for instrument Z\n\nZ is associated with A (treatment)\nZ does not affect Y through its potential effect on A\nZ and Y do not share causes\n\nThe three conditions are satisfied in a double blind randomized controlled trial as\n\nthose who are assigned to treatment (Z=1) are more likely to receive treatment (A=1), indicating association\nexpected in double blind design\nexpected by random assignment\n\nThere is a fourth condition of heterogeneity / monotonicity.\nIf an instrument is unmeasured, sometimes a proxy or surrogate instrument associated with the unmeasured instrument \\(U_z\\) can be used.\n\n\nIn observational studies\nIn observational studies we do not have the random assignment indicator Z. Some common categories of IV:\n\ngenetic factors\npreference\naccess to treatment\n\n\n\nCompare with other methods\nThe conditional exchangeability required by IP weighting, standardization is replaced by other assumptions. The choice of method depends on whether it is easier to identify and measure the confounders, or find an instrument Z and expect monotonicity and heterogeneity.\nViolation of assumptions 1-4 may result in large biases; IP weighting and standardization are more robust compared to IV.\nWhen there are time varying treatment, IV is not ideal as it is suited for point intervention.\n\n\nivreg package\nCheck out this post for an example. Only normal errors."
  },
  {
    "objectID": "method/index.html#causal-inference",
    "href": "method/index.html#causal-inference",
    "title": "Inference and models",
    "section": "Causal inference",
    "text": "Causal inference\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nOverview: causal inference\n\n\nCollider, confounder, mediator and M-bias \n\n\n\n\nGeneral techniques\n\n\n(to be integrated) \n\n\n\n\nG-Computation\n\n\nG-Computation \n\n\n\n\nNotes from book: What If (Part 1)\n\n\nCausal inference notes: chapter 1 to 10 \n\n\n\n\nNotes from book: What if (Part 2)\n\n\nIP weighting, standardization (g-computation) \n\n\n\n\nNotes from book: What if (Part 3)\n\n\nOutcome regression, propensity score \n\n\n\n\nNotes from book: What if (Part x)\n\n\nInstrumental variables \n\n\n\n\nMatching\n\n\nOverview of matching techniques \n\n\n\n\nHypothesis tests\n\n\nTypes of errors \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "method/index.html#other-topics",
    "href": "method/index.html#other-topics",
    "title": "Inference and models",
    "section": "Other topics",
    "text": "Other topics\nTopics on inference in general, missing value handling\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nMissing data and imputation\n\n\nOverview of multiple imputation \n\n\n\n\nMultiple imputation in R\n\n\nMICE, regression, PMM \n\n\n\n\nIntervals\n\n\nConfidence, credible and prediction intervals \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "method/book_whatif_part4_iv.html",
    "href": "method/book_whatif_part4_iv.html",
    "title": "Notes from book: What if (Part x)",
    "section": "",
    "text": "Instrumental variable estimation is one way that does not rely on the assumptions where all variables to adjust for confounding and selection bias to be identified and correctly measured.\n\n\n\n\nflowchart LR\n  Z[Z] --&gt; A[A]\n  A[A] --&gt; Y{Y}\n  U(U) --&gt; A[A]\n  U(U) --&gt; Y{Y}\n\n\n\n\n\n\nZ is the random assignment indicator (1 treatment 0 placebo)\nA is the treatment (1 yes 0 no), not all adhere to original assignment\nY is the outcome\nU is the unmeasured variables that affect both treatment adherence and outcome\n\n\nConditions for instrument Z\n\nZ is associated with A (treatment)\nZ does not affect Y through its potential effect on A\nZ and Y do not share causes\n\nThe three conditions are satisfied in a double blind randomized controlled trial as\n\nthose who are assigned to treatment (Z=1) are more likely to receive treatment (A=1), indicating association\nexpected in double blind design\nexpected by random assignment\n\nThere is a fourth condition of heterogeneity / monotonicity.\nIf an instrument is unmeasured, sometimes a proxy or surrogate instrument associated with the unmeasured instrument \\(U_z\\) can be used.\n\n\nIn observational studies\nIn observational studies we do not have the random assignment indicator Z. Some common categories of IV:\n\ngenetic factors\npreference\naccess to treatment\n\n\n\nCompare with other methods\nThe conditional exchangeability required by IP weighting, standardization is replaced by other assumptions. The choice of method depends on whether it is easier to identify and measure the confounders, or find an instrument Z and expect monotonicity and heterogeneity.\nViolation of assumptions 1-4 may result in large biases; IP weighting and standardization are more robust compared to IV.\nWhen there are time varying treatment, IV is not ideal as it is suited for point intervention.\n\n\nivreg package\nCheck out this post for an example. Only normal errors."
  },
  {
    "objectID": "method/book_whatif_part2_ipw_std.html",
    "href": "method/book_whatif_part2_ipw_std.html",
    "title": "Notes from book: What if (Part 2)",
    "section": "",
    "text": "Validatidy of causal inference requires exchangeability, positivity, consistency, no measurement error and no model misspecification.\nCode: What If. R code\nCausal question: estimate the average causal effect of smoking cessation (treatment, A) on weight gain (outcome, Y)\nDataset: NHEFS. 1566 smokers, age 25-74. Baseline visit and follow-up 10 years later. A=1 means they quit smoking, 0 otherwise. Outcome weight gain measured in kg.\nAdjust for 9 variables measured at baseline:\nR code for the two methods: link"
  },
  {
    "objectID": "method/book_whatif_part2_ipw_std.html#ip-weighting",
    "href": "method/book_whatif_part2_ipw_std.html#ip-weighting",
    "title": "Notes from book: What if (Part 2)",
    "section": "IP weighting",
    "text": "IP weighting\nIP weighting creates pseudo-population to remove covariates L to the treatment A. Properties of the pseudo-population:\n\nA and L are statistically independent\n\\(E_{ps}[Y|A=a]\\) equals the standardized mean in the actual population, \\(\\sum_{l}E[Y|A=a,L=l]P[L=l]\\).\n\nIndividual-specific IP weights for treatment A is \\(W = 1/f(A|L)\\). For the quitters (A=1), \\(f(A|L) = P[A=1|L]\\). This is also the propensity score.\nThe weights can be estimated non-parametrically when the problem is simple: count the number of people treated (A=1) in each stratum (L=1, L=0) and then divide by the number in the stratum. When the problem has more variables (confounders), fit a logistic regression to estimate the probability.\n(more details to be filled in)"
  },
  {
    "objectID": "method/book_whatif_part2_ipw_std.html#g-formula-standardization",
    "href": "method/book_whatif_part2_ipw_std.html#g-formula-standardization",
    "title": "Notes from book: What if (Part 2)",
    "section": "g-formula, standardization",
    "text": "g-formula, standardization\nIP weighting and standardization are estimators of g-formula (1986). g-formula is a synonym of g-computation. (g-estimation is a different method)\n\n\n\n\nflowchart LR\n  L(L) --&gt; A[A]\n  A[A] --&gt; Y{Y}\n  L(L) --&gt; Y{Y}\n\n\n\n\n\nProcedures:\n\nfit a regression with Y as outcome, A as treatment, L as control. It is possible to make polynomials and/or interactions.\ncreate a dataset identical to the original data, but \\(A = 1\\) in every row\ncreate a dataset identical to the original data, but \\(A = 0\\) in every row\nuse model from step 1 to compute adjusted predictions in the two counterfactual datasets.\n\nThe quantity of interest is the difference between the predictions.\n(Note that in Hernan & Robins, three blocks of data are binded into one. The counterfactual datasets (2nd and 3rd) have NA in the outcome. So even though in the regression model it’s using the large dataset with duplicates, it is effectively only using the original data to estimate the parameters.)\n\nRobust methods\nDoubly robust methods combine models for treatment (ipw) and for outcome (standardization) in the same estimator.\n\nAugmented IP weighted estimator\ndoubly robust plug-in estimator"
  },
  {
    "objectID": "method/matching.html",
    "href": "method/matching.html",
    "title": "Matching",
    "section": "",
    "text": "This overview summary is based on the review paper Stuart 2010: Matching methods for causal inference: a review and a look forward\nGoal of matching: choosing well-matched samples of the original groups to reduce confounding - acquire treatment and control groups with similar covariate distributions.\nAlternatives to matching: adjust for covariates in a regression model, instrumental variables, structural equation modeling etc.\nBenefits of matching:\nTwo settings to use matching:\n(note that the outcomes are usually not used even when they are available)\nHistory of matching methods"
  },
  {
    "objectID": "method/matching.html#step-1-define-closeness",
    "href": "method/matching.html#step-1-define-closeness",
    "title": "Matching",
    "section": "Step 1: define closeness",
    "text": "Step 1: define closeness"
  },
  {
    "objectID": "method/matching.html#step-2-implement-matching",
    "href": "method/matching.html#step-2-implement-matching",
    "title": "Matching",
    "section": "Step 2: implement matching",
    "text": "Step 2: implement matching\n\nnearest neighbor matching\n\n\nSubclassification, full matching, weighting"
  },
  {
    "objectID": "method/matching.html#step-3-diagnose-matches",
    "href": "method/matching.html#step-3-diagnose-matches",
    "title": "Matching",
    "section": "Step 3: diagnose matches",
    "text": "Step 3: diagnose matches"
  },
  {
    "objectID": "method/matching.html#step-4-analysis-of-the-outcome",
    "href": "method/matching.html#step-4-analysis-of-the-outcome",
    "title": "Matching",
    "section": "Step 4: analysis of the outcome",
    "text": "Step 4: analysis of the outcome\nVignette: Estimating effects after matching by Noah Greifer\n\nMatching (old notes)\nExact matching:\n\nperfect covariate balance; \\(F(X_i|T_i = 1) = F(X_i|T_i=0)\\)\ninfeasible when covariate is continuous, and when there are many covariates.\n\nProbability of receiving treatment, \\(\\pi(X_i) = P(T_i = 1 | X_i)\\)\nMatching based on distance measures\n\nMahalanobis distance\nEstimated propensity score, \\(D(X_i, X_j) = |P(T_i = 1|X_i) - P(T_j=1 | X_j)|\\)\n\nCheck covariate balance\n\nideally compare joint distribution of all covariates\npractically check lower-dimensional summaries (standardized mean difference, variance ratio, empirical CDF difference)\n\nBalance test\nMatching would reduce number of observations"
  },
  {
    "objectID": "method/matching.html#software",
    "href": "method/matching.html#software",
    "title": "Matching",
    "section": "Software",
    "text": "Software\nMatchIt package\nPS matching is ONE of the many matching techniques that uses PS as the difference.\nLinks\nhttps://stats.stackexchange.com/questions/492218/should-the-choice-of-propensity-score-matching-versus-weighting-depend-on-the-de\nhttps://stats.stackexchange.com/questions/553853/understanding-propensity-score-matching?rq=1\nhttps://aetion.com/evidence-hub/understanding-propensity-score-weighting-methods-rwe/"
  },
  {
    "objectID": "method/interview_clinical_trial.html",
    "href": "method/interview_clinical_trial.html",
    "title": "Interview: clinical trial statistician",
    "section": "",
    "text": "Book: Statistical design, monitoring and analysis of clinical trials"
  },
  {
    "objectID": "method/interview_clinical_trial.html#topics-of-interest",
    "href": "method/interview_clinical_trial.html#topics-of-interest",
    "title": "Interview: clinical trial statistician",
    "section": "Topics of interest",
    "text": "Topics of interest\n\ndesign\nsample size\nanalysis"
  },
  {
    "objectID": "method/interview_clinical_trial.html#design",
    "href": "method/interview_clinical_trial.html#design",
    "title": "Interview: clinical trial statistician",
    "section": "Design",
    "text": "Design\n\ngeneral design\nadaptive design link"
  },
  {
    "objectID": "method/interview_clinical_trial.html#sample-size",
    "href": "method/interview_clinical_trial.html#sample-size",
    "title": "Interview: clinical trial statistician",
    "section": "Sample size",
    "text": "Sample size\n\nsample size"
  },
  {
    "objectID": "method/interview_clinical_trial.html#survival-analysis",
    "href": "method/interview_clinical_trial.html#survival-analysis",
    "title": "Interview: clinical trial statistician",
    "section": "Survival analysis",
    "text": "Survival analysis\n\nadvanced survival"
  },
  {
    "objectID": "method/interview_clinical_trial.html#longitudinal-data",
    "href": "method/interview_clinical_trial.html#longitudinal-data",
    "title": "Interview: clinical trial statistician",
    "section": "Longitudinal data",
    "text": "Longitudinal data\n\nrepeated measurement analysis"
  },
  {
    "objectID": "method/hypothesis_tests.html",
    "href": "method/hypothesis_tests.html",
    "title": "Hypothesis tests",
    "section": "",
    "text": "Concepts\nSignificance level: P(reject H0 when H0 is true), usually at 5%\nLLN, CLT\n\n\nInterview questions\n\nCan you explain Type I and Type II errors and how they impact clinical trial results?\nHow would you calculate power in a clinical trial, and why is it important?\nHow do you address multiple testing problems in a clinical trial with multiple endpoints?\nWhat is an intent-to-treat (ITT) analysis, and why is it important?"
  },
  {
    "objectID": "method/survival_competing_risk.html",
    "href": "method/survival_competing_risk.html",
    "title": "Competing risks",
    "section": "",
    "text": "Links\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html\nhttps://www.danieldsjoberg.com/ggsurvfit/\njmpost: combines survival analysis, mixed effect model https://genentech.github.io/jmpost/main/\nSubdistribution hazards: modifies the traditional hazard function to account for probability of NOT experiencing the event of interest due to competing risks\nCumulative incidence function: used to estimate the probability of experiencing each type of event over time\nAalen-Johansen estimators \\(\\hat{P}(T&lt;=t, X_T = j), j = 1, 2\\) add up to 1 minus Kaplan-Meier estimator \\(\\hat{P}(T&gt;t)\\)\nIndependence assumption: assuming competing events are mutually exclusive"
  },
  {
    "objectID": "method/survival_competing_risk.html#multi-state-modeling",
    "href": "method/survival_competing_risk.html#multi-state-modeling",
    "title": "Competing risks",
    "section": "Multi-state modeling",
    "text": "Multi-state modeling\nMulti-state modeling is not only for time-to-event data. Competing risk is a special case for MS.\nModels the transition between states: healthy to diseased, diseased to death, healthy to death etc\nTransition probabilities\nTransition hazards\nState occupancy probabilities\nUse package msm, mstate, etc\n\nUse msm\nTime of transition is unknown. The time in the dataset is observed state\nTransition structure: defined by a matrix with \\(r, s\\) entry. For example, for transitions allowed below\n\n1 to 2\n2 to 3\n3 to 4\n\nthe transition matrix would be\n\nQ &lt;- rbind(c(0, 1, 0, 0), \n           c(0, 0, 1, 0), \n           c(0, 0, 0, 1),\n           c(0, 0, 0, 0))\n\nTransitions should only be allowed in adjacent states in continuous time. For example, even if we observe 1 -&gt; 3 without seeing the state 2, we know that patients have been through 1 -&gt; 2 -&gt; 3. The transition matrix should NOT be selected based on what we observe in the data!\nMean sojourn time (waiting time): \\(-1/q_{rr}\\) is the expected time to stay in the same state (hence the \\(rr\\))."
  },
  {
    "objectID": "method/regression.html",
    "href": "method/regression.html",
    "title": "Regression",
    "section": "",
    "text": "Aspect\nLinear\nLogistic\nCox\n\n\n\n\nVariable selection\nstepwise  regularisation\n\n\n\n\nModel selection\nAIC  BIC  R2\nAIC  BIC  adjusted R2  ROC/AUC\nAIC  BIC  Concordance index\n\n\nHypothesis test for one or more coefficients\nt-test / Wald test  F-test (overall model)  LRT (less common)\nWald test  LRT\nWald test  Score (log rank) test  LRT\n\n\nDiagnostics\nResidual plot  QQ plot  Influence (Cook’s distance)\nHosmer-Lemeshow test  Calibration plots  influence\nProportional hazard assumption (PH)  residuals (shoenfeld, martingale)  influence measures\n\n\nWhen assumption does not hold\n\n\nPH: stratified cox model  time varying covariats  parametric models  competing risk\n\n\n\n\n\n\nstate hypothesis\ndata exploration\nfit a regression model\ndiagnostics\n\n\n(linear reg:) residual vs fitted\nnormal QQ plot of residuals\nadded variable plots\ninfluence plot (residual vs leverage)\n\n\nfix biggest problem, go back to 3\ncompare alternative models with nested model tests\ninterpret the coefficients\n\n\n\n\n\n\nHypothesis: \\(H_0: \\beta_1 = 0\\)\nTest statistic: \\(T_0 = \\frac{\\hat{\\beta_1} - 0}{se{\\hat{\\beta_1}}}\\)\nReject H0 if \\(t_0 &gt; t_{\\alpha, n-p-1}\\). If one covariate, \\(n-2\\)\n\n\n\nEvaluate the overall significance of model, and compare nested models.\nHypothesis: \\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = ... = 0\\). Joint non-significance of the model\nAlternative hypothesis: \\(H_1: \\beta_i \\neq = 0\\), one of them is significant\nCan be used for two nested linear regression models. Based on variance decomposition, not likelihood.\nCompare unrestricted sum of square of residuals (SSR) with restricted SSR.\n\nrestricted: coefficients are restricted to be 0 (i.e intercept only, H0). Would have higher SSR as no variance is explained by covariates\nunrestricted: not restricted to be 0, H1\n\nIf restricted is much larger than unrestricted, then reject the null.\n\\[F = \\frac{(SSR_r - SSR_u)/p}{SSR_u/n-p-1} \\sim f_{p, n-p-1}\\]\n\n\n\nTypically used for GLM and Cox regression. LRT can also be used for linear regression, but F-test is more common.\nLRT tests goodness-of-fit between two nested models, tests whether removing one predictor improves the model fit. It is based on likelihood function.\nFor GLM we do not have SSR, so comparing nested models requires likelihood from restricted and unrestricted models.\nNull hypothesis: reduced (fewer predictors) model is sufficient\nAlternative: full (more predictors) model is better\nDeviance: measures the difference in log-likelihood between fitted model and saturated model, which means it measures how far the current model is from the ideal model that fits the data perfectly.\n\nnull deviance: intercept only\nresidual deviance: deviance of the model with predictors included. Lower residual deviance, better fit\n\n\\[D = -2 \\times (\\frac{lik_{\\text{fitted model}}}{lik_{\\text{saturated}}}) \\sim \\chi^2_{p1 - p2}\\]\nModel 1 deviance - model 2 deviance\n\n\n\nCan be used to test about individual coefficients or set of coefficients in regression\n\\[W = \\frac{(\\hat{\\theta} - \\theta_0)^2}{I(\\theta)^{-1}} \\sim \\chi^2_1\\] For individual \\(\\beta\\), it is equivalent to t-test (to the power of 2) under normality assumptions\n\\[W = \\frac{(\\hat{\\beta} - \\beta_0)^2}{var(\\hat{\\beta})}\\] For multiple coefficients, testing whether several coefficients are simultaneously zero, the tests uses vectorized theta and covariance matrix.\nWald test is commonly seen in logistic and cox models to test the significance of covariates.\nSee an example here.\n\n\n\n\n\n\nScore test is used to test overall fit of the model without fitting the"
  },
  {
    "objectID": "method/interview_clinical_trial.html#hospital-vs-pharmaceuticals",
    "href": "method/interview_clinical_trial.html#hospital-vs-pharmaceuticals",
    "title": "Interview: clinical trial statistician",
    "section": "Hospital vs Pharmaceuticals",
    "text": "Hospital vs Pharmaceuticals\n\nHospital\nTopics of interest: long-term patient outcomes, QoL, comparative effectiveness of treatments, rare conditions that are less commercially viable. E.g. new surgical technique; compare different treatment regiments for patients with specific biomarkers\nDesign: more flexible and exploratory, novel designs; smaller, single centered trials; early-stage, proof-of-concept\n\n\nPharma\nTopics of interest: strongly focused on meeting requirements of regulation, focus on predefined endpoints like efficacy and safety that align with drug approval processes. E.g. new cancer drug compared to placebo or existing treatment\nDesign: rigid design aligned with regulatory requirements; large, multicenter; RCT with precise inclusion/exclusion criteria"
  },
  {
    "objectID": "method/case_study_liggetid_2_survival.html",
    "href": "method/case_study_liggetid_2_survival.html",
    "title": "Length of hospital stay: Part II",
    "section": "",
    "text": "This analysis is in preparation for interviews related to time-to-event analysis. Focus will be put on the procedure (and how to do it in R), as well as interpretation of the results.\nThe data liggetid was collected at the Geriatric Department at Ullevål Sykehus. Detailed description can be found in part 1 of the case study.\nWe will focus on the following variables:\nlos &lt;- readRDS('data/los.rds')\nhead(los, 3)\n\n  admission_year age    sex admission_from stroke los\n1           1987  81 female           home      0  13\n2           1987  96 female           home      0  17\n3           1987  79 female           home      0   6"
  },
  {
    "objectID": "method/case_study_liggetid_2_survival.html#kaplan-meier-analysis",
    "href": "method/case_study_liggetid_2_survival.html#kaplan-meier-analysis",
    "title": "Length of hospital stay: Part II",
    "section": "Kaplan-Meier analysis",
    "text": "Kaplan-Meier analysis\nCreate KM curve\n\nlibrary(survival)\nlibrary(ggsurvfit)\n\n# from ggsurvfit pkg. using surfit also works\nss &lt;- survfit2(Surv(time, status) ~1, data = los)\n\nggsurvfit(ss) + \n  labs(x = 'Days', \n       y = 'Overall survival prob') + \n  add_confidence_interval() + \n  add_censor_mark() + \n  add_risktable()\n\n\n\n\n\n\n\n\n\nx-day survival probability\n\n# beyond 100 days\nsummary(survfit(Surv(time, status)~1, data = los), times = 100)\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = los)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  100     38     290    0.152  0.0212        0.116          0.2\n\nnrow(los[time&gt;=100]) # n at risk (still alive)\n\n[1] 38\n\n# nrow(los[time&lt;=100])  # this includes censored, 363+1\nnrow(los[status == 1 & time &lt;= 100]) # n event (already happened)\n\n[1] 290\n\n# n at risk: 38; n event 290, p = 0.15\n\n401 total, 38 at risk, within the 364 (-1), 290 died\nNaive but incorrect way of calculating: 1-290/401 = 27.7%. This over-estimates how many are alive, as (364-290) = 74 are censored and we do not know the outcome.\n\n\nMedian survival time\nMedian survival time means half of the subjects have events happen before, and half after. It corresponds to the time where survival probability is 0.5.\nThe median survival time is lower than if using median(time_to_event) on those with outcome, naive but incorrect.\n\nsurvfit(Surv(time, status) ~ 1, data = los)\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = los)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 401    321     35      28      41\n\nmedian(los[status == 1, time]) # only compute time for uncensored\n\n[1] 28\n\n\nCompare with the complete case when there’s no censoring. We use discharge as the outcome status, all subjects have 1. The time variable is los which is the true time. Now these two should be consistent.\n\nsurvfit(Surv(los, discharge) ~ 1, data = los)\n\nCall: survfit(formula = Surv(los, discharge) ~ 1, data = los)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 401    401     28      26      33\n\nmedian(los$los)\n\n[1] 28\n\n\n\nggsurvfit(ss) + geom_vline(xintercept = c(100, 35), col = 'red')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRisk of ignoring censored subjects\n\n\n\n\n\n\nover-estimated (higher) survival probability calculated as (1 - dead/n), since the numerator does not include censored subjects whose outcome is unknown.\nunder-estimated (lower) median survival time, since their time before event will prolong the survival time."
  },
  {
    "objectID": "method/case_study_liggetid_2_survival.html#cox-proportional-hazards-model",
    "href": "method/case_study_liggetid_2_survival.html#cox-proportional-hazards-model",
    "title": "Length of hospital stay: Part II",
    "section": "Cox proportional hazards model",
    "text": "Cox proportional hazards model"
  },
  {
    "objectID": "method/case_study_liggetid_2_survival.html#time-varying-covariate",
    "href": "method/case_study_liggetid_2_survival.html#time-varying-covariate",
    "title": "Length of hospital stay: Part II",
    "section": "Time varying covariate",
    "text": "Time varying covariate"
  },
  {
    "objectID": "method/regression.html#model-building-workflow",
    "href": "method/regression.html#model-building-workflow",
    "title": "Regression",
    "section": "",
    "text": "state hypothesis\ndata exploration\nfit a regression model\ndiagnostics\n\n\n(linear reg:) residual vs fitted\nnormal QQ plot of residuals\nadded variable plots\ninfluence plot (residual vs leverage)\n\n\nfix biggest problem, go back to 3\ncompare alternative models with nested model tests\ninterpret the coefficients"
  },
  {
    "objectID": "method/regression.html#model-selection",
    "href": "method/regression.html#model-selection",
    "title": "Regression",
    "section": "",
    "text": "Interaction"
  },
  {
    "objectID": "method/regression.html#linear-regression",
    "href": "method/regression.html#linear-regression",
    "title": "Regression",
    "section": "Linear regression",
    "text": "Linear regression\nAssumptions:\n\nall relationships are linear\nindependent observation\nno perfect collinearity, no zero variance of independent variance (e.g. only female gender in the data, no male)\nerror term is normally distributed\nhomoscedasticity: error term has expected value of zero, uncorrelated with independent var\nerror term has equal variance\n\nUse residual as estimate for error terms.\n\nResidual vs fitted: should show no pattern. If it shows patterns (clusters, butterfly, U shape …) indicate either non-linearity or heteroscedasticity\n\nheteroscedasticity: try robust standard errors\nnon-linearity: consider transformation\n\nQ-Q plot\nResidual vs leverage: identify outliers (influential observations)\n\nleverage: distance from the mass center of the data\nCook’s distance: overall measure of influence of an observation\n\n\nLess important: scale vs location\nOther plots: car::avPlots\nSee case study: prestige for more examples.\nWhen assumption does not hold"
  },
  {
    "objectID": "method/regression.html#logistic-regression",
    "href": "method/regression.html#logistic-regression",
    "title": "Regression",
    "section": "Logistic regression",
    "text": "Logistic regression\nLogit(p) = log(p/(1-p)) = b0 + bpxp\nAssumptions:\n\nbinary out ordinal outcome\nlarge samples\nindependence\nlinearity of indep variables and log odds (so that it’s linear addition)\nnone or little multicollinearity between independent variables\n\nWhen assumption does not hold"
  },
  {
    "objectID": "method/regression.html#cox-regression",
    "href": "method/regression.html#cox-regression",
    "title": "Regression",
    "section": "Cox regression",
    "text": "Cox regression\nProportional hazards assumption, tested with cox.zph().\n\np-value for each covariate\nsignificant suggests proportional hazards is violated for this covariate\n\nConcordance: model’s ability to predict the ordering of survival times, i.e. how well the model can rank individual subjects by risk. It ranges from 0.5 to 1, the higher the better.\nResidual diagnostics, shouldn’t display patterns\n\nmartingale residual\ndeviance residual\n\n\n\n\n\n\n\nNote\n\n\n\nWhen assumption does not hold\n\nModify the model within Cox: stratified cox, time varying covariate (e.g.landmark analysis)\nParametric model: accelerated failure time AFT model, cure model, competing risk"
  },
  {
    "objectID": "method/regression.html#hypothesis-tests-for-regression",
    "href": "method/regression.html#hypothesis-tests-for-regression",
    "title": "Regression",
    "section": "",
    "text": "Hypothesis: \\(H_0: \\beta_1 = 0\\)\nTest statistic: \\(T_0 = \\frac{\\hat{\\beta_1} - 0}{se{\\hat{\\beta_1}}}\\)\nReject H0 if \\(t_0 &gt; t_{\\alpha, n-p-1}\\). If one covariate, \\(n-2\\)\n\n\n\nEvaluate the overall significance of model, and compare nested models.\nHypothesis: \\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = ... = 0\\). Joint non-significance of the model\nAlternative hypothesis: \\(H_1: \\beta_i \\neq = 0\\), one of them is significant\nCan be used for two nested linear regression models. Based on variance decomposition, not likelihood.\nCompare unrestricted sum of square of residuals (SSR) with restricted SSR.\n\nrestricted: coefficients are restricted to be 0 (i.e intercept only, H0). Would have higher SSR as no variance is explained by covariates\nunrestricted: not restricted to be 0, H1\n\nIf restricted is much larger than unrestricted, then reject the null.\n\\[F = \\frac{(SSR_r - SSR_u)/p}{SSR_u/n-p-1} \\sim f_{p, n-p-1}\\]\n\n\n\nTypically used for GLM and Cox regression. LRT can also be used for linear regression, but F-test is more common.\nLRT tests goodness-of-fit between two nested models, tests whether removing one predictor improves the model fit. It is based on likelihood function.\nFor GLM we do not have SSR, so comparing nested models requires likelihood from restricted and unrestricted models.\nNull hypothesis: reduced (fewer predictors) model is sufficient\nAlternative: full (more predictors) model is better\nDeviance: measures the difference in log-likelihood between fitted model and saturated model, which means it measures how far the current model is from the ideal model that fits the data perfectly.\n\nnull deviance: intercept only\nresidual deviance: deviance of the model with predictors included. Lower residual deviance, better fit\n\n\\[D = -2 \\times (\\frac{lik_{\\text{fitted model}}}{lik_{\\text{saturated}}}) \\sim \\chi^2_{p1 - p2}\\]\nModel 1 deviance - model 2 deviance\n\n\n\nCan be used to test about individual coefficients or set of coefficients in regression\n\\[W = \\frac{(\\hat{\\theta} - \\theta_0)^2}{I(\\theta)^{-1}} \\sim \\chi^2_1\\] For individual \\(\\beta\\), it is equivalent to t-test (to the power of 2) under normality assumptions\n\\[W = \\frac{(\\hat{\\beta} - \\beta_0)^2}{var(\\hat{\\beta})}\\] For multiple coefficients, testing whether several coefficients are simultaneously zero, the tests uses vectorized theta and covariance matrix.\nWald test is commonly seen in logistic and cox models to test the significance of covariates.\nSee an example here.\n\n\n\n\n\n\nScore test is used to test overall fit of the model without fitting the"
  },
  {
    "objectID": "method/survival.html#kaplain-meier-curve",
    "href": "method/survival.html#kaplain-meier-curve",
    "title": "Survival",
    "section": "Kaplain Meier curve",
    "text": "Kaplain Meier curve\nIt is non-parametric estimator of the survival function."
  },
  {
    "objectID": "method/survival.html#log-rank-test",
    "href": "method/survival.html#log-rank-test",
    "title": "Survival",
    "section": "Log-rank test",
    "text": "Log-rank test\nThis is a non-parametric test, comparing two survival distributions without assuming a parametric form for the survival distribution.\n## Cox proportional hazard\nThe Lehmann alternative, \\(S_1(t) = [S_0(t)]^\\psi\\)\nProportional hazard assumption: \\(h_1(t) = \\psi h_0(t)\\). It is key to quantify the difference between two hazard functions.\nHazard ratio, \\(\\psi = e^{x\\beta}\\)\n\nTests\nH0: \\(\\beta = 0\\)\n\nWald test\nScore (logrank) test. The score function is the first derivative of log-likelihood\nLikelihood ratio test\n\n\n\nCox regression\nHazard ratio (relative to baseline hazard) for subject i is \\(\\psi_i = e^{x_i\\beta}\\)\nSemi-parametric model for survival outcome\n\\[h(t|X_i) = h_0(t) exp(\\beta_1 X_{i1} + ... + \\beta_p X_{ip})\\] where\n\n\\(h(t)\\) is hazard, the instantaneous rate at which events occur\n\\(h_0(t)\\) is the underlying baseline hazard\n\nAssumptions\n\nnon-informative censoring\nproportional hazards\n\nHazard ratio HR: the ratio of hazards between two groups at any particular point in time. For example, HR = 0.59 (sex female) means 0.59 times as many females die as males at any given time - females have lower hazard of death than males.\n\n\nLandmark analysis\nCovariates are measured at baseline - before follow-up time for the event begins\nExamples of covariates that are not measured at baseline: transplant failure, compliance, adverse events\nLandmark approach\n\nselect a fixed time after baseline, this should be done based on clinical information\nsubset population for those followed at least until landmark time\ncalculate follow-up from landmark time, and apply log-rank tests or cox regression\n\nIt might be necessary to reset the time (for example by substracting the landmark time, say 90 days)\n\n\nTime-dependent covariate\nThis is more appropriate than landmark analysis when\n\nvalue of a covariate changes over time\nthere isn’t an obvious landmark time\nuse of landmark leads to too many exclusions"
  },
  {
    "objectID": "method/case_study_linearreg.html",
    "href": "method/case_study_linearreg.html",
    "title": "Linear regression example: prestige",
    "section": "",
    "text": "This analysis is in preparation for interviews related to linear regression. Focus will be put on the procedure (and how to do it in R), as well as interpretation of the results.\nsuppressMessages(library(car))\nprestige &lt;- carData::Prestige\n\nm1 &lt;- lm(prestige ~ education + income + women, \n         data = prestige)\n\nsummary(m1)\n\n\nCall:\nlm(formula = prestige ~ education + income + women, data = prestige)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.8246  -5.3332  -0.1364   5.1587  17.5045 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.7943342  3.2390886  -2.098   0.0385 *  \neducation    4.1866373  0.3887013  10.771  &lt; 2e-16 ***\nincome       0.0013136  0.0002778   4.729 7.58e-06 ***\nwomen       -0.0089052  0.0304071  -0.293   0.7702    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.846 on 98 degrees of freedom\nMultiple R-squared:  0.7982,    Adjusted R-squared:  0.792 \nF-statistic: 129.2 on 3 and 98 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "method/case_study_linearreg.html#hypothesis-tests",
    "href": "method/case_study_linearreg.html#hypothesis-tests",
    "title": "Prestige dataset",
    "section": "Hypothesis tests",
    "text": "Hypothesis tests\nt-test on the coefficients\nF-test on the overall significance"
  },
  {
    "objectID": "method/case_study_linearreg.html#diagnostics",
    "href": "method/case_study_linearreg.html#diagnostics",
    "title": "Linear regression example: prestige",
    "section": "Diagnostics",
    "text": "Diagnostics\n\npar(mfrow = c(2, 2))\nplot(m1)"
  },
  {
    "objectID": "method/case_study_logistic.html",
    "href": "method/case_study_logistic.html",
    "title": "Logistic regression example: lung",
    "section": "",
    "text": "This analysis is in preparation for interviews related to logistic regression. Focus will be put on the procedure (and how to do it in R), as well as interpretation of the results.\nmtcars |&gt; head()\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nmlr1 &lt;- glm(vs ~ mpg + wt, family = 'binomial', data = mtcars)\nsummary(mlr1)\n\n\nCall:\nglm(formula = vs ~ mpg + wt, family = \"binomial\", data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -12.5412     8.4660  -1.481   0.1385  \nmpg           0.5241     0.2604   2.012   0.0442 *\nwt            0.5829     1.1845   0.492   0.6227  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.860  on 31  degrees of freedom\nResidual deviance: 25.298  on 29  degrees of freedom\nAIC: 31.298\n\nNumber of Fisher Scoring iterations: 6\nDeviance\nThe difference is how much adding two variables has improved the model"
  },
  {
    "objectID": "method/case_study_logistic.html#hypothesis-tests",
    "href": "method/case_study_logistic.html#hypothesis-tests",
    "title": "Logistic regression example: lung",
    "section": "Hypothesis tests",
    "text": "Hypothesis tests\nWald test: z-values are the wald test statistics\n\nLikelihood ratio test for two nested LR\nBuild a second model\n\nmlr2 &lt;- glm(vs ~ mpg, family = 'binomial', data = mtcars)\nsummary(mlr2)\n\n\nCall:\nglm(formula = vs ~ mpg, family = \"binomial\", data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -8.8331     3.1623  -2.793  0.00522 **\nmpg           0.4304     0.1584   2.717  0.00659 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.860  on 31  degrees of freedom\nResidual deviance: 25.533  on 30  degrees of freedom\nAIC: 29.533\n\nNumber of Fisher Scoring iterations: 6\n\nanova(mlr2, mlr1, test = 'Chisq') # analysis of deviance\n\nAnalysis of Deviance Table\n\nModel 1: vs ~ mpg\nModel 2: vs ~ mpg + wt\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1        30     25.533                     \n2        29     25.298  1  0.23546   0.6275\n\n\n\nmlr3 &lt;- glm(vs ~ wt, family = 'binomial', data = mtcars)\nsummary(mlr3)\n\n\nCall:\nglm(formula = vs ~ wt, family = \"binomial\", data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   5.7147     2.3014   2.483  0.01302 * \nwt           -1.9105     0.7279  -2.625  0.00867 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.860  on 31  degrees of freedom\nResidual deviance: 31.367  on 30  degrees of freedom\nAIC: 35.367\n\nNumber of Fisher Scoring iterations: 5\n\nanova(mlr3, mlr1, test = 'Chisq') # analysis of deviance\n\nAnalysis of Deviance Table\n\nModel 1: vs ~ wt\nModel 2: vs ~ mpg + wt\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  \n1        30     31.367                       \n2        29     25.298  1   6.0689  0.01376 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAIC(mlr1, mlr2, mlr3)\n\n     df      AIC\nmlr1  3 31.29788\nmlr2  2 29.53334\nmlr3  2 35.36673\n\nBIC(mlr1, mlr2, mlr3)\n\n     df      BIC\nmlr1  3 35.69508\nmlr2  2 32.46481\nmlr3  2 38.29820"
  },
  {
    "objectID": "method/case_study_logistic.html#diagnostics",
    "href": "method/case_study_logistic.html#diagnostics",
    "title": "Logistic regression example: lung",
    "section": "Diagnostics",
    "text": "Diagnostics"
  },
  {
    "objectID": "method/regression.html#general-vif-cooks-distance-gof",
    "href": "method/regression.html#general-vif-cooks-distance-gof",
    "title": "Regression",
    "section": "General: VIF, Cook’s distance, GoF",
    "text": "General: VIF, Cook’s distance, GoF"
  },
  {
    "objectID": "method/regression.html#subgroup-analysis",
    "href": "method/regression.html#subgroup-analysis",
    "title": "Regression",
    "section": "Subgroup analysis",
    "text": "Subgroup analysis\nE.g. analyse effect of new treatment on patient under 50 vs above 50 to see if treatment works differently in two age subgroups.\n\npre-specified (a priori): planned in SAP\npost-hoc: conducted afterwards. useful for generating hypothesis, but high risk of false positives (type I) due to multiple comparisons\n\nSteps: define subgroup -&gt; conduct analysis and estimate treatment effect -&gt; check for interaction -&gt; interpretation\n\nRisks of subgroup analysis, how to mitigate\n\n\n\n\n\n\n\n\n\nRisk\n\nSolution\nComment\n\n\n\n\nMultiple comparison\nIncrease the risk of FP (type I error), statistically significant differences occur by chance\nBonferroni correction, FDR adjustments\n\n\n\nReduced power\n(even) Smaller sample\n\n\n\n\nOver-interpretation\nPost-hoc analysis are not confirming the hypothesis made in SAP, interpretation need to be cautious\nRelate to other studies\n\n\n\np-hacking\nSearch for significant subgroup without clear hypothesis\n\n\n\n\nLoss of generalizability\nObscure the overall treatment effect"
  },
  {
    "objectID": "method/observational_design.html",
    "href": "method/observational_design.html",
    "title": "Observational study design",
    "section": "",
    "text": "Prospective studies: cohort. Population at risk and incidence are directly observed over time\nRetrospective studies: CC.\nIn cohort studies and RCT, participants are selected based on exposure status; in CC studies, they are selected based on outcome status."
  },
  {
    "objectID": "method/observational_design.html#relative-risk-risk-ratio-rr-ddds-ratio-or",
    "href": "method/observational_design.html#relative-risk-risk-ratio-rr-ddds-ratio-or",
    "title": "Observational study design",
    "section": "Relative risk (risk ratio) RR, ddds ratio OR",
    "text": "Relative risk (risk ratio) RR, ddds ratio OR\n\nCase control studies\nOdds ratio is usually used over risk ratio for the following reasons\n\nCC studies aim to compare the exposure history (risk factor) between cases and controls. As subjects are selected based on outcome, you can’t compute risk (which measures the population)\nincidence or risk calculation: it is not directly measureable you do not have total number of people developing the disease. In CC typically matched as a ratio\nodds is computable\nin rare outcomes, OR approximates RR\nlogistic regression estimates OR"
  },
  {
    "objectID": "method/observational_design.html#relative-risk-risk-ratio-rr-odds-ratio-or",
    "href": "method/observational_design.html#relative-risk-risk-ratio-rr-odds-ratio-or",
    "title": "Observational study design",
    "section": "Relative risk (risk ratio) RR, odds ratio OR",
    "text": "Relative risk (risk ratio) RR, odds ratio OR\n\nCase control studies\nOdds ratio is usually used over risk ratio for the following reasons\n\nCC studies aim to compare the exposure history (risk factor) between cases and controls. As subjects are selected based on outcome, you can’t compute risk (which measures the population)\nincidence or risk calculation: it is not directly measureable you do not have total number of people developing the disease. In CC typically matched as a ratio\nodds is computable\nin rare outcomes, OR approximates RR\nlogistic regression estimates OR"
  },
  {
    "objectID": "method/index.html#interview-preparation",
    "href": "method/index.html#interview-preparation",
    "title": "Inference and models",
    "section": "Interview preparation",
    "text": "Interview preparation\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nInterview: clinical trial statistician\n\n\nKnowledge framework \n\n\n\n\nInterview: behavior\n\n\nList of questions \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "method/interview_behavior.html",
    "href": "method/interview_behavior.html",
    "title": "Interview: behavior",
    "section": "",
    "text": "STAR: situation, task, action, result\n\n\nHighly relevant\n\nCollaboration & Communication\nExplain statistical concepts to non-experts; collaborate with cross-functional teams; resolve disagreements\nTailoring technical language, teamwork, conflict resolution, and maintaining professional relationships\n\n\nTell me about a time when you had to explain complex statistical concepts to a non-statistical audience (e.g., clinical staff or stakeholders). How did you ensure they understood?\n\nFocus on how you tailor technical language to a non-technical audience.\nCase 1: teaching sensitivity, specificity to clinicians\n\nS: part of the statistics course, audience are not very familiar with math and probability\nT: this topic requires linking sens, spec with prevalence to show that rare disease screening with diagnostic tests have many false positives\nA: instead of looking at the formulae, use visual aids, draw some squares with different color to indicate the change of prevalence, and how a large proportion of positives indicated by a very sensitive test are actually false positives\nR: students appreciate the visual aids, positive feedback, and good test results\n\nCase 2: teaching randomness and distribution with webR\n\nT: illustrate how fixed parameters for a distribution (such as normal) could take slightly different forms, difference of random seed\nA: make interactive code chunks in class, so students would try it\n\n\n\n\nDescribe a situation where you collaborated with a cross-functional team (e.g., clinical researchers, data managers, regulatory staff) to complete a project. How did you ensure effective communication and collaboration?\n\nFocus on your teamwork and adaptability in a multidisciplinary setting.\nCase 1: work experience at FHI\n\nS: covid times, surveillance, important to deliver reports in a timely manner\nT: maintain RT surv. system, with a range of people both inside the team (epi, project manager, technical people and web dev) and outside (infectious disease people, intl. network, media)\nA: we had different strength, I use my R skills to do the package maintenance, and quickly developed skills for shiny websites to support the team\nR: successful deliverables, improved the efficiency from 5 reports to thousands\n\nCase 2: phd project\n\nS:\nT:\nA:\nR:\n\n\n\n\nHave you ever faced a situation where the clinical team (or else) disagreed with your statistical analysis or interpretation of data? (can be any other disagreements such as work style..) How did you handle it?\n\nEmphasize how you navigate differences of opinion, maintain professional relationships, and resolve conflicts.\n\nstatistical advising in general, sometimes have to choose different methods\n\nCase 1: Phd AHUS paper\n\nS: paper collaborated with clinicians who provided data\nT: disagreement on how close to look at individual patients\nA: compromise from both side, do careful analysis in a next paper\nR: developed an R package to do that\n\nCase 2: Norkost dietary report\n\nS:\nT: need to carry out analysis with data provider\nA:\nR:\n\n\n\n\nCan you tell me about a time when you had to meet tight deadlines while ensuring the accuracy of your statistical work? How did you manage it?\n\nHighlight time management, prioritization, and quality control under pressure.\nCase 1: PhD dtw paper revision\n\nS: Review paper with major corrections\nT: lots of additional analysis to do\nA: implement, test fast, send to cluster to run at large scale. all while keeping an eye on the deadline, and leave sufficient time to revise and write\nR: managed to do it\n\n\n\n\nProblem Solving & Adaptability\nIdentify and address data or analysis issues; manage incomplete data; adapt to protocol changes\nAnalytical thinking, attention to detail, flexibility, ensuring study continuation\n\n\nDescribe a time when you identified a potential problem in the data or the statistical analysis plan. How did you address it?\n\nFocus on your problem-solving skills and attention to detail in identifying and rectifying errors.\nCase 1: PhD dtw paper revision\n\nS:\nT:\nA:\nR:\n\n\n\n\nImportant!Have you ever encountered incomplete or inconsistent data during a clinical trial? How did you handle the situation, and what actions did you take to ensure the study’s integrity?\n\nDiscuss your approach to data cleaning, imputation, or working with missing data.\n\nS:\nT:\nA:\nR:\n\n\n\n\nTell me about a time when you had to adapt quickly to changes in a clinical trial protocol or unexpected results from an interim analysis (or else). How did you manage the change?\n\nHighlight your adaptability, ability to pivot, and your role in ensuring the study’s continuation.\nCase 1: PhD dtw paper revision\n\nS:\nT:\nA:\nR:\n\n\n\n\nEthics & Compliance\nEnsure regulatory compliance and handle sensitive data\nKnowledge of FDA/EMA/ICH-GCP guidelines, ethical decision-making, data integrity & confidentiality\n\n\nTell me about a time when you had to handle sensitive or confidential data. How did you ensure the integrity and confidentiality of the data?\n\nFocus on data security, compliance with protocols (like HIPAA), and maintaining confidentiality.\n\nGenerally there is a system in place, such as TSD. So need to refuse the analysis when others breach it\n\nCase: advising project\n\nS: clinician sent me very sensitive dataset via email\nT:\nA: refused to work on it until it’s properly denonymised\nR:\n\n\n\n\nLeadership & Initiative\nTake initiative to improve processes; lead or mentor team members\nProcess improvement, leadership, mentoring, aligning team with trial goals\n\n\nDescribe a situation where you ((took the initiative** to improve a process or approach within your work. What was the outcome?\n\nHighlight examples of process improvement, innovation, or contributions to more efficient study designs or analysis methods.\nCase: renovating statistical course with R and Quarto\n\nS:\nT:\nA:\nR:\n\n\n\n\nTell me about a time when you had to lead or mentor junior statisticians or team members during a study. How did you ensure they were aligned with the study’s goals and protocols?\n\nDemonstrate your leadership, mentoring, and coaching skills.\nCase: noreden\n\nS:\nT:\nA: set out plan for the r programming part, set up schedule to sit together to work on problems, do quality check in the end; create documentation website for collaboration\nR:\n\n\n\n\nDealing with Challenges or Failures\nManage unexpected challenges; learn from mistakes or trial failures\nResilience, critical thinking, learning from setbacks, making informed decisions\n\n\nCan you tell me about a project that did not go as expected due to statistical issues or challenges? How did you handle the situation and what did you learn from it?\n\nShowcase your resilience, learning from setbacks, and steps taken to mitigate future issues.\nCase: phd paper 3 which didn’t go as planned\n\nS:\nT:\nA: use time in a better way, pivot to something else\nR: lesson learned: better planning would be ideal; find things to do;\n\n\n\n\nImportant!Describe a time when you had to make a difficult decision during a study, such as adjusting the analysis approach or advising the study to stop early based on statistical results. What was the outcome?\n\nFocus on your critical thinking, ethical decision-making, and impact on trial success.\nCase: phd paper 3 which didn’t go as planned\n\nS:\nT:\nA:\nR:\n\n\n\n\nAttention to Detail & Accuracy\nIdentify and correct errors; ensure precision in statistical work\nMeticulousness, responsibility, focus on ensuring trial success through accuracy\n\n\nTell me about a situation where you caught a mistake in your own statistical work or in that of a colleague. How did you handle it and what was the result?\n\nHighlight your meticulous attention to detail and responsibility in ensuring data accuracy.\nCase: COVITA study, table making\n\nS:\nT: review the paper before submission\nA: tell the first author that it’s crucial to change it\nR:\n\nCase: CB study, PCA analysis\n\nS:\nT: reproduce the analysis made by someone else, unable to do so\nA: tell the first author that they need to revise\nR:\n\n\n\n\nDescribe a time when your attention to detail significantly impacted the success or outcome of a study. What was at stake, and how did your actions make a difference?\n\nEmphasize the importance of precision in statistical analysis and its effect on trial outcomes.\nCase: COVITA study, table making\n\nS:\nT:\nA:\nR:\n\n\n\n\nAdaptation to Tools & Technologies\nLearn new software/tools for trial analysis Quick learning, adaptability, proficiency with statistical tools (e.g., SAS, R)\n\n\nCan you tell me about a time when you had to learn and implement a new statistical software or technology for a clinical trial? How did you manage the learning curve?\n\nDiscuss your ability to quickly adapt to new tools, such as SAS, R, or other clinical trial software.\n\nUse of AI\n\nCase: course website with R and quarto\n\nS:\nT:\nA:\nR:\n\n\n\n\n\nLess relevant\n\nWork Under Regulatory Pressure\nManage audits or regulatory scrutiny; maintain accuracy under pressure\nComposure, stress management, maintaining statistical precision under high stakes\n\n\nDescribe a situation where you had to manage high levels of regulatory scrutiny or audit during a clinical trial. How did you handle the stress, and how did it affect your statistical work?\n\nTalk about maintaining accuracy and composure under pressure, especially in high-stakes audits.\n\nS:\nT:\nA:\nR:"
  }
]