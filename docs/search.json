[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Apothecary’s Notes",
    "section": "",
    "text": "Github\n  \n\n  \n  \nHello, welcome!\nThis is the note repository for modern data science skills with a focus on drug development and clinical trials. Content will be gradually added while I learn the topics. Therefore, it is by no means a complete guide by the time you read it!\nI try to organize the content in a modular way. I think these should cover the important aspects in which a data scientist / modern statistician should know.\n\nstudy design\ninference\nmodels\nreporting\nprogramming\n\nAny feedback / mistake corrections are welcome, you can find me here. Thank you!"
  },
  {
    "objectID": "documentation/understand_data.html",
    "href": "documentation/understand_data.html",
    "title": "Part 1: Understand the current diet",
    "section": "",
    "text": "Load the aggregated food groups and their attributes. We have 28 food groups.\n\nfoods <- read.csv('data/foods.csv', sep = ',')\ndata.table::setDT(foods) # use data.table format\nfoods[1:10,] # show the first 10\n\n                   food intake energy protein   fat carbs sugar alcohol  ghge\n 1:               Bread  175.4 10.696   0.091 0.030 0.441 0.002       0 0.001\n 2:        Other grains   45.0 14.022   0.100 0.042 0.607 0.011       0 0.002\n 3:               Cakes   35.6 14.185   0.067 0.152 0.424 0.185       0 0.002\n 4:            Potatoes   67.8  3.791   0.021 0.007 0.178 0.000       0 0.000\n 5:          Vegetables  154.6  1.565   0.015 0.008 0.050 0.005       0 0.001\n 6:             Legumes    3.5  8.571   0.143 0.029 0.286 0.000       0 0.001\n 7:      Fruit, berries  171.5  2.729   0.008 0.004 0.134 0.029       0 0.001\n 8:               Juice  111.0  1.928   0.005 0.002 0.103 0.000       0 0.001\n 9:                Nuts    4.3 25.581   0.209 0.535 0.116 0.000       0 0.005\n10: Vegetarian products    0.7  4.286   0.143 0.000 0.000 0.000       0 0.003\n\n\nDefine the constraints on\n\nenergy\nprotein, fat, carbs, sugar, alcohol\nGHGE (greenhouse gas equivalent)\n\n\nconstraints <- data.frame(\n  constraint = c('lower', 'upper'), \n  energy = c(9000, 10000), \n  protein = c(55, 111.5), \n  fat = c(61.8, 98.8), \n  carbs = c(250, 334.6), \n  sugar = c(0, 54.8), \n  alcohol = c(0, 10),\n  ghge = c(0, 4.7)\n)\nconstraints\n\n  constraint energy protein  fat carbs sugar alcohol ghge\n1      lower   9000    55.0 61.8 250.0   0.0       0  0.0\n2      upper  10000   111.5 98.8 334.6  54.8      10  4.7"
  },
  {
    "objectID": "documentation/index.html",
    "href": "documentation/index.html",
    "title": "Documentation",
    "section": "",
    "text": "It is important to document while you go.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nPart 1: Understand the current diet\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "What is Data Apothecary’s Notes?",
    "section": "",
    "text": "Hello, welcome!\nThis is the note repository for some documentation and demonstration on modern data science skills related to drug development.\nI created this repo so that I can find notes with code and rendered results easily. Therefore, the target audience is the future-me; if it helps you in some way as well, I’d be more than happy!\nIf you see some errors or have any suggestions, please feel free to reach out.\nFind out who I am here!"
  },
  {
    "objectID": "reporting/index.html",
    "href": "reporting/index.html",
    "title": "Reporting",
    "section": "",
    "text": "It is important to document while you go.\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nClinical trial: reporting results\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "reporting/understand_data.html",
    "href": "reporting/understand_data.html",
    "title": "Part 1: Understand the current diet",
    "section": "",
    "text": "Load the aggregated food groups and their attributes. We have 28 food groups.\n\nfoods <- read.csv('data/foods.csv', sep = ',')\ndata.table::setDT(foods) # use data.table format\nfoods[1:10,] # show the first 10\n\n                   food intake energy protein   fat carbs sugar alcohol  ghge\n 1:               Bread  175.4 10.696   0.091 0.030 0.441 0.002       0 0.001\n 2:        Other grains   45.0 14.022   0.100 0.042 0.607 0.011       0 0.002\n 3:               Cakes   35.6 14.185   0.067 0.152 0.424 0.185       0 0.002\n 4:            Potatoes   67.8  3.791   0.021 0.007 0.178 0.000       0 0.000\n 5:          Vegetables  154.6  1.565   0.015 0.008 0.050 0.005       0 0.001\n 6:             Legumes    3.5  8.571   0.143 0.029 0.286 0.000       0 0.001\n 7:      Fruit, berries  171.5  2.729   0.008 0.004 0.134 0.029       0 0.001\n 8:               Juice  111.0  1.928   0.005 0.002 0.103 0.000       0 0.001\n 9:                Nuts    4.3 25.581   0.209 0.535 0.116 0.000       0 0.005\n10: Vegetarian products    0.7  4.286   0.143 0.000 0.000 0.000       0 0.003\n\n\nDefine the constraints on\n\nenergy\nprotein, fat, carbs, sugar, alcohol\nGHGE (greenhouse gas equivalent)\n\n\nconstraints <- data.frame(\n  constraint = c('lower', 'upper'), \n  energy = c(9000, 10000), \n  protein = c(55, 111.5), \n  fat = c(61.8, 98.8), \n  carbs = c(250, 334.6), \n  sugar = c(0, 54.8), \n  alcohol = c(0, 10),\n  ghge = c(0, 4.7)\n)\nconstraints\n\n  constraint energy protein  fat carbs sugar alcohol ghge\n1      lower   9000    55.0 61.8 250.0   0.0       0  0.0\n2      upper  10000   111.5 98.8 334.6  54.8      10  4.7"
  },
  {
    "objectID": "models/index.html",
    "href": "models/index.html",
    "title": "Models",
    "section": "",
    "text": "Models for repeated measurement, longitudinal data and related matters.\nOr more precisely: different endpoints\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nMixed models for repeted measurements\n\n\n\n\nSurvival\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "models/understand_data.html",
    "href": "models/understand_data.html",
    "title": "Mixed models",
    "section": "",
    "text": "Yet to be filled in"
  },
  {
    "objectID": "study_design/index.html",
    "href": "study_design/index.html",
    "title": "Study design",
    "section": "",
    "text": "Topics (ongoing)\n\nClinical trial design\n\nPhase I, II, III\nadaptive design\n\nSample size calculation\n\ncomparing a few groups (visualization TBD)\nregression (LR, GLM)\nmore advanced model (e.g. GLMM)\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nAdaptive design: overview\n\n\nIntro to adaptive design \n\n\n\n\nSample size (part I)\n\n\nOverview, mean and proportion comparison \n\n\n\n\nSample size (part II)\n\n\nRegression \n\n\n\n\nClinical trial design: overview\n\n\nNotes related to clinical trial design. \n\n\n\n\n\nNo matching items\n\n\n\nCHECK\nhttps://stats.stackexchange.com/questions/48374/sample-size-calculation-for-mixed-models\nICH Guidelines (EMA)\nhttps://www.ema.europa.eu/en/ich-e10-choice-control-group-clinical-trials-scientific-guideline#current-effective-version-section\nhttps://www.ema.europa.eu/en/documents/scientific-guideline/ich-e-10-choice-control-group-clinical-trials-step-5_en.pdf\nhttps://clinicaltrials.gov"
  },
  {
    "objectID": "study_design/understand_data.html",
    "href": "study_design/understand_data.html",
    "title": "Part 1: Understand the current diet",
    "section": "",
    "text": "Load the aggregated food groups and their attributes. We have 28 food groups.\n\nfoods <- read.csv('data/foods.csv', sep = ',')\ndata.table::setDT(foods) # use data.table format\nfoods[1:10,] # show the first 10\n\n                   food intake energy protein   fat carbs sugar alcohol  ghge\n 1:               Bread  175.4 10.696   0.091 0.030 0.441 0.002       0 0.001\n 2:        Other grains   45.0 14.022   0.100 0.042 0.607 0.011       0 0.002\n 3:               Cakes   35.6 14.185   0.067 0.152 0.424 0.185       0 0.002\n 4:            Potatoes   67.8  3.791   0.021 0.007 0.178 0.000       0 0.000\n 5:          Vegetables  154.6  1.565   0.015 0.008 0.050 0.005       0 0.001\n 6:             Legumes    3.5  8.571   0.143 0.029 0.286 0.000       0 0.001\n 7:      Fruit, berries  171.5  2.729   0.008 0.004 0.134 0.029       0 0.001\n 8:               Juice  111.0  1.928   0.005 0.002 0.103 0.000       0 0.001\n 9:                Nuts    4.3 25.581   0.209 0.535 0.116 0.000       0 0.005\n10: Vegetarian products    0.7  4.286   0.143 0.000 0.000 0.000       0 0.003\n\n\nDefine the constraints on\n\nenergy\nprotein, fat, carbs, sugar, alcohol\nGHGE (greenhouse gas equivalent)\n\n\nconstraints <- data.frame(\n  constraint = c('lower', 'upper'), \n  energy = c(9000, 10000), \n  protein = c(55, 111.5), \n  fat = c(61.8, 98.8), \n  carbs = c(250, 334.6), \n  sugar = c(0, 54.8), \n  alcohol = c(0, 10),\n  ghge = c(0, 4.7)\n)\nconstraints\n\n  constraint energy protein  fat carbs sugar alcohol ghge\n1      lower   9000    55.0 61.8 250.0   0.0       0  0.0\n2      upper  10000   111.5 98.8 334.6  54.8      10  4.7"
  },
  {
    "objectID": "programming/index.html",
    "href": "programming/index.html",
    "title": "Programming",
    "section": "",
    "text": "This page hosts study notes on programming, with a strong flavour of R.\n\nR programming\n\n\n\n\n\nTitle\n\n\n\n\n\n\nError handling in R\n\n\n\n\nFunctions\n\n\n\n\nOOP in R: S3\n\n\n\n\n\nNo matching items\n\n\n\n\nR package\n\n\n\n\n\nTitle\n\n\n\n\n\n\nDependencies\n\n\n\n\nMy workflow in working with functions in a package\n\n\n\n\nR package Engineering Workflow\n\n\n\n\nR package: tests\n\n\n\n\n\nNo matching items\n\n\n\n\nWeb and quarto\n\n\n\n\n\nTitle\n\n\n\n\n\n\nMake slides with reveal.js\n\n\n\n\nWeb basics\n\n\n\n\nWebR: Use with an existing quarto website\n\n\n\n\n\nNo matching items\n\n\n\n\nOther topics\n\n\n\n\n\nTitle\n\n\n\n\n\n\nSet up version control\n\n\n\n\nSoftware papers\n\n\n\n\nTopic coverage: R developer\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "programming/understand_data.html",
    "href": "programming/understand_data.html",
    "title": "Part 1: Understand the current diet",
    "section": "",
    "text": "Load the aggregated food groups and their attributes. We have 28 food groups.\n\nfoods <- read.csv('data/foods.csv', sep = ',')\ndata.table::setDT(foods) # use data.table format\nfoods[1:10,] # show the first 10\n\n                   food intake energy protein   fat carbs sugar alcohol  ghge\n 1:               Bread  175.4 10.696   0.091 0.030 0.441 0.002       0 0.001\n 2:        Other grains   45.0 14.022   0.100 0.042 0.607 0.011       0 0.002\n 3:               Cakes   35.6 14.185   0.067 0.152 0.424 0.185       0 0.002\n 4:            Potatoes   67.8  3.791   0.021 0.007 0.178 0.000       0 0.000\n 5:          Vegetables  154.6  1.565   0.015 0.008 0.050 0.005       0 0.001\n 6:             Legumes    3.5  8.571   0.143 0.029 0.286 0.000       0 0.001\n 7:      Fruit, berries  171.5  2.729   0.008 0.004 0.134 0.029       0 0.001\n 8:               Juice  111.0  1.928   0.005 0.002 0.103 0.000       0 0.001\n 9:                Nuts    4.3 25.581   0.209 0.535 0.116 0.000       0 0.005\n10: Vegetarian products    0.7  4.286   0.143 0.000 0.000 0.000       0 0.003\n\n\nDefine the constraints on\n\nenergy\nprotein, fat, carbs, sugar, alcohol\nGHGE (greenhouse gas equivalent)\n\n\nconstraints <- data.frame(\n  constraint = c('lower', 'upper'), \n  energy = c(9000, 10000), \n  protein = c(55, 111.5), \n  fat = c(61.8, 98.8), \n  carbs = c(250, 334.6), \n  sugar = c(0, 54.8), \n  alcohol = c(0, 10),\n  ghge = c(0, 4.7)\n)\nconstraints\n\n  constraint energy protein  fat carbs sugar alcohol ghge\n1      lower   9000    55.0 61.8 250.0   0.0       0  0.0\n2      upper  10000   111.5 98.8 334.6  54.8      10  4.7"
  },
  {
    "objectID": "inference/index.html",
    "href": "inference/index.html",
    "title": "Inference and models",
    "section": "",
    "text": "Different settings to apply methods.\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nReal-world Data, Real-world Evidence\n\n\nRWD, RWE \n\n\n\n\nGenomics in Drug Discovery\n\n\nUse of machine learning techniques \n\n\n\n\nAntibiotics\n\n\nBackground of antimicrobial drugs and resistance \n\n\n\n\nRWD EHR Vendor Engagement\n\n\nOverview of vendor engagement \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "inference/understand_data.html",
    "href": "inference/understand_data.html",
    "title": "(Placeholder)",
    "section": "",
    "text": "Define the constraints on\n\nenergy\nprotein, fat, carbs, sugar, alcohol\nGHGE (greenhouse gas equivalent)\n\n\nconstraints <- data.frame(\n  constraint = c('lower', 'upper'), \n  energy = c(9000, 10000), \n  protein = c(55, 111.5), \n  fat = c(61.8, 98.8), \n  carbs = c(250, 334.6), \n  sugar = c(0, 54.8), \n  alcohol = c(0, 10),\n  ghge = c(0, 4.7)\n)\nconstraints\n\n  constraint energy protein  fat carbs sugar alcohol ghge\n1      lower   9000    55.0 61.8 250.0   0.0       0  0.0\n2      upper  10000   111.5 98.8 334.6  54.8      10  4.7"
  },
  {
    "objectID": "study_design/methodnotes_20230601_trialdesign/index.html",
    "href": "study_design/methodnotes_20230601_trialdesign/index.html",
    "title": "Clinical trial design",
    "section": "",
    "text": "Coursera course Design and interpretation of clinical trials by Johns Hopkins University"
  },
  {
    "objectID": "study_design/methodnotes_20230601_trialdesign/index.html#types-of-trials-designs",
    "href": "study_design/methodnotes_20230601_trialdesign/index.html#types-of-trials-designs",
    "title": "Clinical trial design",
    "section": "Types of trials designs",
    "text": "Types of trials designs\nPhase 1: 10-30, identify tolerable dose, information on drug metabolism, extretion and toxicity. Often not controlled\nPhase 2: 30-100, efficacy, safety and side effects,\nPhase 3: 100+, often randomized\nPhase 4: demonstration\n\nTypes of design\nPopulation have the disease outcome of interest; not healthy voluteers vs diseased.\nRandomisation unit: persons, two eyes of a person, or groups of persons\nComparison structure: parallel, crossover, group allocation\n\nParallel: simultaneous treatment and control groups, subjects randomly assigned to one group.\nCrossover: randomize of order in which treatments are received; TC or CT. Each patient is his/her own control. Washout period: time between two treatments.\n\nVariability reduced because less variability within patient than between patients. Fewer patients needed.\nDisadvantages: only certain treatments can use crossover design, treatment can’t have permanent effects. Carry-over effects from first period; washout needs to be long enough. Dropouts more significant, analysis may be more difficult: correlated outcomes.\nConstant intensity of underlying disease: chronic diseases (e.g. asthma, hypertension, arthritis) + short-term treatment effects (relief of signs or symptoms)\ne.g. morning dose vs evening dose\n\nGroup allocation: a group of subjects (community, school, clinic).\n\nExtensions of the parallel design: factorial, large simple\n\nFactorial: two interventions tested simultaneously. Can be presented in a 2 by 2 table (treatment A +-, treatment B +-); or 3 by 2 etc.\n\nInterested in main effect (if no interaction expected). A vs no A; B vs no B. The other treatment doesn’t matter.\n\nLarge simple: large number of patients, possibly from many study sites.\n\n\nTests other than superiority\n\nEquivalency: intervention response is close to control group response\nNon-inferiority: Treatment A (new) is at least as good as B (established). One-sided test, if A is worse than B, one can be rejected. Does not require as big sample size.\n\n\n\nAdaptive design\nPossible adaptations\n\nrandomization probabilities\nsample size (e.g. group sequential methods)\nvisit schedule: shorten/lengthen follow-up time, change number of timing of visits, treatments (dose/duration, concomitant meds)\nhypothesis tested"
  },
  {
    "objectID": "study_design/methodnotes_20230601_trialdesign/index.html#randomisation-and-masking",
    "href": "study_design/methodnotes_20230601_trialdesign/index.html#randomisation-and-masking",
    "title": "Clinical trial design",
    "section": "Randomisation and masking",
    "text": "Randomisation and masking\nRationale:\n\navoid selection bias: prognostic factors related to treatment assignment\ntends to produce comparable treatment groups\n\n\nSchemes\nSimple randomization, restricted randomization, adaptive randomization\n\nSimple rz\nEach assignment is unpredictable, number of patients in each group should be equal in the long run.\nRisks: imbalances in number assigned to treatment groups, or confounding factors (gender, disease severity) -> reduced power\n\n\nRestricted rz\nSchemes with constraints to produce expected assignment ratio\n\nblocking\nstratification\n\nBlocking. Block of size 2 with treatment allocation ratio 1:1: A,B. Size 4: 2As, 2Bs. Need to be permuted: AABB, ABAB, … in total 6 combinations. Then choose one of the permutations.\nStratification. Ensure balance in treatment assignments with subgroups defined before rz. Limit to a few variables (highly related to outcome and/or logistical): e.g. clinic in a multicenter trial, surgeon (skills, procedures), stage of disease, demographic such as gender and age.\nUse these two together.\n\n\nAdaptive rz\nProbability of assignment does not remain constant, but determined by the current balance and composition of the groups.\n\nminimization: choose the design that gives the smallest imbalance.\nplay the winner: change allocation ratio or favor the better treatment based on the primary outcome. Need to evaluate outcomes relatively quickly.\n\n\n\n\nMasking (blinding)\nTreatment assignment is not known after rz.\n\npatient, clinical personnel, evaluators, data processors, …\nsingle (only participant), double (+ investigator), triple (+ data processors, …), quadruple …\n\nPurpose: remove bias related to treatment effects.\nDifferent levels of masking protects to different extent against bias in different aspects\n\ndata reporting\ndata collection / follow-up\ntesting, behaviors\noutcome assessment\n\nDecision to mask treatments\n\nethical?\npossible? can you make the treatment seem identical so the participants do not know?\ntrial design features: more important to mask subjective ones (e.g. alive or dead is the least subjective, hence wouldn’t benefit much; however if participants need to report effects that are not objectively measureable, they might report that treatment is better in contrast to placebo group)\nfeasible? cost-benefit, practicality (adherence)\n\nSometimes investigators in a double blind study might know which treatment is being assigned to participants, if the effect of drug is very obvious (both good or bad).\nUnmasking\n\nPlanned: inform participants once the trial finished\nUnplanned (discouraged): in the event of adverse event"
  },
  {
    "objectID": "study_design/methodnotes_20230601_trialdesign/index.html#outcomes-and-analysis",
    "href": "study_design/methodnotes_20230601_trialdesign/index.html#outcomes-and-analysis",
    "title": "Clinical trial design",
    "section": "Outcomes and analysis",
    "text": "Outcomes and analysis\nOutcome: endpoint. It is a quantitaive measure.\nObjectives of the trial\n\nefficacy / effectiveness\nsafety\nprocess\ncosts\n\nExample: evaluate treatment for asthma\nOutcomes: exhaled nitrous oxide, lung function (spirometry measures), asthma symptoms (wheezing, night awakenings), …\nExample: evaluate a procedure to reduce perioperative morbidity\nOutcome considerations: time window (what is postoperative), specific events to be considered an outcome, procedures to establish outcomes, …\n\nMetrics for events as outcomes\n\ndichotomous: 1/0 for presence absense, normal abnormal; clinical state or cut-off value\ntime-to-event: in addition to dichotomous, add time dimension; allow for censoring. More powerful than dichotomous.\nrates: 1/0 but allow for repeats, analyze count or rate. Events within a person are usually not independent, need to account for it.\ncontinuous variables: value or change from baseline; standard units (lab values, scores). Need to define an important difference. Distributional assumptions more important.\nordinal scale: ranked categories (e.g. adverse event grading, 1-5). Difference between categories is usually qualitative.\n\nPatients opinions are subjective\n\nhealth status / change in status, e.g. pain relief, quality of life\nmasking is more important\nhawthorne / placebo effect: effect of being studies, usually positive\nquantify with standardized scales\n\n\n\nInfluence of outcomes on design\nEfficacy vs effectiveness:\nIn a vaccine trial, efficacy is the clinical case with lab confirmation; effectivenenss is the clinical case of influenza in a larger population, may or may not be confirmed.\nIn asthma, efficacy is FEV1, effectiveness is the decrease of the hospitalizations/steroid courses.\nConsiderations (3Bs)\n\nbiology: does outcome reflect a clinically relevant fact/change\nbiostatistics: detectable difference between groups is plausible and practical\nbudget: afford total N and can measure it reliably in every participant\n\nExample: HIV trial outcomes\n\nsurvival (deaths; AIDS status)\nimmunologic response\nvirologic response\nchange in patient status (e.g QoL)\nspecified toxicity\nother side effects\n\nChoice of primary outcome depends on the objectives or stage of research\n\nphase 1, emphasis on safety\nphase 2, short-term efficacy\nphase 3, long-term efficacy\nphase 4, long-term effectiveness\n\n\n\nIntention to treat ITT\nCross-overs after rz: some patients might have a treatment (yes or no) beyond what they were assigned to, e.g. refuse surgery or medical treatment\nNon-adherence during followup: some in treatment group refuse of can not tolerate certain treatment; while some in placebo group require medication or take on their own\n\n\nSubgroup analysis\nStratified analysis: estimate treatment effect separately in subgroups. Does not tell difference across different subgroups\nTest for interaction: use of main effect and interaction.\nIssue of multiple testing when doing a series of analyses\n\ninflate sample size to plan for subgroup analysis\nreport number of subgroup analyses performed\npossibly adjust for multiple comparisons\nreport CI instead of just p-values"
  },
  {
    "objectID": "study_design/methodnotes_20230601_trialdesign/index.html#reporting-results",
    "href": "study_design/methodnotes_20230601_trialdesign/index.html#reporting-results",
    "title": "Clinical trial design",
    "section": "Reporting results",
    "text": "Reporting results\n\nCONSORT\nConsolidated Standards of Reporting Trials\n25 item checklist. Not a guideline of how to do the trial; but a guideline for writers on how to report clearly.\nTitle\n\nkey design items\ntreatment(s) evaluated\ndisease or population studied\n\nAbstract\n\ndesign, method, results, conclusions\n\nIntroduction\n\nbackgrounds, rationale, establish equipoise (balance), systematic review, objectives / hypothesis\n\nMethod\n\nIRB review and approvals\ntrial design, allocation ratio\neligibility criteria\nsetting and location of the trial\nintervention - detailed enough to allow replication\noutcomes - primary, secondary, how assessed and defined\nsample size - how determined, interim analyses\nimportant changes during trial\nrandomization, allocation concealment, implementation\nmasking - who, how\nstatistical methods - primary and secondary; subgroup analyses\n\nConvenient to use a flow-chart\nBaseline characteristics by treatment group\np-values (common to not report in rct?)\n\n\nEvaluate literature\n\nLegitimacy, is it a fair comparison\nTrustworthy investigators? conflict of interest\nAdequate protections against bias\n\nrandomization\nmasking\nfollow-up design and execution\n\nITT analysis\n\nhave all events (outcomes) observed been counted in the treatment group assigned?\nvariations in denominators explained and consistent with good practice?\n\nappropriate subgroup analysis interpretation\n\nad hoc or post hoc status"
  },
  {
    "objectID": "programming/r_roop.html",
    "href": "programming/r_roop.html",
    "title": "OOP in R: S3",
    "section": "",
    "text": "Useful references:"
  },
  {
    "objectID": "programming/r_roop.html#terminology",
    "href": "programming/r_roop.html#terminology",
    "title": "OOP in R: S3",
    "section": "Terminology",
    "text": "Terminology\n\nObject: individual instances of a class\nClass: type of an object, i.e. what an object is\nMethod: a function associated with a particular class, i.e. what the object can do\n\ngeneric method: mean() of a vector of numbers is a number, mean() of a vector of dates is a date\nInherit: a sub-class inherits all the attributes and methods from the super-class. E.g. generalized linear model inherits from a linear model.\nmethod dispatch: the process of finding the correct method given a class\n\n\nEncapsulated OOP:\n\nmethods belong to object or classes\nobject.method(arg1, arg2)\ncommon in most languages\nR6, RC (reference class) are examples of this type\n\nFunctional OOP:\n\nmethods belong to generic functions\ngeneric(object, arg2, arg3)\nS3 is an informal implementation of this type"
  },
  {
    "objectID": "programming/r_roop.html#base-types",
    "href": "programming/r_roop.html#base-types",
    "title": "OOP in R: S3",
    "section": "Base types",
    "text": "Base types\nCheck whether an object is object-oriented, or base object:\n\nis.object()\nsloop::otype(): returns base or S3/S4\nattr(obj_name, 'class'): OO objects has a class attribute, BO does not.\n\n\nx &lt;- 1:10 # a numeric vector\ny &lt;- factor(c('a', 'b'))  # a factor\n\nc(is.object(x), is.object(y))\n\n[1] FALSE  TRUE\n\nc(sloop::otype(x), sloop::otype(y))\n\n[1] \"base\" \"S3\"  \n\nattr(x, 'class') \n\nNULL\n\nattr(y, 'class')\n\n[1] \"factor\"\n\n\nAll objects have a base type; not all are OO objects.\n\ntypeof(1:10) returns ‘integer’\n25 base types in total\n\nvectors: e.g. NULL, logical, integer, double, complex, character, list, raw\nfunctions: e.g. closure, special, builtin\nenvironments: environment\nS4: S4\nlanguage components, symbol, language, pairlist the rest are less common."
  },
  {
    "objectID": "programming/r_roop.html#generic-or-method",
    "href": "programming/r_roop.html#generic-or-method",
    "title": "OOP in R: S3",
    "section": "Generic or method?",
    "text": "Generic or method?\n\ngeneric.class(), for example: print.factor()\ndo not call the method directly; use the generic (dispatch) to find it.\ngenerally has the . in the name; however it is not guaranteed * t.test() is a generic like print(), as t.test() can be used on multiple types of inputs\n\nas.factor() is not an OO object, hence not S3\n\n\n\nCheck function type with sloop::ftype()\n\nsloop::ftype(predict) # predict is a generic\n\n[1] \"S3\"      \"generic\"\n\nsloop::ftype(predict.glm)  # glm (class) method for predict() generic\n\n[1] \"S3\"     \"method\"\n\n\n\n\nCheck methods with methods()\nmethods() checks all the methods that either:\n\nbelongs to a generic (the function), such as plot, predict, t.test\nbelongs to a class (the type of input), such as lm, ar\n\n\nmethods('predict')  \n\n [1] predict.ar*                predict.Arima*            \n [3] predict.arima0*            predict.glm               \n [5] predict.HoltWinters*       predict.lm                \n [7] predict.loess*             predict.mlm*              \n [9] predict.nls*               predict.poly*             \n[11] predict.ppr*               predict.prcomp*           \n[13] predict.princomp*          predict.smooth.spline*    \n[15] predict.smooth.spline.fit* predict.StructTS*         \nsee '?methods' for accessing help and source code\n\nmethods(class = 'lm')\n\n [1] add1           alias          anova          case.names     coerce        \n [6] confint        cooks.distance deviance       dfbeta         dfbetas       \n[11] drop1          dummy.coef     effects        extractAIC     family        \n[16] formula        hatvalues      influence      initialize     kappa         \n[21] labels         logLik         model.frame    model.matrix   nobs          \n[26] plot           predict        print          proj           qr            \n[31] residuals      rstandard      rstudent       show           simulate      \n[36] slotsFromS3    summary        variable.names vcov          \nsee '?methods' for accessing help and source code\n\n\nEquivalently, use sloop::s3_methods_*(), as it gives more information in the output.\n\nsloop::s3_methods_generic('predict') \n\n# A tibble: 16 × 4\n   generic class             visible source             \n   &lt;chr&gt;   &lt;chr&gt;             &lt;lgl&gt;   &lt;chr&gt;              \n 1 predict ar                FALSE   registered S3method\n 2 predict Arima             FALSE   registered S3method\n 3 predict arima0            FALSE   registered S3method\n 4 predict glm               TRUE    stats              \n 5 predict HoltWinters       FALSE   registered S3method\n 6 predict lm                TRUE    stats              \n 7 predict loess             FALSE   registered S3method\n 8 predict mlm               FALSE   registered S3method\n 9 predict nls               FALSE   registered S3method\n10 predict poly              FALSE   registered S3method\n11 predict ppr               FALSE   registered S3method\n12 predict prcomp            FALSE   registered S3method\n13 predict princomp          FALSE   registered S3method\n14 predict smooth.spline     FALSE   registered S3method\n15 predict smooth.spline.fit FALSE   registered S3method\n16 predict StructTS          FALSE   registered S3method\n\nsloop::s3_methods_class('lm')\n\n# A tibble: 35 × 4\n   generic        class visible source             \n   &lt;chr&gt;          &lt;chr&gt; &lt;lgl&gt;   &lt;chr&gt;              \n 1 add1           lm    FALSE   registered S3method\n 2 alias          lm    FALSE   registered S3method\n 3 anova          lm    FALSE   registered S3method\n 4 case.names     lm    FALSE   registered S3method\n 5 confint        lm    TRUE    stats              \n 6 cooks.distance lm    FALSE   registered S3method\n 7 deviance       lm    FALSE   registered S3method\n 8 dfbeta         lm    FALSE   registered S3method\n 9 dfbetas        lm    FALSE   registered S3method\n10 drop1          lm    FALSE   registered S3method\n# ℹ 25 more rows"
  },
  {
    "objectID": "programming/r_roop.html#class-assignment",
    "href": "programming/r_roop.html#class-assignment",
    "title": "OOP in R: S3",
    "section": "Class assignment",
    "text": "Class assignment\nTwo options: structure(), or class(existing_obj)\n\nsimple_number &lt;- structure(1, class = 'simple')\nclass(simple_number)\n\n[1] \"simple\"\n\n\nOr, you can do it for an existing object by giving it a class\n\nsimple_char &lt;- 'your_name'\nclass(simple_char) &lt;- 'simple'\nclass(simple_char)\n\n[1] \"simple\""
  },
  {
    "objectID": "programming/r_roop.html#constructor",
    "href": "programming/r_roop.html#constructor",
    "title": "OOP in R: S3",
    "section": "Constructor",
    "text": "Constructor\n\nfruit &lt;- function(x){\n  stopifnot(is.character(x))\n  # checks if x is char\n  # better use a named list, easier to call\n  structure(list(fruit_name = x), class = 'fruit') \n}\n\nfruit1 &lt;- fruit('pineapple')\nfruit2 &lt;- fruit('apple')\n\nExamine what comes out\n\nfruit1\n\n$fruit_name\n[1] \"pineapple\"\n\nattr(,\"class\")\n[1] \"fruit\""
  },
  {
    "objectID": "programming/r_roop.html#define-new-generic-and-method",
    "href": "programming/r_roop.html#define-new-generic-and-method",
    "title": "OOP in R: S3",
    "section": "Define new generic and method",
    "text": "Define new generic and method\n[name of method] &lt;- functionn(x){UseMethod(\"[name of method]\")}\nNow we define one generic function f, and two methods. One for class plus2, and another for class plus10.\n\nf &lt;- function(x){UseMethod('f')} # define generic f\nf.plus2 &lt;- function(x) x+2 # f method for class plus2\nf.plus10 &lt;- function(x) x+10 # f method for class plus10\n\nNow we try to give the function some input. First use a numeric number, 1 (the class for a number is double and numeric).\n\nnumber &lt;- 1\nf(number) # returns error, class of number does not match!\n\nError in UseMethod(\"f\"): no applicable method for 'f' applied to an object of class \"c('double', 'numeric')\"\n\n\nThis returns an error, because the class of number is not defined for function f (plus2, plus10).\n\n# can check what f(number) tried \n# none of these exist \nsloop::s3_dispatch(f(number))\n\n   f.double\n   f.numeric\n   f.default\n\n\nWe need to match it. Assign the number with plus2 class, and evaluate it. You can check which method has been used (dispatched).\n\n# fix: assign a class to number\nclass(number) &lt;- 'plus2'\nf(number) # number+2, f.plus2 method\n\n[1] 3\nattr(,\"class\")\n[1] \"plus2\"\n\nsloop::s3_dispatch(f(number))\n\n=&gt; f.plus2\n   f.default\n\n\nNow we try another number, but let it be plus10 class.\n\nnumberx &lt;- 200\nclass(numberx) &lt;- 'plus10'\nf(numberx)\n\n[1] 210\nattr(,\"class\")\n[1] \"plus10\"\n\nsloop::s3_dispatch(f(numberx))\n\n=&gt; f.plus10\n   f.default"
  },
  {
    "objectID": "programming/r_roop.html#new-method-for-existing-generic-print",
    "href": "programming/r_roop.html#new-method-for-existing-generic-print",
    "title": "OOP in R: S3",
    "section": "New method for existing generic (print())",
    "text": "New method for existing generic (print())\nWe create the S3 object using the constructor defined above, fruit().\n\npineapple &lt;- fruit('pineapple') # create by the constructor\npineapple\n\n$fruit_name\n[1] \"pineapple\"\n\nattr(,\"class\")\n[1] \"fruit\"\n\n\nThe output does not look very nice, we can modify what prints out. Since print() is an exisiting generic function, we do not need to define a new one (i.e. UseMethod). We define the new method directly: generic.your_class.\n\n# we do not need to define print() as generic, bec it IS already\n# directly define print.fruit\nprint.fruit &lt;- function(x){\n  cat('I used constructor for my fruit:', x$fruit_name)\n}\n\nprint.fruit(pineapple)\n\nI used constructor for my fruit: pineapple"
  },
  {
    "objectID": "study_design/rct_design_overview.html",
    "href": "study_design/rct_design_overview.html",
    "title": "Clinical trial design: overview",
    "section": "",
    "text": "Considerations: choice of comparator and trial outcome measures (due to the grrowing number of treatmenet options, standard care would change over time), annd definition of target patient population (e.g. molecular profiling makes it possible to identify smaller subgroups of patient with defined tumor type)."
  },
  {
    "objectID": "study_design/rct_design_overview.html#types-of-trials-designs",
    "href": "study_design/rct_design_overview.html#types-of-trials-designs",
    "title": "Clinical trial design: overview",
    "section": "Types of trials designs",
    "text": "Types of trials designs\nPhase 1: 10-30, identify tolerable dose, information on drug metabolism, extretion and toxicity. Often not controlled\nPhase 2: 30-100, efficacy, safety and side effects,\nPhase 3: 100+, often randomized\nPhase 4: demonstration\n\nTypes of design\nPopulation have the disease outcome of interest; not healthy voluteers vs diseased.\nRandomisation unit: persons, two eyes of a person, or groups of persons\nComparison structure: parallel, crossover, group allocation\n\nParallel: simultaneous treatment and control groups, subjects randomly assigned to one group.\nCrossover: randomize of order in which treatments are received; TC or CT. Each patient is his/her own control. Washout period: time between two treatments.\n\nVariability reduced because less variability within patient than between patients. Fewer patients needed.\nDisadvantages: only certain treatments can use crossover design, treatment can’t have permanent effects. Carry-over effects from first period; washout needs to be long enough. Dropouts more significant, analysis may be more difficult: correlated outcomes.\nConstant intensity of underlying disease: chronic diseases (e.g. asthma, hypertension, arthritis) + short-term treatment effects (relief of signs or symptoms)\ne.g. morning dose vs evening dose\n\nGroup allocation: a group of subjects (community, school, clinic).\n\nExtensions of the parallel design: factorial, large simple\n\nFactorial: two interventions tested simultaneously. Can be presented in a 2 by 2 table (treatment A +-, treatment B +-); or 3 by 2 etc.\n\nInterested in main effect (if no interaction expected). A vs no A; B vs no B. The other treatment doesn’t matter.\n\nLarge simple: large number of patients, possibly from many study sites.\n\n\nTests other than superiority\n\nEquivalency: intervention response is close to control group response\nNon-inferiority: Treatment A (new) is at least as good as B (established). One-sided test, if A is worse than B, one can be rejected. Does not require as big sample size.\n\n\n\nAdaptive design\nPossible adaptations\n\nrandomization probabilities\nsample size (e.g. group sequential methods)\nvisit schedule: shorten/lengthen follow-up time, change number of timing of visits, treatments (dose/duration, concomitant meds)\nhypothesis tested"
  },
  {
    "objectID": "study_design/rct_design_overview.html#randomisation-and-masking",
    "href": "study_design/rct_design_overview.html#randomisation-and-masking",
    "title": "Clinical trial design: overview",
    "section": "Randomisation and masking",
    "text": "Randomisation and masking\nRationale:\n\navoid selection bias: prognostic factors related to treatment assignment\ntends to produce comparable treatment groups\n\n\nSchemes\nSimple randomization, restricted randomization, adaptive randomization\n\nSimple rz\nEach assignment is unpredictable, number of patients in each group should be equal in the long run.\nRisks: imbalances in number assigned to treatment groups, or confounding factors (gender, disease severity) -> reduced power\n\n\nRestricted rz\nSchemes with constraints to produce expected assignment ratio\n\nblocking\nstratification\n\nBlocking. Block of size 2 with treatment allocation ratio 1:1: A,B. Size 4: 2As, 2Bs. Need to be permuted: AABB, ABAB, … in total 6 combinations. Then choose one of the permutations.\nStratification. Ensure balance in treatment assignments with subgroups defined before rz. Limit to a few variables (highly related to outcome and/or logistical): e.g. clinic in a multicenter trial, surgeon (skills, procedures), stage of disease, demographic such as gender and age.\nUse these two together.\n\n\nAdaptive rz\nProbability of assignment does not remain constant, but determined by the current balance and composition of the groups.\n\nminimization: choose the design that gives the smallest imbalance.\nplay the winner: change allocation ratio or favor the better treatment based on the primary outcome. Need to evaluate outcomes relatively quickly.\n\n\n\n\nMasking (blinding)\nTreatment assignment is not known after rz.\n\npatient, clinical personnel, evaluators, data processors, …\nsingle (only participant), double (+ investigator), triple (+ data processors, …), quadruple …\n\nPurpose: remove bias related to treatment effects.\nDifferent levels of masking protects to different extent against bias in different aspects\n\ndata reporting\ndata collection / follow-up\ntesting, behaviors\noutcome assessment\n\nDecision to mask treatments\n\nethical?\npossible? can you make the treatment seem identical so the participants do not know?\ntrial design features: more important to mask subjective ones (e.g. alive or dead is the least subjective, hence wouldn’t benefit much; however if participants need to report effects that are not objectively measureable, they might report that treatment is better in contrast to placebo group)\nfeasible? cost-benefit, practicality (adherence)\n\nSometimes investigators in a double blind study might know which treatment is being assigned to participants, if the effect of drug is very obvious (both good or bad).\nUnmasking\n\nPlanned: inform participants once the trial finished\nUnplanned (discouraged): in the event of adverse event"
  },
  {
    "objectID": "study_design/rct_design_overview.html#outcomes-and-analysis",
    "href": "study_design/rct_design_overview.html#outcomes-and-analysis",
    "title": "Clinical trial design: overview",
    "section": "Outcomes and analysis",
    "text": "Outcomes and analysis\nOutcome: endpoint. It is a quantitaive measure.\nObjectives of the trial\n\nefficacy / effectiveness\nsafety\nprocess\ncosts\n\nExample: evaluate treatment for asthma\nOutcomes: exhaled nitrous oxide, lung function (spirometry measures), asthma symptoms (wheezing, night awakenings), …\nExample: evaluate a procedure to reduce perioperative morbidity\nOutcome considerations: time window (what is postoperative), specific events to be considered an outcome, procedures to establish outcomes, …\n\nMetrics for events as outcomes\n\ndichotomous: 1/0 for presence absense, normal abnormal; clinical state or cut-off value\ntime-to-event: in addition to dichotomous, add time dimension; allow for censoring. More powerful than dichotomous.\nrates: 1/0 but allow for repeats, analyze count or rate. Events within a person are usually not independent, need to account for it.\ncontinuous variables: value or change from baseline; standard units (lab values, scores). Need to define an important difference. Distributional assumptions more important.\nordinal scale: ranked categories (e.g. adverse event grading, 1-5). Difference between categories is usually qualitative.\n\nPatients opinions are subjective\n\nhealth status / change in status, e.g. pain relief, quality of life\nmasking is more important\nhawthorne / placebo effect: effect of being studies, usually positive\nquantify with standardized scales\n\n\n\nInfluence of outcomes on design\nEfficacy vs effectiveness:\nIn a vaccine trial, efficacy is the clinical case with lab confirmation; effectivenenss is the clinical case of influenza in a larger population, may or may not be confirmed.\nIn asthma, efficacy is FEV1, effectiveness is the decrease of the hospitalizations/steroid courses.\nConsiderations (3Bs)\n\nbiology: does outcome reflect a clinically relevant fact/change\nbiostatistics: detectable difference between groups is plausible and practical\nbudget: afford total N and can measure it reliably in every participant\n\nExample: HIV trial outcomes\n\nsurvival (deaths; AIDS status)\nimmunologic response\nvirologic response\nchange in patient status (e.g QoL)\nspecified toxicity\nother side effects\n\nChoice of primary outcome depends on the objectives or stage of research\n\nphase 1, emphasis on safety\nphase 2, short-term efficacy\nphase 3, long-term efficacy\nphase 4, long-term effectiveness\n\n\n\nIntention to treat ITT\nCross-overs after rz: some patients might have a treatment (yes or no) beyond what they were assigned to, e.g. refuse surgery or medical treatment\nNon-adherence during followup: some in treatment group refuse of can not tolerate certain treatment; while some in placebo group require medication or take on their own\n\n\nSubgroup analysis\nStratified analysis: estimate treatment effect separately in subgroups. Does not tell difference across different subgroups\nTest for interaction: use of main effect and interaction.\nIssue of multiple testing when doing a series of analyses\n\ninflate sample size to plan for subgroup analysis\nreport number of subgroup analyses performed\npossibly adjust for multiple comparisons\nreport CI instead of just p-values"
  },
  {
    "objectID": "study_design/rct_design_overview.html#reporting-results",
    "href": "study_design/rct_design_overview.html#reporting-results",
    "title": "Clinical trial design",
    "section": "Reporting results",
    "text": "Reporting results\n\nCONSORT\nConsolidated Standards of Reporting Trials\n25 item checklist. Not a guideline of how to do the trial; but a guideline for writers on how to report clearly.\nTitle\n\nkey design items\ntreatment(s) evaluated\ndisease or population studied\n\nAbstract\n\ndesign, method, results, conclusions\n\nIntroduction\n\nbackgrounds, rationale, establish equipoise (balance), systematic review, objectives / hypothesis\n\nMethod\n\nIRB review and approvals\ntrial design, allocation ratio\neligibility criteria\nsetting and location of the trial\nintervention - detailed enough to allow replication\noutcomes - primary, secondary, how assessed and defined\nsample size - how determined, interim analyses\nimportant changes during trial\nrandomization, allocation concealment, implementation\nmasking - who, how\nstatistical methods - primary and secondary; subgroup analyses\n\nConvenient to use a flow-chart\nBaseline characteristics by treatment group\np-values (common to not report in rct?)\n\n\nEvaluate literature\n\nLegitimacy, is it a fair comparison\nTrustworthy investigators? conflict of interest\nAdequate protections against bias\n\nrandomization\nmasking\nfollow-up design and execution\n\nITT analysis\n\nhave all events (outcomes) observed been counted in the treatment group assigned?\nvariations in denominators explained and consistent with good practice?\n\nappropriate subgroup analysis interpretation\n\nad hoc or post hoc status"
  },
  {
    "objectID": "reporting/reporting_overview.html",
    "href": "reporting/reporting_overview.html",
    "title": "Clinical trial: reporting results",
    "section": "",
    "text": "Coursera course Design and interpretation of clinical trials by Johns Hopkins University"
  },
  {
    "objectID": "reporting/reporting_overview.html#consort",
    "href": "reporting/reporting_overview.html#consort",
    "title": "Clinical trial: reporting results",
    "section": "CONSORT",
    "text": "CONSORT\nConsolidated Standards of Reporting Trials\n25 item checklist. Not a guideline of how to do the trial; but a guideline for writers on how to report clearly.\nTitle\n\nkey design items\ntreatment(s) evaluated\ndisease or population studied\n\nAbstract\n\ndesign, method, results, conclusions\n\nIntroduction\n\nbackgrounds, rationale, establish equipoise (balance), systematic review, objectives / hypothesis\n\nMethod\n\nIRB review and approvals\ntrial design, allocation ratio\neligibility criteria\nsetting and location of the trial\nintervention - detailed enough to allow replication\noutcomes - primary, secondary, how assessed and defined\nsample size - how determined, interim analyses\nimportant changes during trial\nrandomization, allocation concealment, implementation\nmasking - who, how\nstatistical methods - primary and secondary; subgroup analyses\n\nConvenient to use a flow-chart\nBaseline characteristics by treatment group\np-values (common to not report in rct?)\n\nEvaluate literature\n\nLegitimacy, is it a fair comparison\nTrustworthy investigators? conflict of interest\nAdequate protections against bias\n\nrandomization\nmasking\nfollow-up design and execution\n\nITT analysis\n\nhave all events (outcomes) observed been counted in the treatment group assigned?\nvariations in denominators explained and consistent with good practice?\n\nappropriate subgroup analysis interpretation\n\nad hoc or post hoc status"
  },
  {
    "objectID": "study_design/sample_size.html",
    "href": "study_design/sample_size.html",
    "title": "Sample size",
    "section": "",
    "text": "Sample size calculation is to determine the smallest number of subjects required, to detect a clinical meaningful effect. Why not recruiting as many as possible? Too expensive; or unethical (i.e. more people will be having potentially harmful or futile treatments)."
  },
  {
    "objectID": "dev/question_list.html",
    "href": "dev/question_list.html",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "In phase 2, difference in design (sample size) for single arm and randomizedf\n\n\nUnbiased assignment of treatment. Break the link between prognosis and prescription (allocation). Application of statistical methods based on random sampling\nE.g. avoid giving the same treatment to group A that have significantly worse health condition\n\n\n\n\n\n\nMight not observe an effect in RCT, but effect in OBS studies\n\npossible no effect in RCT: trial too short to see effect; late stage of disease among candidates to see any improvements of treatment; different treatment in OBS and RCT; comparing initiators vs non-initiators (hence shorter trial period)\npossible effect in OBS: residual confounding: social and environmental exposures; some already using treatment before enrolment\nsampling bias, in OBS the asymptomatic subjects might be missed (or under reported) while in RCT you can count every subject\nsurvival bias: survived long enough to be enrolled in OBS, and those dead are not; hence boosting effect to prolong survival\n\n\n\n\nNon-inferiority trials adopt one-sided test\nEquivalence: if the alternative is simpler, cheaper or have fewer side effects. Aim is to determine if difference is between +- delta\n\n\n\nWhy would these type of design reduce type 1 error rate by accounting for correlation?"
  },
  {
    "objectID": "dev/question_list.html#survival-analysis",
    "href": "dev/question_list.html#survival-analysis",
    "title": "Data Apothecary's Notes",
    "section": "Survival analysis",
    "text": "Survival analysis\nCan you analyse duration data with t-test? Why not?"
  },
  {
    "objectID": "study_design/sample_size.html#relevant-conncepts",
    "href": "study_design/sample_size.html#relevant-conncepts",
    "title": "Sample size",
    "section": "Relevant conncepts",
    "text": "Relevant conncepts\nStudy design\n\nparallel: group 1 TxA, group 2 TxB\ncrossover: requires fewer ssample than parallel; but requires wash-out period. Group 1 TxA -> TxB; group 2 TxB -> TxA\n\nTests\n\n\\(\\mu_T, \\mu_s\\): mean of new Tx or standard procedure\n\\(\\delta\\): minimum clinically important difference\n\\(\\delta_{NI}\\): non-inferiority margin\n\n\n\n\n\n\n\n\n\nTest for\nH0\nH1\n\n\n\n\nEquality\n\\(\\mu_T - \\mu_s = 0\\)\n\\(\\mu_T - \\mu_s \\neq 0\\)\n\n\nEquivalence\n\\(|\\mu_T - \\mu_s| \\geq 0\\)\n\\(|\\mu_T - \\mu_s| < 0\\)\n\n\nSuperiority\n\\(\\mu_T - \\mu_s \\geq 0\\)\n\\(\\mu_T - \\mu_s < 0\\)\n\n\nNon-inferiority\n\\(\\mu_T - \\mu_s \\leq -\\delta_{NI}\\)\n\\(\\mu_T - \\mu_s \\geq -\\delta_{NI}\\)\n\n\n\nErrors\n\nType I error, significance level \\(\\alpha\\). P(reject H0 |H0). Usually set to 0.05\nType II error \\(\\beta\\). P(not reject H0 |H1).\nPower, \\(1 - \\beta\\). P(reject H0 |H1). Usually set to 80% or 90%\n\nPrimary outcome\n\ncan be categorical or continuous\nMinimal meaningful detecable difference MD: the smallest difference to be considered as clinically meaningful in the primary outcome\n\nDropout rate: need to be adjusted.\nAllocation ratio: unequal sample size.\nEffect size (Cohen’s d, f) should be found in the literature. In general,\n\nvery small, d = 0.01\nsmall, d = 0.2\nmedium, d = 0.5\nlarge, d = 0.8\nvery large, d = 1.2\nhuge, d = 2"
  },
  {
    "objectID": "study_design/sample_size.html#proportions",
    "href": "study_design/sample_size.html#proportions",
    "title": "Sample size",
    "section": "Proportions",
    "text": "Proportions\nCohen’s h is used as the effect size, \\(h = 2arcsin(\\sqrt{p_1} - 2arcsin(\\sqrt{p_2}))\\). Use 0.2, 0.5, 0.8 for small, medium and large effect sizes.\n\n# one group\npwr::pwr.p.test(h = 0.5, sig.level = 0.05, power = 0.8, alternative = 'two.sided')\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.5\n              n = 31.39544\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n# two groups\npwr::pwr.2p.test(h = 0.5, sig.level = 0.05, power = 0.8, alternative = 'two.sided')\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.5\n              n = 62.79088\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes"
  },
  {
    "objectID": "study_design/sample_size.html#chi-square-test",
    "href": "study_design/sample_size.html#chi-square-test",
    "title": "Sample size",
    "section": "Chi-square test",
    "text": "Chi-square test\nCohen’s w. Use \\(l, k\\) to compute degrees of freedom.\n\n# k: number of groups; f: effect ssize\npwr::pwr.chisq.test(w = 0.3, df = (2-1)*(3-1), sig.level = 0.05, power = 0.8)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 107.0521\n             df = 2\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations"
  },
  {
    "objectID": "study_design/sample_size.html#exact-test",
    "href": "study_design/sample_size.html#exact-test",
    "title": "Sample size",
    "section": "Exact test",
    "text": "Exact test\nNeed to specify the proportion in each group (control, treatment). Allocation ratio 1:1\n\nexact2x2::ss2x2(p0 = 0.2, p1 = 0.8, n1.over.n0 = 1, sig.level = 0.05, power = 0.8, \n                approx = F, print.steps = T, paired = F)\n\n[1] \"starting calculation at n0= 11  n1= 11\"\n[1] \"n0=11 n1=11 power=0.734302912043505\"\n[1] \"n0=19 n1=19 power=0.962100966603327\"\n[1] \"n0=15 n1=15 power=0.872315260457242\"\n[1] \"n0=13 n1=13 power=0.868827534112504\"\n[1] \"n0=12 n1=12 power=0.811527612034704\"\n\n\n\n     Power for Fisher's Exact Test \n\n          power = 0.8115276\n             n0 = 12\n             n1 = 12\n             p0 = 0.2\n             p1 = 0.8\n      sig.level = 0.05\n    alternative = two.sided\n  nullOddsRatio = 1\n\nNOTE: errbound= 1e-06"
  },
  {
    "objectID": "study_design/sample_size_2.html",
    "href": "study_design/sample_size_2.html",
    "title": "Sample size (part II)",
    "section": "",
    "text": "(This is the part II on sample size calculation)\n\nCorrelation\nCorrelation coefficient is used for effect size measure. Usse 0.1, 0.3, 0.5 to represent small, medium and large sizes.\n\n# correlation coeff\npwr::pwr.r.test(r = 0.3, sig.level = 0.05, power = 0.8, alternative = 'two.sided')\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 84.07364\n              r = 0.3\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\nLinear regression (F-test)\nF-test for linear regression is testinng whether \\(R^2\\) is greater than zero (one-sided). \\(R^2\\) is the explained variance by using the predictors, \\(R^2 = 0.3\\) means that 30% of the variance are explained by the model.\nCohen’s f2, based on \\(R^2\\), goodness of fit (\\(f2 = R^2/(1-R^2)\\)). use 0.02, 0.15, 0.35 to represent small, medium and large effect sizes.\n\nu: number of predictors\nv: n-u-1\nas a result, sample size n = v+u+1\n\n\n# effect size f2 = 0.15; use u=3 predictors\npwr::pwr.f2.test(u = 3, f2 = 0.3, sig.level = 0.05, power = 0.8)\n\n\n     Multiple regression power calculation \n\n              u = 3\n              v = 36.47078\n             f2 = 0.3\n      sig.level = 0.05\n          power = 0.8\n\n\nHere v = 73, sample size is 73+3+1 = 77.\nAlternatively, can use pwrss::pwrss.f.reg(). The parameter is r2 rather than f2 (but can also use f2).\n\npwrss::pwrss.f.reg(r2 = 0.3, k = 0.3, power = 0.8, alpha = 0.05)\n\n Linear Regression (F test) \n R-squared Deviation from 0 (zero) \n H0: r2 = 0 \n HA: r2 &gt; 0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 14 \n ------------------------------ \n Numerator degrees of freedom = 0.3 \n Denominator degrees of freedom = 12.415 \n Non-centrality parameter = 5.878 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n\n# should be equivalent to\n# pwr::pwr.f2.test(u = 3, f2 = 0.3/0.7, sig.level = 0.05, power = 0.8)\n\n\n\nGLM\n\nResources\n\nSample size calculation in clinical trial using R.\nPark et al. 2023. https://doi.org/10.7602/jmis.2023.26.1.9\nBulus, M (2023) pwrss: Statistical Power and Sample Size Calculation Tools. R package version 0.3.1. https://CRAN.R-project.org/package=pwrss. Vignette documentation"
  },
  {
    "objectID": "study_design/sample_size_1.html",
    "href": "study_design/sample_size_1.html",
    "title": "Sample size (part I)",
    "section": "",
    "text": "(This is the part I on sample size calculation)\nSample size calculation is to determine the smallest number of subjects required, to detect a clinical meaningful effect. Why not recruiting as many as possible? Too expensive; or unethical (i.e. more people will be having potentially harmful or futile treatments)."
  },
  {
    "objectID": "study_design/sample_size_1.html#relevant-concepts",
    "href": "study_design/sample_size_1.html#relevant-concepts",
    "title": "Sample size (part I)",
    "section": "Relevant concepts",
    "text": "Relevant concepts\nStudy design\n\nparallel: group 1 TxA, group 2 TxB\ncrossover: requires fewer ssample than parallel; but requires wash-out period. Group 1 TxA -&gt; TxB; group 2 TxB -&gt; TxA\n\nTests\n\n\\(\\mu_T, \\mu_s\\): mean of new Tx or standard procedure\n\\(\\delta\\): minimum clinically important difference\n\\(\\delta_{NI}\\): non-inferiority margin\n\n\n\n\n\n\n\n\n\nTest for\nH0\nH1\n\n\n\n\nEquality\n\\(\\mu_T - \\mu_s = 0\\)\n\\(\\mu_T - \\mu_s \\neq 0\\)\n\n\nEquivalence\n\\(|\\mu_T - \\mu_s| \\geq 0\\)\n\\(|\\mu_T - \\mu_s| &lt; 0\\)\n\n\nSuperiority\n\\(\\mu_T - \\mu_s \\geq 0\\)\n\\(\\mu_T - \\mu_s &lt; 0\\)\n\n\nNon-inferiority\n\\(\\mu_T - \\mu_s \\leq -\\delta_{NI}\\)\n\\(\\mu_T - \\mu_s \\geq -\\delta_{NI}\\)\n\n\n\nErrors\n\nType I error, significance level \\(\\alpha\\). P(reject H0 |H0). Usually set to 0.05\nType II error \\(\\beta\\). P(not reject H0 |H1).\nPower, \\(1 - \\beta\\). P(reject H0 |H1). Usually set to 80% or 90%\n\nPrimary outcome\n\ncan be categorical or continuous\nMinimal meaningful detecable difference MD: the smallest difference to be considered as clinically meaningful in the primary outcome\n\nDropout rate: need to be adjusted.\nAllocation ratio: unequal sample size.\nEffect size (Cohen’s d, f) should be found in the literature. In general,\n\nvery small, d = 0.01\nsmall, d = 0.2\nmedium, d = 0.5\nlarge, d = 0.8\nvery large, d = 1.2\nhuge, d = 2"
  },
  {
    "objectID": "study_design/sample_size_1.html#two-sample-t-test",
    "href": "study_design/sample_size_1.html#two-sample-t-test",
    "title": "Sample size (part I)",
    "section": "Two sample t-test",
    "text": "Two sample t-test\nNote that this result is for one group: in total it’s times two.\n\n# effect size: 0.5\npwr::pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8, type = 'two.sample', alternative = 'two.sided')\n\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group"
  },
  {
    "objectID": "study_design/sample_size_1.html#anova",
    "href": "study_design/sample_size_1.html#anova",
    "title": "Sample size (part I)",
    "section": "ANOVA",
    "text": "ANOVA\nResult is for each group.\n\n# k: number of groups; f: effect ssize\npwr::pwr.anova.test(k = 3, f = 0.25, sig.level = 0.05, power = 0.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.3966\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group"
  },
  {
    "objectID": "study_design/sample_size_1.html#proportions",
    "href": "study_design/sample_size_1.html#proportions",
    "title": "Sample size (part I)",
    "section": "Proportions",
    "text": "Proportions\nCohen’s h is used as the effect size, \\(h = 2arcsin(\\sqrt{p_1} - 2arcsin(\\sqrt{p_2}))\\). Use 0.2, 0.5, 0.8 for small, medium and large effect sizes.\n\n# one group\npwr::pwr.p.test(h = 0.5, sig.level = 0.05, power = 0.8, alternative = 'two.sided')\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.5\n              n = 31.39544\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n# two groups\npwr::pwr.2p.test(h = 0.5, sig.level = 0.05, power = 0.8, alternative = 'two.sided')\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.5\n              n = 62.79088\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes"
  },
  {
    "objectID": "study_design/sample_size_1.html#chi-square-test",
    "href": "study_design/sample_size_1.html#chi-square-test",
    "title": "Sample size (part I)",
    "section": "Chi-square test",
    "text": "Chi-square test\nCohen’s w. Use \\(l, k\\) to compute degrees of freedom.\n\n# k: number of groups; f: effect ssize\npwr::pwr.chisq.test(w = 0.3, df = (2-1)*(3-1), sig.level = 0.05, power = 0.8)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 107.0521\n             df = 2\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations"
  },
  {
    "objectID": "study_design/sample_size_1.html#exact-test",
    "href": "study_design/sample_size_1.html#exact-test",
    "title": "Sample size (part I)",
    "section": "Exact test",
    "text": "Exact test\nNeed to specify the proportion in each group (control, treatment). Allocation ratio 1:1\n\nexact2x2::ss2x2(p0 = 0.2, p1 = 0.8, n1.over.n0 = 1, sig.level = 0.05, power = 0.8, \n                approx = F, print.steps = T, paired = F)\n\n[1] \"starting calculation at n0= 11  n1= 11\"\n[1] \"n0=11 n1=11 power=0.734302912043505\"\n[1] \"n0=19 n1=19 power=0.962100966603327\"\n[1] \"n0=15 n1=15 power=0.872315260457242\"\n[1] \"n0=13 n1=13 power=0.868827534112504\"\n[1] \"n0=12 n1=12 power=0.811527612034704\"\n\n\n\n     Power for Fisher's Exact Test \n\n          power = 0.8115276\n             n0 = 12\n             n1 = 12\n             p0 = 0.2\n             p1 = 0.8\n      sig.level = 0.05\n    alternative = two.sided\n  nullOddsRatio = 1\n\nNOTE: errbound= 1e-06"
  },
  {
    "objectID": "programming/web_basics.html",
    "href": "programming/web_basics.html",
    "title": "Web basics",
    "section": "",
    "text": "Resources\nhttps://jakobtures.github.io/web-scraping/rvest1.html"
  },
  {
    "objectID": "programming/web_basics.html#attributes",
    "href": "programming/web_basics.html#attributes",
    "title": "Web basics",
    "section": "Attributes",
    "text": "Attributes\nBasic syntax: &lt;tag attribute=\"value\"&gt;...&lt;/tag&gt;. No space between equal sign and value.\n\nWeb link\n&lt;a href=\"https://jakobtures.github.io/web-scraping/html.html\"&gt;This is a link&lt;/a&gt;\nThe code above creates an active link. The code below also points to this link, but opens in a new page using target=\"_blank\".\n&lt;a href=\"https://jakobtures.github.io/web-scraping/html.html\" target=\"_blank\"&gt;This is a link&lt;/a&gt;\n\n\nImages\nTwo images, one with adjusted size\n&lt;img src=\"https://jakobtures.github.io/web-scraping/Rlogo.png\"&gt;\n&lt;img src=\"https://jakobtures.github.io/web-scraping/Rlogo.png\" width=\"100\" height=\"100\"&gt;\nCan combine image with link, by puttinng the links within the anchor.\n&lt;a href=\"https://www.r-project.org/\" target=\"_blank\"&gt;&lt;img src=\"https://jakobtures.github.io/web-scraping/Rlogo.png\"&gt;&lt;/a&gt;"
  },
  {
    "objectID": "programming/web_basics.html#entities",
    "href": "programming/web_basics.html#entities",
    "title": "Web basics",
    "section": "Entities",
    "text": "Entities\nCoded representations of certain characters, &..;. For example,\n\n&lt; less than, &lt;\n&quot; ”\n&amp; &: ampersand\n&nbsp; non-breaking space"
  },
  {
    "objectID": "programming/web_basics.html#html-tags",
    "href": "programming/web_basics.html#html-tags",
    "title": "Web basics",
    "section": "HTML tags",
    "text": "HTML tags\n&lt;!DOCTYPE html&gt;\n\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Hello World!&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;b&gt;Hello World!&lt;/b&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nLine 1: declare which version of HTML. For now it is 5\nThe rest are different tags.\nImportant tags:\n\nheader, &lt;h1&gt;, &lt;h2&gt;, ..., &lt;h6&gt;\ndivision or span, &lt;div&gt;, &lt;span&gt;\nparagraph, &lt;p&gt;\nline break, &lt;br&gt;. This does not need to be closed with &lt;/br&gt;\nbold, italics &lt;b&gt;, &lt;i&gt;\nlists - ordered &lt;ol&gt;, unordered &lt;ul&gt;. Within the tags, use &lt;li&gt;\ntables. Lines are defined by &lt;tr&gt; (table row), table header &lt;th&gt; and &lt;td&gt;, table data.\nanchor, &lt;a&gt;, useful for url links: but it is different from &lt;link&gt; tag (which links to files such as JS or CSS)."
  },
  {
    "objectID": "study_design/rct_design_overview.html#intention-to-treat-itt",
    "href": "study_design/rct_design_overview.html#intention-to-treat-itt",
    "title": "Clinical trial design: overview",
    "section": "Intention to treat ITT",
    "text": "Intention to treat ITT\nCross-overs after rz: some patients might have a treatment (yes or no) beyond what they were assigned to, e.g. refuse surgery or medical treatment\nNon-adherence during followup: some in treatment group refuse of can not tolerate certain treatment; while some in placebo group require medication or take on their own"
  },
  {
    "objectID": "study_design/rct_design_overview.html#subgroup-analysis",
    "href": "study_design/rct_design_overview.html#subgroup-analysis",
    "title": "Clinical trial design: overview",
    "section": "Subgroup analysis",
    "text": "Subgroup analysis\nStratified analysis: estimate treatment effect separately in subgroups. Does not tell difference across different subgroups\nTest for interaction: use of main effect and interaction.\nIssue of multiple testing when doing a series of analyses\n\ninflate sample size to plan for subgroup analysis\nreport number of subgroup analyses performed\npossibly adjust for multiple comparisons\nreport CI instead of just p-values\n\n\nResources\n\nCoursera course Design and interpretation of clinical trials by Johns Hopkins University\nBook Fast Facts: Clinical trialss inn oncology: The fundamentals"
  },
  {
    "objectID": "study_design/adaptive_design.html",
    "href": "study_design/adaptive_design.html",
    "title": "Adaptive design: overview",
    "section": "",
    "text": "Resources\n\nR package rpact and tutorial\nR package gsDesign and tutorial\n\n\nSequential design\nNumber of patient isn’t set in advance. A good group sequential design can reduce the sample size needed, while keeping the desired statistical power and controlling the overall type I error.\nGSD includes pre-determined number of stages (interim, final). Each stage specified by\n\nsample size\ncritical values\nstopping criterion to support or reject null hypothesis\n\nTBC"
  },
  {
    "objectID": "inference/overview.html",
    "href": "inference/overview.html",
    "title": "Overview: causal inference",
    "section": "",
    "text": "\\(Y^{a=1}, Y^{a=0}\\) are the potential outcomes under treatment 1 and 0. They are random variables. Treatment A has causal effect if \\(Y^{a=1} \\neq Y^{a=0}\\).\nFor individual \\(i\\), \\(Y_i^{a=1}, Y_i^{a=0}\\) are deterministic.\nIn reality we do not observe both potential outcomes for an individual, since we only have ONE outcome. We observe \\(Y\\) and \\(A\\). For a population, average treatment effect (ATE) can be estimate"
  },
  {
    "objectID": "inference/estimation_gcomp_ipw.html",
    "href": "inference/estimation_gcomp_ipw.html",
    "title": "Estimation",
    "section": "",
    "text": "Introduction\nSmoking example\n\ndoes smoking cause lung cancer?\ndoes lung cancer cause people to smoke?\nis there a third factor that causes both smoking and lung cancer?\n\nPotential outcome framework\n\\(Y_i = Y_i(z)\\) if \\(Z_i = z\\), under treatment Z = z\nEstimand: a precise description of the treatment effect"
  },
  {
    "objectID": "models/survival.html",
    "href": "models/survival.html",
    "title": "Survival",
    "section": "",
    "text": "Links\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html\nhttps://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/\nhttps://www.danieldsjoberg.com/ggsurvfit/\nhttps://www.coursera.org/learn/survival-analysis-r-public-health\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html\nhttps://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/\njmpost: combines survival analysis, mixed effect model https://genentech.github.io/jmpost/main/"
  },
  {
    "objectID": "models/mixed_models.html",
    "href": "models/mixed_models.html",
    "title": "Mixed models for repeted measurements",
    "section": "",
    "text": "Resources:\n\nmmrm package vignette\nMixed models with R\n\nMMRM has one distinct feature compared to other linear mixed models: subject-specific random effects are considered as residual effects (part of error correlation matrix).\n\nMethodology\nBasic linear mixed-effects model for a single level of grouping\n\\[\ny_i = X_i \\beta + Z_i b_i + \\epsilon_i, i = 1, ..., n\n\\] \\[\nb_i \\sim N(0, \\Psi), \\epsilon \\sim N(0, \\sigma^2 I)\n\\]\n\n\\(\\beta\\) is p-dim vector of fixed effects\n\\(b\\) is q-dim vecor of random patient specific effects\n\\(X_i\\) of size \\(n_i \\times p\\) and \\(Z_i\\) of size \\(n_i \\times q\\) are regressor matrices relating observations to the fixed effects and random effects.\n\\(\\epsilon_i\\) is \\(n_i\\)-dimensional within-subject error"
  },
  {
    "objectID": "inference/rwd_rwe.html",
    "href": "inference/rwd_rwe.html",
    "title": "Real-world Data, Real-world Evidence",
    "section": "",
    "text": "RWD is any data collected outside clinical trial setting, can be combined with clinical trials.\n(The general benefits and disadvantages of RWD is coherent with EHR data)\nTherapheutic areas: oncology, rare diseases, infectious diseases among others\nTraditionally regarded as inferior evidence: lack of randomization, limited information on potential relevant prognostic factors.\nFDA: (2016) 21st Century Cures Act, evaluate the use of RWD in support of regulatory approvals and post-approval safety studies.\nEMA (2017): HMA/EMA Joint Big Data Task Force, establish a roadmap for the use of RWD in regulatory assessments\nChallenges of using RCT\nFocus: whether RWD can be trusted to reliably measure treatment effects of new drugs, causal relationship. The main difference between RCT and RWD is the confounding bias."
  },
  {
    "objectID": "inference/covariate_adjustment.html",
    "href": "inference/covariate_adjustment.html",
    "title": "Covariate adjustment",
    "section": "",
    "text": "https://jbetz-jhu.github.io/CovariateAdjustmentTutorial/\nBaseline covariates are variables measured prior to randomization, expected to have strong association with outcome. Potential confounding occurs when the distribution of baseline covariates between treatment groups are imbalanced.\nTo address confounding:\n\ndesign stage: stratified randomization to reduce imbalance\nanalysis stage: covariate adjustment\n\nBenefits:\n\nreduce sample size\nimprove precision - smaller CI, higher power\nsome CA do not depend on a correctly specified model\n\nRecent FDA guidance requires distinction between conditional and marginal treatment effects. These two coincide in linear models, but not in non-linear models (e.g. binary, ordinal, count, time-to-event outcomes)."
  },
  {
    "objectID": "programming/webr.html",
    "href": "programming/webr.html",
    "title": "WebR: Use with an existing quarto website",
    "section": "",
    "text": "When you create a new quarto website, inside _quarto.yml the output-dir isn’t specified. In this case, when you follow the tutorial by James J Balamuta (creator of the extension quarto-webr) you will be able to render a functional webR page. Yet, if you already have an existing quarto website deployed by Github actions (with output-dir: docs), the default solution might not work.\nFortunately, the fix is simple enough. I think this feature will be added to the future versions of quarto-webr extension, because this extension is just great."
  },
  {
    "objectID": "programming/shiny.html",
    "href": "programming/shiny.html",
    "title": "Shiny",
    "section": "",
    "text": "Number of bins:"
  },
  {
    "objectID": "programming/webr.html#installation",
    "href": "programming/webr.html#installation",
    "title": "WebR: Use with an existing quarto website",
    "section": "Installation",
    "text": "Installation\nOpen terminal, install the extension in the root of the current quarto project. This is important, as quarto extensions are project-based, i.e. need to be included in each quarto project.\nquarto add coatless/quarto-webr"
  },
  {
    "objectID": "programming/webr.html#configuration",
    "href": "programming/webr.html#configuration",
    "title": "WebR: Use with an existing quarto website",
    "section": "Configuration",
    "text": "Configuration\nAdd the following lines in the yaml header in the quarto file you want to run webR. For example,\ntitle: \"WebR demo\"\nengine: knitr\nformat: html\nfilters: \n  - webr\nwebr: \n  channel-type: \"post-message\"\nImportant bits:\n\nspecify engine to knitr\nspecify filters to - webr. This could alternatively be specified in the overall _quarto.yml file to apply to every qt document.\nadd channel-type: \"post-message\" under webr. No dash in front."
  },
  {
    "objectID": "programming/webr.html#configuration-1",
    "href": "programming/webr.html#configuration-1",
    "title": "WebR",
    "section": "Configuration",
    "text": "Configuration\nNow use the curly bracket {webr-r} for your code chunk (which used to be just {r}),\nLoading\n  webR...\n\n\n  \n\n\nA histogram that changes every time you click RUN CODE\nLoading\n  webR..."
  },
  {
    "objectID": "programming/webr.html#execution",
    "href": "programming/webr.html#execution",
    "title": "WebR: Use with an existing quarto website",
    "section": "Execution",
    "text": "Execution\nNow use the curly bracket {webr-r} for your code chunk (which used to be just {r}),\nLoading\n  webR...\n\n\n  \n\n\nA histogram that changes every time you click RUN CODE. This proves that we are running interactively the R code inside the web browser.\nLoading\n  webR..."
  },
  {
    "objectID": "programming/r_pkg_wf.html",
    "href": "programming/r_pkg_wf.html",
    "title": "R package Engineering Workflow",
    "section": "",
    "text": "Useful references:\nSteps suggested:"
  },
  {
    "objectID": "programming/r_pkg_wf.html#step-2-design-docs",
    "href": "programming/r_pkg_wf.html#step-2-design-docs",
    "title": "R package Engineering Workflow",
    "section": "Step 2: Design docs",
    "text": "Step 2: Design docs\nPurpose and scope: the package pkg_name shall …\nPackage requirements:\n\nlibrary(gt)\ndf &lt;- data.frame(obligation = c('Duty', 'Desire', 'Intension'), \n                 keyword = c('shall', 'should', 'will'), \n                 description = c('must have', 'nice to have', 'optional'))\n\ngt(df) |&gt; \n  cols_label(obligation = md('**Obligation**'), \n             keyword = md('**Key word**'), \n             description = md('**Description**'))\n\n\n\n\n\n  \n    \n    \n      Obligation\n      Key word\n      Description\n    \n  \n  \n    Duty\nshall\nmust have\n    Desire\nshould\nnice to have\n    Intension\nwill\noptional\n  \n  \n  \n\n\n\n\nUse some documentation tools (md, qmd, or diagram draw.io)"
  },
  {
    "objectID": "programming/r_pkg_wf.html#step-3-packaging",
    "href": "programming/r_pkg_wf.html#step-3-packaging",
    "title": "R package Engineering Workflow",
    "section": "Step 3: packaging",
    "text": "Step 3: packaging\n\ncreate basic project\nCopy and paste exisint R scripts, refactor if necessary (i.e. give self-explanatory names)\nCreate R generic functions (print, summary)\nDocument\n\n\n\n\n\n\n\nTo do\n\n\n\n\nreturn result as a list, with class attribute\nreturn argument"
  },
  {
    "objectID": "programming/r_pkg_wf.html#step-4-qualiy-code",
    "href": "programming/r_pkg_wf.html#step-4-qualiy-code",
    "title": "R package Engineering Workflow",
    "section": "Step 4: Qualiy code",
    "text": "Step 4: Qualiy code\nIt is important to have clean code, so that it is easier to read and maintain; easier to exntend; and the code runs faster.\n\nnaming. Make the function and argument names easy to understand\nformatting. Indentation, spacing, bracketing should be consistent\nsimplicity. Avoid unnecesary complexity. Split large source files into smaller chunks, preferably less than 1000 lines.\nsingle responsibility principle (SRP). Each function should have ONE single purpose.\ndon’t repeat yourself. Make a function!\ncomment.\nerror handling. Include error handling messages. tryCatch()\n\n\n\n\n\n\n\nTo do\n\n\n\n\nError handling.\n\n\n\n\nTesting\nWhy do you need unit tests? In short, increase reliability and maintainability of the code.\nOther types of tests exist: integration testing, performance testing, snapshot testing. Package testthat allows not only unit tests, but also the other types of tests.\n\n\n\n\n\n\nNote\n\n\n\nA more comprehensive guide see my other note: R package: tests\n\n\nTest coverage: use covr. Ideally should cover 100%.\n\n\nPackage quality check\nR CMD Check\n\n\nCode style\nUse tidyverse style guide.\n\nstyler: restyle text, files or entire project.\nlintr: perform automated checks to confirm that our code conform to the style guide.\ndevtools::spell_check."
  },
  {
    "objectID": "programming/r_pkg_tests.html",
    "href": "programming/r_pkg_tests.html",
    "title": "R package: tests",
    "section": "",
    "text": "Useful references:\nUnit test: tests whether your function returns values as expected.\nBenefits:\nExample situations:"
  },
  {
    "objectID": "programming/r_pkg_wf.html#step-5.-publication",
    "href": "programming/r_pkg_wf.html#step-5.-publication",
    "title": "R package Engineering Workflow",
    "section": "Step 5. Publication",
    "text": "Step 5. Publication\npkgdown website might be the most useful place.\nVersioning\n\nx.y.z\nx is major, breaking changes\ny is minor, new features\nz is patch, bug fixes\ntry usethis::use_version()"
  },
  {
    "objectID": "programming/revealjs.html",
    "href": "programming/revealjs.html",
    "title": "Make slides with reveal.js",
    "section": "",
    "text": "iframe stands for inline frame. It is an HTML element that loads another HTML page within the document."
  },
  {
    "objectID": "programming/test_slides.html#hello-there",
    "href": "programming/test_slides.html#hello-there",
    "title": "Quarto Presentations",
    "section": "",
    "text": "This presentation will show you examples of what you can do with Quarto and Reveal.js, including:\n\nPresenting code and LaTeX equations\nIncluding computations in slide output\nImage, video, and iframe backgrounds\nFancy transitions and animations\nPrinting to PDF"
  },
  {
    "objectID": "programming/revealjs.html#embed-in-your-own-web-page",
    "href": "programming/revealjs.html#embed-in-your-own-web-page",
    "title": "Make slides with reveal.js",
    "section": "",
    "text": "iframe stands for inline frame. It is an HTML element that loads another HTML page within the document."
  },
  {
    "objectID": "programming/index.html#r-programming",
    "href": "programming/index.html#r-programming",
    "title": "Programming",
    "section": "",
    "text": "R related topics from basic to advanced.\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nOOP in R: S3\n\n\n\n\nError handling in R\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "programming/index.html#r-package",
    "href": "programming/index.html#r-package",
    "title": "Programming",
    "section": "R package",
    "text": "R package\nR package development\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nR package: tests\n\n\n\n\nR package Engineering Workflow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "programming/index.html#web",
    "href": "programming/index.html#web",
    "title": "Programming",
    "section": "Web",
    "text": "Web\nWeb and quarto topics\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nWebR: Use with an existing quarto website\n\n\n\n\nWeb basics\n\n\n\n\nMake slides with reveal.js\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "programming/r_pkg_0_wf.html",
    "href": "programming/r_pkg_0_wf.html",
    "title": "R package Engineering Workflow",
    "section": "",
    "text": "Useful references:\nSteps suggested:"
  },
  {
    "objectID": "programming/r_pkg_0_wf.html#step-2-design-docs",
    "href": "programming/r_pkg_0_wf.html#step-2-design-docs",
    "title": "R package Engineering Workflow",
    "section": "Step 2: Design docs",
    "text": "Step 2: Design docs\nPurpose and scope: the package pkg_name shall …\nPackage requirements:\n\nlibrary(gt)\ndf &lt;- data.frame(obligation = c('Duty', 'Desire', 'Intension'), \n                 keyword = c('shall', 'should', 'will'), \n                 description = c('must have', 'nice to have', 'optional'))\n\ngt(df) |&gt; \n  cols_label(obligation = md('**Obligation**'), \n             keyword = md('**Key word**'), \n             description = md('**Description**'))\n\n\n\n\n\n\n\n\nObligation\nKey word\nDescription\n\n\n\n\nDuty\nshall\nmust have\n\n\nDesire\nshould\nnice to have\n\n\nIntension\nwill\noptional\n\n\n\n\n\n\n\n\nUse some documentation tools (md, qmd, or diagram draw.io)"
  },
  {
    "objectID": "programming/r_pkg_0_wf.html#step-3-packaging",
    "href": "programming/r_pkg_0_wf.html#step-3-packaging",
    "title": "R package Engineering Workflow",
    "section": "Step 3: packaging",
    "text": "Step 3: packaging\n\ncreate basic project\nCopy and paste exisint R scripts, refactor if necessary (i.e. give self-explanatory names)\nCreate R generic functions (print, summary)\nDocument\n\n\n\n\n\n\n\nTo do\n\n\n\n\nreturn result as a list, with class attribute\nreturn argument"
  },
  {
    "objectID": "programming/r_pkg_0_wf.html#step-4-qualiy-code",
    "href": "programming/r_pkg_0_wf.html#step-4-qualiy-code",
    "title": "R package Engineering Workflow",
    "section": "Step 4: Qualiy code",
    "text": "Step 4: Qualiy code\nIt is important to have clean code, so that it is easier to read and maintain; easier to exntend; and the code runs faster.\n\nnaming. Make the function and argument names easy to understand\nformatting. Indentation, spacing, bracketing should be consistent\nsimplicity. Avoid unnecesary complexity. Split large source files into smaller chunks, preferably less than 1000 lines.\nsingle responsibility principle (SRP). Each function should have ONE single purpose.\ndon’t repeat yourself. Make a function!\ncomment.\nerror handling. Include error handling messages. tryCatch()\n\n\n\n\n\n\n\nTo do\n\n\n\n\nError handling.\n\n\n\n\nTesting\nWhy do you need unit tests? In short, increase reliability and maintainability of the code.\nOther types of tests exist: integration testing, performance testing, snapshot testing. Package testthat allows not only unit tests, but also the other types of tests.\n\n\n\n\n\n\nNote\n\n\n\nA more comprehensive guide see my other note: R package: tests\n\n\nTest coverage: use covr. Ideally should cover 100%.\n\n\nPackage quality check\nR CMD Check\n\n\nCode style\nUse tidyverse style guide.\n\nstyler: restyle text, files or entire project.\nlintr: perform automated checks to confirm that our code conform to the style guide.\ndevtools::spell_check."
  },
  {
    "objectID": "programming/r_pkg_0_wf.html#step-5.-publication",
    "href": "programming/r_pkg_0_wf.html#step-5.-publication",
    "title": "R package Engineering Workflow",
    "section": "Step 5. Publication",
    "text": "Step 5. Publication\npkgdown website might be the most useful place.\nVersioning\n\nx.y.z\nx is major, breaking changes\ny is minor, new features\nz is patch, bug fixes\ntry usethis::use_version()\n\nAdding badges\n\n# lifecycle\nusethis::use_lifecycle_badge(stage = 'experimental')\n# R-CMD-check \nusethis::use_github_action_check_standard()"
  },
  {
    "objectID": "programming/intv_rdev_topics.html",
    "href": "programming/intv_rdev_topics.html",
    "title": "Topic coverage: R developer",
    "section": "",
    "text": "This topic list does NOT include\n\n\n\n\nshiny developer\nstatistical programmer (would require more on specific statistical packages)\ndata scientist with a focus on engineering\n\n\n\nResources:\n\nAdvanced R\nR packages (2e)\n\nTypical thought process:\n\nWhat is x\nWhen do you use x\nWhat are the pros and cons of x\n\n\n\n\n\n\n\nScattered knowledge dumpster\n\n\n\n\nvector: diff na.rm = T and na.omit(x)\nenvironment: function in global env. what does it mean (scope)\nerror message: difference between stop() and rlang::abort()\n\ndata manipulation\n\ndata table: why do I prefer to use it compared to tibble and df\n\n\n\n\nR Programming\nFoundations\n\nData structures in R\nEnvironments\n\nFunctions and functional programming\n\napply() functions\nEvaluation\nArgument\n\nOOP\n\nS3 generics\n\nDebugging\n\nSomething related to debugging\n\n\n\nData manipulation\n\nTidyverse suite\nText processing\n\n\n\nR package\n\nDevelopment workflow\nR code and functions\nData\nDESCRIPTION\nDependencies and enviroment\nTesting\nDocumentation: code\nDocumentation: vignettes\nDoucmentation: website\nMaintenance"
  },
  {
    "objectID": "programming/r_pkg_tests.html#use-testthat-in-a-package",
    "href": "programming/r_pkg_tests.html#use-testthat-in-a-package",
    "title": "R package: tests",
    "section": "Use testthat in a package",
    "text": "Use testthat in a package\n\n1. Initialize\nCreate tests/testthat/ directory\n\nusethis::use_testthat(3)\n\nThis creates the directory with\n\nan empty folder testthat where you write your tests\nan R script testthat.R where tests are run when R CMD check is run. Do not modify this file.\n\n\n\n2. Create a test\nTest files must have names that start with test. For example, a function is R/fn_name.R, then test is tests/testthat/test-fn_name.R.\n\nusethis::use_test('testname')\n\n\n\n3. Run a test\n\ntestthat::test_file('tests/testthat/test-foofy.R')\nRun Tests button\ndevtools::test() for entire test suite. Cmd + Shift + T\ndevtools::check()\n\n\n\n\n\n\n\nA workflow worked for me\n\n\n\n\nCreate a simple function\nCreate a test file immediately, with clear naming. Inside this test file, can write various tests for the same function.\n\n\nhave at least a test that expects the correct result (expect_identical() or else)\nhave at least a test that expects error, (expect_error()). Inside the original function, error should be thrown by rlang::abort().\n\n\nRun test\nRun test coverage, covr::package_coverage() or covr::code_coverage()\nCheck"
  },
  {
    "objectID": "programming/r_functions.html",
    "href": "programming/r_functions.html",
    "title": "Functions",
    "section": "",
    "text": "Special argument .... This type of argument is varargs (variable arguments). The function can take any numbers of arguments.\nPrimary uses:\n\nyour function takes a function as an argument, and we want to pass additional arguments\nS3 generic, need to allow methods to take extra arguments\n\nDownsides:\n\nneed to explain where the arguments go to\nmisspelled argument will not raise an error\n\n\n\nThe ... can be something like na.rm = F.\nGiven that the first argument is being averaged upon, if a vector is not specified correctly, only the first element is being averaged; and the other elements are treated as additional arguments that are not necessarily used.\n\n# mean(c(1,2,3)) \nmean(1, 2, 3)\n\n[1] 1\n\nmean(c(1, 2), 3)\n\n[1] 1.5\n\n\n\nfplus &lt;- function(a, ...){\n  list(sum(a), ...)\n}\nfplus(a = c(1,2,3))\n\n[[1]]\n[1] 6\n\nfplus(a = c(1,2,3), 4)\n\n[[1]]\n[1] 6\n\n[[2]]\n[1] 4\n\n\n\nf1 &lt;- function(a, ...){\n  args &lt;- list(...)\n  if('y' %in% names(args)){\n    args$y &lt;- 0.5 + args$y\n    do.call(f2, args) # second arg need to be a list\n  }else{\n    a+1\n  }\n\n}\nf2 &lt;- function(y){return(y)}"
  },
  {
    "objectID": "programming/r_functions.html#dot-dot-dot",
    "href": "programming/r_functions.html#dot-dot-dot",
    "title": "Functions",
    "section": "",
    "text": "Special argument .... This type of argument is varargs (variable arguments). The function can take any numbers of arguments.\nPrimary uses:\n\nyour function takes a function as an argument, and we want to pass additional arguments\nS3 generic, need to allow methods to take extra arguments\n\nDownsides:\n\nneed to explain where the arguments go to\nmisspelled argument will not raise an error\n\n\n\nThe ... can be something like na.rm = F.\nGiven that the first argument is being averaged upon, if a vector is not specified correctly, only the first element is being averaged; and the other elements are treated as additional arguments that are not necessarily used.\n\n# mean(c(1,2,3)) \nmean(1, 2, 3)\n\n[1] 1\n\nmean(c(1, 2), 3)\n\n[1] 1.5\n\n\n\nfplus &lt;- function(a, ...){\n  list(sum(a), ...)\n}\nfplus(a = c(1,2,3))\n\n[[1]]\n[1] 6\n\nfplus(a = c(1,2,3), 4)\n\n[[1]]\n[1] 6\n\n[[2]]\n[1] 4\n\n\n\nf1 &lt;- function(a, ...){\n  args &lt;- list(...)\n  if('y' %in% names(args)){\n    args$y &lt;- 0.5 + args$y\n    do.call(f2, args) # second arg need to be a list\n  }else{\n    a+1\n  }\n\n}\nf2 &lt;- function(y){return(y)}"
  },
  {
    "objectID": "programming/r_pkg_functions.html",
    "href": "programming/r_pkg_functions.html",
    "title": "My workflow in working with functions in a package",
    "section": "",
    "text": "Ongoing notes\n\n\n\nSome of the content are being added as I go.\nFunctions make up the whole R package, except for data-only packages. The workflow should help me navigate the process.\nA short example borrowed from GSWEP4R workshop is documented here, function code and test code"
  },
  {
    "objectID": "programming/r_pkg_functions.html#structure",
    "href": "programming/r_pkg_functions.html#structure",
    "title": "My workflow in working with functions in a package",
    "section": "Structure",
    "text": "Structure"
  },
  {
    "objectID": "programming/r_pkg_functions.html#write",
    "href": "programming/r_pkg_functions.html#write",
    "title": "My workflow in working with functions in a package",
    "section": "Write",
    "text": "Write\n\nWhat to return?\nPut the arguments in a list named result, then attach other outputs to result.\n\nresult &lt;- list(arg1 = arg1, arg2 = arg2)\n\nAfterwards, set class attribute to something meaningful, for example,\n\nresult &lt;- structure(result, class = 'SimulationResult')\n\nThis allows us to implement generics.\n\n\nGenerics\nCommon generics\n\nprint\nsummary\nplot\n\n\nprint.classname &lt;- function(x, ...){\n  # x is an object of `classname`\n  # it should be a list, and have class attribute `classname`\n}"
  },
  {
    "objectID": "programming/r_pkg_functions.html#test",
    "href": "programming/r_pkg_functions.html#test",
    "title": "My workflow in working with functions in a package",
    "section": "Test",
    "text": "Test\nTests for functions and generic functions seem to be the same as before. Need to go back to the function and add error messages."
  },
  {
    "objectID": "programming/r_pkg_functions.html#document",
    "href": "programming/r_pkg_functions.html#document",
    "title": "My workflow in working with functions in a package",
    "section": "Document",
    "text": "Document\nThe way to document generic functions is exactly the same as any other function.\n\n#' Print method\n#'\n#' @description\n#' Generic function to print a `SimulationResult` object\n#'\n#' @param x a \\code{SimulationResult} object to print\n#' @param ... further arguments to pass from other methods\n#'\n#' @return something printed\n#' @export\n#'\n#' @examples\n#' simd &lt;- fsim(n1 = 10, n2 = 10, mean1 = 0, mean2 = 5, sd1 = 1, sd2 = 1)\n#' print(simd)"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git_branching.html",
    "href": "dev/oldnotes_programming/git/Git_branching.html",
    "title": "Update from origin and update branches",
    "section": "",
    "text": "as the same user (main github account)\n\ncloned the repo Paper3, it created a new path /Documents/GitHub/Paper3/"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git_branching.html#branches",
    "href": "dev/oldnotes_programming/git/Git_branching.html#branches",
    "title": "Update from origin and update branches",
    "section": "Branches",
    "text": "Branches\nType 1 branch: local branches\nType 2 branch: remote-tracking branches\n\n\n\ngit_branching\n\n\n\nCreate and switch to branch\ngit checkout -b my_new_branch\n\n# what it does:: \ngit branch my_new_branch\ngit checkout my_new_branch"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git_branching.html#master-is-ahead-of-branch",
    "href": "dev/oldnotes_programming/git/Git_branching.html#master-is-ahead-of-branch",
    "title": "Update from origin and update branches",
    "section": "master is ahead of branch:",
    "text": "master is ahead of branch:\nUpdate the branch.\ngit checkout my_new_branch  # go to branch\ngit status # this branch is still up to date with origin/my_new_branch, even though it is behind master\nCan check the difference between these two. Note that the order matter: these two are not the same!\ngit diff my_new_branch master \ngit diff master my_new_branch\nmerge into master (even though branch is behind master!)\ngit merge master  # when on branch\nAt this stage, the branch will be ahead of the remote branch origin/my_new_branch. Now need to push the updates through, to make the remote branch updated too.\n\n\n\ngit_merge_eg2"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git_branching.html#branch-is-ahead-of-master",
    "href": "dev/oldnotes_programming/git/Git_branching.html#branch-is-ahead-of-master",
    "title": "Update from origin and update branches",
    "section": "branch is ahead of master",
    "text": "branch is ahead of master\n\nOption 1: merge branch into master locally, then push master\nNeed to check difference to make sure everything is correct.\ngit merge my_new_branch  # when on master. \n\n\n\ngit_merge_eg3\n\n\nNote: from network of branches, this merge will NOT appear as the merge is done locally.\n\n\nOption 2: push to remote branch, pull request, and merge into master\nOn branch, NOT master!\n\nIf push to branch (NOT master, WITHOUT merge) at this point, the changes will appear on github under this branch (remote), which requires a pull request.\nIf merge master (on branch), nothing will happen.\n\n\n\n\ngit_merge_eg"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git_branching.html#delete-branch-after-pull-requested-and-merging-from-remote",
    "href": "dev/oldnotes_programming/git/Git_branching.html#delete-branch-after-pull-requested-and-merging-from-remote",
    "title": "Update from origin and update branches",
    "section": "delete branch (after pull requested and merging from remote)",
    "text": "delete branch (after pull requested and merging from remote)\ngit fetch -p \n\n# check which branch is there\ngit branch\ngit branch --merged # which merged branch we can delete"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html",
    "href": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html",
    "title": "Template 1: minimal",
    "section": "",
    "text": "_quarto.yml"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#metadata",
    "href": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#metadata",
    "title": "Template 1: minimal",
    "section": "",
    "text": "_quarto.yml"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#index",
    "href": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#index",
    "title": "Template 1: minimal",
    "section": "index",
    "text": "index\nindex.qmd"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#about",
    "href": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#about",
    "title": "Template 1: minimal",
    "section": "about",
    "text": "about\nabout.qmd"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#style-definition",
    "href": "dev/oldnotes_programming/rstudio_extension/quarto_site_component.html#style-definition",
    "title": "Template 1: minimal",
    "section": "style definition",
    "text": "style definition\nstyle.css\n\n\nTo do\nimport one file from inst"
  },
  {
    "objectID": "dev/oldnotes_programming/unix/BashAndLinux.html",
    "href": "dev/oldnotes_programming/unix/BashAndLinux.html",
    "title": "General",
    "section": "",
    "text": "it is a standard subdirectory of the root directory / in unix-like OS that contains tthe executables. It contains the most basic programs (ls, rm, …) . Name means binaries, compiled programs. (most important one)\nwhich programName will give where the program is (path). For example, which R will give /usr/local/bin/R and a few other useful programs are under here (psql, brew etc).\n/usr stands for Unix System Resources\n`` is called ‘backticks’.\n\n\n\n\n\n\nvariable\npurpose\nValue\n\n\n\n\nHOME\nhome direcotry\n/Users/andrea\n\n\nPWD\npresent working directory\npwd\n\n\nSHELL\nwhich shell is used\n/bin/bash\n\n\nUSER\nuser’s ID\nandrea\n\n\n\nset | grep HISTFILESIZE # history file size \nsetting\n\n\necho text  # print text\necho USER  # print USER, it is a variable name\necho $USER # print value of the variable, which is andrea \necho $OSTYPE # darwin18, in other places might be linux-gnu\n\n\n\n\nlocal, user defined\nselfdefinedvar=seasonal/winter.csv  # without space! \necho $selfdefinedvar\nhead -n 1 $selfdefinedvar     # note $ is necessary\n\n\n\nlocale  # check local variables \nnano ~/.profile  # write what is necessary \n. ~/.profile  # execute\njust use language “en_US.UTF-8”\nLC_ALL=\"en_US.UTF-8\"\n\n\n\nChiZhangWork:~ andrea$ pwd\n/Users/andrea\n\n\n\nman head   # manual for head\n\n\n\nhead summer.csv  # reports error, wrong directory\ncd seasonal/\n\n!head   # useful! automatically runs \nhistory  # displays history \n!3      # if that is the 3rd command \n\n\n\nread, write, execute.\nls -l [path]\n-rwxr----x    \n\nfirst character is file type: - means normal file, d means directory\nnext 3 characters are permissions for owner. if -, absence of permission.\nnext 3 characters are permissions for the group. Read, write and execute. In this example, no permission apart from read.\nlast 3 are permissions for others.\n\nchmod [permission][path]\n# example \nchmod g+x frog.png\n\nwho change? [ugoa]: user (owner), group, others, all\ngranting or revoking the permission? + or -\nwhich permission are we setting? r, w, x.\n\nPermission 755 is -rwxr-xr-x, which means standard file -, owner has all permission rwx, group and others have the same permission r-x.\nchmod 755 frog.png"
  },
  {
    "objectID": "dev/oldnotes_programming/unix/BashAndLinux.html#view-content",
    "href": "dev/oldnotes_programming/unix/BashAndLinux.html#view-content",
    "title": "General",
    "section": "View content",
    "text": "View content\n\nselect rows\ncat  # concatenate\nless bashLearning/Bash.md blogdown.md  # view 2, use :n and :p to shift file \nmore \nhead # 10 rows \nhead -n 3 file.txt\ntail \n\n\nList all\nls -R  # R for recursive \nls -F  # F will give a * after all the runable program (i.e. C++)\n\n\nselect columns\n-f for fields to specify columns\n-d for delimiter for separator\ncut -f 2-5, 8, -d , values.csv    # select col 2 to 5 and 8, using comma as separator. \n\n\nselect with search\nselect lines according to what file contains\ngrep keyword file.txt     # returns lines that match keyword \n\n-c # count of matching lines \n-n # line number\n-v # those don't contain\n-h # suppress the file names (default when there's only 1 file)\ngrep -c incisor seasonal/autumn.csv seasonal/winter.csv # count occurence in 2 files \n\n\ncount\nwc -l # line count\nwc -c # characters,bytes\nwc -w # word count\n\n\nsort\nsort -n  # numerically\nsort -r # reverse \nsort -b # ignore leading blanks \nsort -f # fold case\nOften used with unique,\ncut -d , -f 2 seasonal/winter.csv | grep -v Tooth | sort |uniq -c  # give unique and count\n\n\nredirect output\nhead -n 5 seasonal/summer.csv &gt; top.csv # saved to top.csv\nNote: never in the middle of a pipe!\n\n\npipe\nhead -n 5 seasonal/summer.csv | tail -n 3\n\n\nWild cards\n*.csv\n?017.txt     # single character \n201[78].txt. # either 2017 or 2018. ONLY ONE CHARACTER\n{*.txt, *.csv}  # anything inside \n\n\nloop\nfor … variable … in .. list .. ; do .. body.. ; done\nITskills andrea$ for filename in *.md; do echo $filename; done  # this gives the names of md files in ITskills\nIt is convenient to name a bunch of files with one shell variable, then refer to it later on.\nfiles=seasonal/*.csv\nfor f in $files; do echo $f; done    # the $ in the first part is important"
  },
  {
    "objectID": "dev/oldnotes_programming/unix/BashAndLinux.html#edit-files",
    "href": "dev/oldnotes_programming/unix/BashAndLinux.html#edit-files",
    "title": "General",
    "section": "Edit files",
    "text": "Edit files\ncreate a blank file: touch fileName.\nnano file.txt\nCtrl + K: delete\nCtrl + U: un-delete\nCtrl + O: save file (output)\nCtrl + X: exit editor"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-debugging.html",
    "href": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-debugging.html",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "The chapter of debugging\n\ntraceback()"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt3-oop.html",
    "href": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt3-oop.html",
    "title": "OOP in R",
    "section": "",
    "text": "Chapter 3: object oriented programming with R"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt3-oop.html#base-types",
    "href": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt3-oop.html#base-types",
    "title": "OOP in R",
    "section": "Base types",
    "text": "Base types\nEverything in R is an object, but not object-oriented.\nobject (in R)\n|\n|_base objects\n|_OO objects\nEvery object has a base type.\ntypeof(1:10)  # integer\ntypeof(mtcars)  # list\n\n# object type\notype(1:10)  # base\notype(mtcars)  # s3\n\nvectors\n\nNULL\nlogical, integer, double, character, complex, raw\nlist\n\nFunctions\n\nclosure (regular R functions)\nspecial (internal functions, such as [)\nbuiltin (primitive functions, such as sum)\n\nlanguage components\n\nsymbol, language, pairlist"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/r_regular_expression.html",
    "href": "dev/oldnotes_programming/r_advanced_r_book/r_regular_expression.html",
    "title": "Regular expressions",
    "section": "",
    "text": "Regular expressions\ncombination of literals (i.e. words) and metacharacters (i.e. grammar)\nmatching literals (exact words) are not enough, we need a way to express\n\nwhitespace\nsets of literals\nBeginning and end of a line\nalternatives (‘war’ or ‘peace’)\n…\n\nuse meta characters!\n\n^: start of a line\n^i think matches\ni think we are ..\ni think this …\n\n\n$: end of a line\nmorning$ matches\nwell they had something this morning\ngood morning\n\n\n[] character classes\n[Bb][Uu][Ss][Hh] matches\nBush\nbush\n^[Ii] am matches I am, i am\n^[0-9][a-zA-Z] matches 7th …, 2nd…, 3am…, …. (line starts with one number and one letter)\n\n\n^ inside []: Not in the class\n[^?.]$ matches characters/sentences that do NOT end with ? or .\ni like basketballs\n6 and 9\nanyway!\n\n\n.: any character\n9.11 matches\n9-11\n9.11\n169.114\n9:11:46AM\n… 8199119725 …\n\n\n\\.: period\n(. in this case is not a meta character, but a literal character)\n\n\n?: optional (outside [])\n[Gg]eorge( [Ww]\\.)? [Bb]ush\ngeorge bush\nGeorge W. Bush\ntwo george bushes\n\n\n|: or\nflood|fire matches firewire …, global flood makes sense,…, floods, horricanes\n^[Gg]ood|[Bb]ad means good/Good needs to be at the beginning, or Bad/bad anywhere\n^([Gg]ood|[Bb]ad) means good/Good and Bad/bad need to be at the beginning\n\n\n*: any number, including none\n(.*) matches\nchat? (24, m, germany)\n()\n\n\n+: at least one of the item\n[0-9]+ (.*)[0-9]+\nmeans a number can be repeated many times\n\n\n{}: inerval\n[Bb]ush( +[^ ]+ +){1, 5} debate\nat least one space, followed by NOT space, followed by a space === a word, repeated between 1 and 5 times\nBush has historically won all major debates he’s done"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt1-basics.html",
    "href": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt1-basics.html",
    "title": "Names and values",
    "section": "",
    "text": "Notes from book Advanced R (part 1, foundations)"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt1-basics.html#handlers",
    "href": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt1-basics.html#handlers",
    "title": "Names and values",
    "section": "handlers",
    "text": "handlers\ntryCatch() for error\nfail_with &lt;- function(expr, value = NULL){\n  tryCatch(\n    error = function(cnd) value,  # this function here is necessary\n    expr\n  )\n}\nfail_with(log(10))   # execute normally\nfail_with(log('x'))  # here value is prompted\nfail_with(log('x'), value = NA)   # returns NA\nDifference between try() and tryCatch(): if the former meets an error, it passes onto the next code chunk to execute. The later will modify the behaviour of the exit, for example, give a specific value if an error occurs.\nwithCallingHandlers() for warning and messages"
  },
  {
    "objectID": "dev/oldnotes_stat/survival/notes_survival_intro.html",
    "href": "dev/oldnotes_stat/survival/notes_survival_intro.html",
    "title": "KM plot",
    "section": "",
    "text": "probability of surviving\nsurvival function"
  },
  {
    "objectID": "dev/oldnotes_stat/survival/notes_survival_intro.html#life-tables",
    "href": "dev/oldnotes_stat/survival/notes_survival_intro.html#life-tables",
    "title": "KM plot",
    "section": "life tables",
    "text": "life tables\nmeasure p of death at a given age, and life expectancy at varying ages\ncohort/generational life tables, current/period life tables"
  },
  {
    "objectID": "dev/oldnotes_stat/inference/notes_sample_size.html",
    "href": "dev/oldnotes_stat/inference/notes_sample_size.html",
    "title": "Sample size calculation",
    "section": "",
    "text": "Two types of statistical inference: confidence intervals, hypothesis testing\nTwo approaches\nSource of error\ntruth + systematic error + random error\nsystematic error (bias): faulty design, lack of randomization, blinding\nrandom error can be reduced by increasing the sample size\nThe effect of random error decreases by the square roo of n as samples increases\nSE of the mean: sigma/sqrt(n)\nSE of a proportion: sqrt(p(1-p)/n)"
  },
  {
    "objectID": "dev/oldnotes_stat/inference/notes_sample_size.html#precision-based",
    "href": "dev/oldnotes_stat/inference/notes_sample_size.html#precision-based",
    "title": "Sample size calculation",
    "section": "Precision based",
    "text": "Precision based\n\nmean\nfor given precision a, standard deviation sigma, number of observation required:\nn = 4 sigma^2 / a^2\n\n\nproportion\nn = 4 p(1-p)/a^2"
  },
  {
    "objectID": "dev/oldnotes_stat/inference/notes_sample_size.html#hypothesis-testing",
    "href": "dev/oldnotes_stat/inference/notes_sample_size.html#hypothesis-testing",
    "title": "Sample size calculation",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nPower = P(reject h0 | ha true), i.e. identifying a difference between groups if there is a difference\n80%"
  },
  {
    "objectID": "dev/oldnotes_stat/inference/notes_sample_size.html#effect-size",
    "href": "dev/oldnotes_stat/inference/notes_sample_size.html#effect-size",
    "title": "Sample size calculation",
    "section": "Effect size",
    "text": "Effect size\nformula depends on type of analysis\n\ntwo sample t-test\npaired -test\ncompare proportions\n\nEffect size\n\nclinically relevant difference, \\(\\Delta\\)\nstadard deviation in both groups, sigma\neffect size: \\(\\Delta\\)/sigma\n\nExample\n\ncompare two groups, difference set to be \\(\\Delta = 0.5\\), standard deviation \\(\\sigma = 1\\)\nEffect size: \\(0.5/1 = 0.5\\)\nSet significance level alpha = 0.05, power 1-beta = 0.8\n(by nomogram) total sample size is 120, i.e. each group needs 60"
  },
  {
    "objectID": "dev/oldnotes_stat/inference/notes_sample_size.html#formula-two-means-indep-samples",
    "href": "dev/oldnotes_stat/inference/notes_sample_size.html#formula-two-means-indep-samples",
    "title": "Sample size calculation",
    "section": "Formula (two means, indep samples)",
    "text": "Formula (two means, indep samples)\nn = 2 sigma2/delta2 k\n\nk = (u+v)^2\nu: one sided percentage point of normal distribution corresponding to beta\n\nu = 0.84 for 80% power\n\nv: two-sided percentage point of normal distribution corresponding to alpha\n\nv = 1.96 for 5% significance level\n\n\n(above example)\n\nn = 2/0.5^2 *(1.96+0.84)^2 = 62.72"
  },
  {
    "objectID": "dev/oldnotes_stat/inference/notes_sample_size.html#formula-paired-data",
    "href": "dev/oldnotes_stat/inference/notes_sample_size.html#formula-paired-data",
    "title": "Sample size calculation",
    "section": "Formula: paired data",
    "text": "Formula: paired data\ncross-over study\neffect size: 2 delta/sigma d\n\nsigma d is the sd of difference between the two measurements"
  },
  {
    "objectID": "dev/oldnotes_stat/inference/notes_sample_size.html#proportions-in-two-groups",
    "href": "dev/oldnotes_stat/inference/notes_sample_size.html#proportions-in-two-groups",
    "title": "Sample size calculation",
    "section": "Proportions in two groups",
    "text": "Proportions in two groups\n\np1, p2\nrelevant difference: p1-p2\naverage proportion: pbar = (p1+p2)/2\neffect size: (p1-p2)/(sqrt(pbar * (1-pbar)))\n\nExample:\nproportions are 0.1, 0.2\naverage is 0.15\neffect size: (0.2-0.1)/sqrt(0.15*(1-0.15)) = 0.28"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/diagnostics.html",
    "href": "dev/oldnotes_stat/lm_glm/diagnostics.html",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "(most of them will be negative)\nif increase, indicates better fit"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/diagnostics.html#log-likelihood",
    "href": "dev/oldnotes_stat/lm_glm/diagnostics.html#log-likelihood",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "(most of them will be negative)\nif increase, indicates better fit"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/count_4_bayes_glm_count.html",
    "href": "dev/oldnotes_stat/lm_glm/count_4_bayes_glm_count.html",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "count data modeling in the bayesian framework\n\nData: yi | beta ~ pois(lambda_i), log(lambda_i) = bx_i\nPriors: normal priors\n\nBeta ~ N()\n\n\nsetting prior for beta (normal)\n\nmean: assume typical average response is around 7\n\ncan set beta_0 ~ N(2, sigma) (on log scale)\nLog(7) = 1.95\n\nsd\n\nlogged number is around 2 (between 1-3, for instance)\nactual number is between 3 to 20 (exp(1), exp(3))"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/count_2_overdisp.html",
    "href": "dev/oldnotes_stat/lm_glm/count_2_overdisp.html",
    "title": "Quasi poisson",
    "section": "",
    "text": "Over dispersion\n\nThese are still in the GLM framework. They all have the same log-linear mean function \\(log(\\mu) = X\\beta\\), but the remaining assumptions are different.\nsandwich covariates\nAfter using this, the SE is larger -&gt; more conservative for significant covariates -&gt; fewer significant\n\nQuasi poisson\n\\(\\phi\\) estimated from data, not fixed at 1\n\\(\\beta\\) estimates are the same, but inference is different\nqp is not a full maximum likelihood model, but a quasi-ml model\nless assumption:\n\nlog-linear relationship between e(y|x) and xb\nlinear relationship between variance and expectation\n\nuse robust sandwich standard errors\n\n\nestimated differently, can not use ML. Some tests can not be used\n(doesn’t seem to have a bayesian version)\n\n\nNB regression\n\\(\\phi = 1\\), \\(V(\\mu) = \\mu + \\mu^2/\\theta\\)\nGeometric model (theta = 1)\npositive reciprocal dispersion parameter \\(\\phi\\) included, such that\n\\(sd(y|x) = \\sqrt{E(y|x) + E(y|x)^2 /\\phi}\\)\n\nSmaller \\(\\phi\\), more dispersion (i.e. larger sd(y|x))\n\\(\\phi\\) -&gt; inf, approach poisson (equal mean and var)"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/count_3_zeroinf.html",
    "href": "dev/oldnotes_stat/lm_glm/count_3_zeroinf.html",
    "title": "Excess 0",
    "section": "",
    "text": "Assumes 0 comes from 2 different process. These are not GLMs. 0’s likelihood is increased\nKey difference between the 2 types of subject:\n\nhurdle: 1. never experience outcome; 2. experience at least once\nzero inflated: 1. never experience outcome; 2. experience, but not always. (Can be zero)\n\n\n\nbinary process.\n\noff with probably \\(\\pi\\), only zero counts possible\non with probablity \\(1-\\pi\\).\n\n\n\n\n\n\nCount need to be at least 1.\nUse zero-truncated PD\n\n\n\nConceptual example: first decide whether buy things: yes or no. If yes, can end up buying (positive counts), can also end up buying nothing (out of stock)\nUse an usual discrete PD for counts, such as Poisson or NB"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/count_3_zeroinf.html#zero-part-same-for-both",
    "href": "dev/oldnotes_stat/lm_glm/count_3_zeroinf.html#zero-part-same-for-both",
    "title": "Excess 0",
    "section": "",
    "text": "binary process.\n\noff with probably \\(\\pi\\), only zero counts possible\non with probablity \\(1-\\pi\\)."
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/count_3_zeroinf.html#counting-part",
    "href": "dev/oldnotes_stat/lm_glm/count_3_zeroinf.html#counting-part",
    "title": "Excess 0",
    "section": "",
    "text": "Count need to be at least 1.\nUse zero-truncated PD\n\n\n\nConceptual example: first decide whether buy things: yes or no. If yes, can end up buying (positive counts), can also end up buying nothing (out of stock)\nUse an usual discrete PD for counts, such as Poisson or NB"
  },
  {
    "objectID": "dev/oldnotes_stat/multivariate/Correspondence_analysis.html",
    "href": "dev/oldnotes_stat/multivariate/Correspondence_analysis.html",
    "title": "Simple CA",
    "section": "",
    "text": "Used for categorical (mostly survey) data, to identify\n\na group of individuals with similar profiles in their answers\nassociation between variable categories\n\n\ncontingency table (counts)\nx_ij: number of individuals with category i of V1, and category j of V2.\nindependence events: P(A and B) = P(A) * P(B)\nindependence qualitative variables: \\(p_{ij} = p_{.i} * p_{.j}\\)\n\njoint probability = product of marginal probabilities\n\nequivalently, \\(\\frac{p_{ij}}{p_{.i}} = p_{.j}\\)\n\nconditional probability = marginal probability\n\nCA works with table of probabilities, but it is not a test. aims to visualise links between variables\nhttp://www.sthda.com/english/articles/22-principal-component-methods/67-correspondence-analysis-course-using-factominer/\nhttps://www.youtube.com/watch?v=Z5Lo1hvZ9fA\nhttp://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/114-mca-multiple-correspondence-analysis-in-r-essentials/\n\n\nSimple CA\n\nstep 0: create contingency table\nThis requires summarising the raw data into counts.\n\n\nstep 1: chi-sq test for independence\nexamine association between row and column\n# Chi-square statistics\nchi2 &lt;- 1944.456\n# Degree of freedom\ndf &lt;- (nrow(housetasks) - 1) * (ncol(housetasks) - 1)\n# P-value\npval &lt;- pchisq(chi2, df = df, lower.tail = FALSE)\npval\n\n\nstep 2: CA, check eigenvalue\nLargest eigenvalue corresponds to the axis that explains the most variance\n\n\n\nMultivariate CA"
  },
  {
    "objectID": "dev/oldnotes_stat/multivariate/Notes_factor_analysis.html",
    "href": "dev/oldnotes_stat/multivariate/Notes_factor_analysis.html",
    "title": "Exploratory factor analysis",
    "section": "",
    "text": "Exploratory factor analysis\ngoal: identify underrlying relationships between variables"
  },
  {
    "objectID": "dev/oldnotes_stat/multivariate/Notes_multivariate_analysis.html",
    "href": "dev/oldnotes_stat/multivariate/Notes_multivariate_analysis.html",
    "title": "covariance, correlation, distance",
    "section": "",
    "text": "covariance, correlation, distance\ncovariance: linear dependence of two rv\n\n\nPCA\nexploratory, not inferential\nintuition\n\nPC1: find a straightline to fit your data (maximise the variance in this direction; minimise the error with this line)\nPC2: find a straightline to fit the error from above\n\n\ntechnical details\ndescribe correlated X variables by uncorrelated Z variables, which are linear combinations of the Xs\nPC1 = \\(z_1 = a_{11}x_1 + a_{12} x2 + ... + a_{1q}x_q\\)\nSample variance is greatest among all such linear combinations, under the constraint that \\(a_1^Ta_1 = 1\\)\nSample variance of \\(z_1 = a_1^TSa_1\\) (a1 is the vector)\n\\(a_1\\) that satisfies the solution is the either vector of the sample covariance matrix S correponding to the largest eigenvalue.\n\\(\\sum_{i= 1}^{q} \\lambda_i = s_1^2 + s_2^2 + ... + s_q^2\\), equivalently \\(\\sum_{i= 1}^{q} \\lambda_i = trace(S)\\)\n\n\nExtracted on sample correlation matrix\nPCA is not scale-invariant. to be safe, use correlation matrix"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/book_ros_chpt9_pred_bayesinf.html",
    "href": "dev/oldnotes_stat/lm_glm/book_ros_chpt9_pred_bayesinf.html",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "Need to understand whether it is necessary to use bayesian prediction.\n\nrstanarm (different from arm)\nsection 1.6, 8.4, 9.5\nrstanarm::stan_glm: bayesian glm with optionnal prior distribution for the coefficients, intercept and auxilliary parameters\nFor a regression, the results are\n\nintercept\nslope\nresidual standard deviation (reported as auxiliary parameters)\n\nmedian, mad_sd (median absolute deviation)\n\nprediction (bayesian)\npoint prediction: \\(\\hat{a} + \\hat{b}x^{new}\\)\n\none point, no uncertainty\ne.g. point prediction is the best estimate of the average blood pressure in the population, conditional on dose \\(x^{new}\\)\n\nLinear predictor with uncertainty: \\(a + bx^{new}\\)\n\nContains inferential uncertainty in a, b\ndistribution of uncertainty of expected or average \\(y\\) for a new observation\nn -&gt; inf, uncertainty -&gt; 0 as a and b are more and more precise\n\npredictive distribution for a new observation: \\(a + bx^{new} + error\\)\n\nuncertainty about new obervation\nblood pressure of a single person drawn at random from this population\nn-&gt;inf, uncertainty -&gt; residual standard deviation sigma\n\n\n\npriors\nweekly informative prior (default)\n\nlinear model:\n\nbk ~N(0, 2.5*sd(y)/sd(xk))\na ~ N(mu_y, 2.5*sd(y))\nsigma ~ exp(1/sd(y))\n\n\ncheck prior: prior_summary()\n\n\nimplementation of arm::sim()\ngetMethod(arm::sim, 'lm') can provide how beta and sigma are simulated\nhttps://stats.stackexchange.com/questions/192996/why-does-the-sim-function-in-gelmans-arm-package-simulate-sigma-from-inverse-ch"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/count_1_poireg.html",
    "href": "dev/oldnotes_stat/lm_glm/count_1_poireg.html",
    "title": "Poisson regression",
    "section": "",
    "text": "Poisson regression\na very good resource: https://data.princeton.edu/wws509/notes/c4.pdf\nWhen count is large, can also use linear. (as Poisson distribution can be approximated for large \\(\\lambda\\), 20)\n\nhowever the concern is the variance, it is not constant: larger count has larger variance due to \\(\\mu\\)\n\n\nCAUTION: for poisson example, it’s NOT \\(log(y) = xb + e\\), it is \\(log(\\mu) = xb\\). Log in the mean!\n\nLog-linear regression (an inappropriate alternative)\n\n\\(log (y) = x'\\beta + u, u \\sim N(0, \\sigma^2)\\)\nmean function is scaled by \\(exp(0.5*\\sigma^2)\\), the expectation of error term (log linear)\nignores the discrete nature of data (y=0, 1, 2…)\n0 count inadmissible: log(0) not defined\ninference problem: standard error\nbias introduced: parameters estimated away from 0.\n\n\n\nInterpretation: additive on the log scale\n\\(exp(\\beta_1 x_1 + \\beta_2 x_2) = exp(\\beta_1 x_1) * exp(\\beta_2 x_2)\\)\nInterpreted as (the natural logarithm of) ratios\n\nincidence rate ratio\nrelative risk\n\nOne increase in \\(x_i\\) increases the log mean by \\(\\beta\\) units, \\(log(\\mu_i) = x_i \\beta\\)\nOne increase in \\(x_i\\) increases the mean by \\(exp(\\beta)\\) times, \\(mu_i = exp(x_i \\beta)\\)\ndispersion parameter \\(\\phi = 1\\)\n\n\nDeviance\ndifference: model deviance, residual devviance + degree of freedom\n\n\nInference\nUnderestimates the variance in the data, hence need to use more robust measures, such as sandwich covariates\nAnalytical SE\n\n\n\nQuestions\nhow to judge whether the prediction is good, given that the results are counts? with Logistic reg there are classification metrics, with linear reg there are MSE, how about counts?"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/GLM.html",
    "href": "dev/oldnotes_stat/lm_glm/GLM.html",
    "title": "Formulation",
    "section": "",
    "text": "\\(E(Y|X) = \\mu = g^{-1} (X\\beta)\\)\n\n\\(E(Y|X)\\) is the expected value of Y conditional on X\n\\(g\\) is the link function, from exponential family, need to be invertible\nmean function is for the outcome. e.g. \\(\\mu = g^{-1} =exp(X\\beta)\\) for poisson regression.\n\n\\(Var(Y|X) = V(\\mu) = V(g^{-1}(X\\beta))\\)\nCAUTION: for poisson example, it’s NOT \\(log(y) = xb + e\\), it is \\(log(\\mu) = xb\\)\nwhere is the error term\n\n\n\\(f(x|\\theta) = h(x)*exp[\\eta(\\theta)T(x) - A(\\theta)]\\)\n\nT(x) is the sufficient statistic\n\\(\\eta(x)\\) is the natural parameter\n\\(h(x)\\) is the base\n\\(A(\\theta)\\) is the cumulant generating function\n\nEF is used for lower variance of unbiased estimator (??)\nA sufficient statistic for the parameter exists\n\\(A(\\eta(\\theta))\\) can be used to derive moments (mean, variance) by taking first and second derivative\n\n\n\n\nLogit model (logistic regression):\\(\\eta(\\theta) = log(\\frac{p}{1-p})\\), it is the natural parameter of bernoulli distribution\nPoisson distribution, \\(\\eta(\\theta) = log(\\lambda)\\)\n\nNote that other distributions can have natural parameters, but do not correspond directly to GLM. such as normal.\n\n\n\nLogistic reg has logit link \\(log(\\frac{p}{1-p})\\). Only predicts 1 and 0\nBinomial regression has logit link \\(log(\\frac{p}{n-p})\\), predicts number of success in n trials, 0, 1, .. N.\n\n\n\nPredict the values of the conditional mean \\(E(Y|X) = g^{-1}(X\\beta)\\) through the linear predictor.\n\nfitted(model) is equivalent to predict(model, type = 'response'), returns \\(g^{-1}(X\\hat\\beta)\\) , the conditional response\npredict(model, type = 'link') returns \\(X\\hat{\\beta}\\)\n\nCI need to be extracted manually; or use the other function confint\n\n\n\n?? more predictors, larger variance, less power??\nThe response is heterskedastic, unlike linear case.\n\n\n\n\n\ndifference of log-likelihod between fitted model and saturated model. it is a generalisation of residual sum of squares RSS (i.e. sum of squared error SSE)\nResidual deviance: \\(D := -2[loglik(\\hat\\beta) - loglik_s]\\phi\\)\nNull deviance: \\(D_0 := -2[loglik(\\hat\\beta_0) - loglik_s]\\phi\\), the intercept model and saturated model. It iis a generalisation of total sum of squares, SST = sum(yi - mean(y)^2)\n\n\n\ndeviance\n\n\nThe deviance is approximately chi-square distributed with \\(n-p\\) degrees of freedom.\n\nNull deviance will be n-1, for the intercept"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/GLM.html#prediction",
    "href": "dev/oldnotes_stat/lm_glm/GLM.html#prediction",
    "title": "Formulation",
    "section": "",
    "text": "Predict the values of the conditional mean \\(E(Y|X) = g^{-1}(X\\beta)\\) through the linear predictor.\n\nfitted(model) is equivalent to predict(model, type = 'response'), returns \\(g^{-1}(X\\hat\\beta)\\) , the conditional response\npredict(model, type = 'link') returns \\(X\\hat{\\beta}\\)\n\nCI need to be extracted manually; or use the other function confint"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/GLM.html#inference",
    "href": "dev/oldnotes_stat/lm_glm/GLM.html#inference",
    "title": "Formulation",
    "section": "",
    "text": "?? more predictors, larger variance, less power??\nThe response is heterskedastic, unlike linear case."
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/GLM.html#diagnostics-and-selection",
    "href": "dev/oldnotes_stat/lm_glm/GLM.html#diagnostics-and-selection",
    "title": "Formulation",
    "section": "",
    "text": "difference of log-likelihod between fitted model and saturated model. it is a generalisation of residual sum of squares RSS (i.e. sum of squared error SSE)\nResidual deviance: \\(D := -2[loglik(\\hat\\beta) - loglik_s]\\phi\\)\nNull deviance: \\(D_0 := -2[loglik(\\hat\\beta_0) - loglik_s]\\phi\\), the intercept model and saturated model. It iis a generalisation of total sum of squares, SST = sum(yi - mean(y)^2)\n\n\n\ndeviance\n\n\nThe deviance is approximately chi-square distributed with \\(n-p\\) degrees of freedom.\n\nNull deviance will be n-1, for the intercept"
  },
  {
    "objectID": "dev/oldnotes_stat/lm_glm/book_ros_chpt15_glm.html",
    "href": "dev/oldnotes_stat/lm_glm/book_ros_chpt15_glm.html",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "glm in the bayesian framework\n\n\nglm components\n\nvector of outcome data y\nmatrix of predictors X, vector of coefficients \\(\\beta\\) forming a linear predictor vector \\(X\\beta\\)\nlink function \\(g\\), giving transformed data \\(\\hat{y} = g^{-1}(X\\beta)\\)\ndata distribution \\(y|\\hat{y}\\)\nother parameters\n\n\n\nexamples\n\nLinear regression, \\(g(u) = u\\), data distribution is normal with sd \\(\\sigma\\)\nlogistic regression, \\(g^{-1}(u) = logit^{-1}(u)\\)\n\nLogit (log odds) link, \\(g(p) = ln(p/1-p)\\)\nLogit(y|x) = xb\ndata distribution defined by probability, \\(p(y=1) = \\hat{y}\\)\n\nPoisson, nb\n\nlog link, log(y|x) = xb\nLog(lambda_i) = b0 + b1x1i + b2x2i + …\n\\(y_i\\) ~ \\(poisson(exp^{x_i\\beta})\\)\nequal mean and variance conditional on x: larger mean, larger variance (more sparse)\n\nlogistic-binomial (also a count)\n\ny is number of successes in n_i trials\ndependent on a limit (total trials)\nuse poisson/nb when the limit is much larger (e.g. population)"
  },
  {
    "objectID": "dev/oldnotes_stat/mixed_effect/Notes_mixed_models.html",
    "href": "dev/oldnotes_stat/mixed_effect/Notes_mixed_models.html",
    "title": "Mixed effects models",
    "section": "",
    "text": "Mixed effects models\nhow is it different from multilevel modeling\nhow is it different from longitudinal analysis\nhierarchical: e.g. repeated observations nested within students, students nested within schools, schools nested in districts\n\n\nrepeated measurement (e.g. longitudinal)\ngpa example\nModel 1: linear regression\n\ngpa = \\(\\beta_0\\) + \\(\\beta\\) * occasion + \\(\\epsilon\\)\n\nModel 2: random intercept\n\nOne intercept for one student (student effect)\ngpa = \\(\\beta_0\\) + effect_i + \\(\\beta\\) * occasion + \\(\\epsilon\\)\ngpa = \\(\\beta_{0,i}\\) + \\(\\beta\\) * occasion + \\(\\epsilon\\) , where \\(\\beta_{0,i} ~ N(\\beta_0, \\tau)\\)\nrandom intercept is distributed around overall intercept\n\n?? variance output for the randomeffect \n?? correlation with fixed effect? \n\n\ncross effect and nested effect\nnot sure how the former is different"
  },
  {
    "objectID": "dev/oldnotes_programming/database/Database-psql101.html",
    "href": "dev/oldnotes_programming/database/Database-psql101.html",
    "title": "Database",
    "section": "",
    "text": "Relational database management systems. Relational model shapes whatever information to be stored by defining them as related entities with attributes across tables (i.e. schemas)\n\n\ndisadvantages:\nThe way certain functionality gets handled with MySQL (e.g. references, transactions, auditing etc.) renders it a little-less reliable compared to some other RDBMSs.\nSince MySQL does not [try to] implement the full SQL standard, this tool is not completely SQL compliant. If you might need integration with such RDBMSs, switching from MySQL will not be easy.\n\n\n\nCompared to other RDBMSs, PostgreSQL differs itself with its support for highly required and integral object-oriented and/or relational database functionality, such as the complete support for reliable transactions, i.e. Atomicity, Consistency, Isolation, Durability (ACID)."
  },
  {
    "objectID": "dev/oldnotes_programming/database/Database-psql101.html#installation",
    "href": "dev/oldnotes_programming/database/Database-psql101.html#installation",
    "title": "Database",
    "section": "installation",
    "text": "installation\nhttps://www.postgresql.org/download/macosx/\n# install homebrew\n\nbrew install postgresql\nbrew services stop postgresql  \nbrew services start postgresql  # necessary\ncheck version\npostgres -V  \n\ninstall a graphical user interface\nI choose postico"
  },
  {
    "objectID": "dev/oldnotes_programming/database/Database-psql101.html#getting-started",
    "href": "dev/oldnotes_programming/database/Database-psql101.html#getting-started",
    "title": "Database",
    "section": "Getting started",
    "text": "Getting started\nandrea$ psql postgres\n\n1. Create user\ncreate with psql (CREATE ROLE) and give it permission ALTER ROLE.\npostgres=# CREATE ROLE chizhang WITH LOGIN PASSWORD 'mypassword'; (andrea is my password)\npostgres=# ALTER ROLE chizhang CREATEDB;\nboth show some information of the database\npostgres=# \\du\npostgres=&gt; \\list\nto quit\npostgres=# \\q \n\n\n2. connect to a default database\nchange user, then create database. The prompt &gt; indicate now it’s not a super user account (andrea).\nandrea$ psql postgres -U chizhang\nCREATE DATABASE mimicdata;\npostgres=&gt; GRANT ALL PRIVILEGES ON DATABASE mimicdata TO chizhang;\n\n\n-------- updated 12.2\n\nCREATE DATABASE demo;\npostgres=&gt; GRANT ALL PRIVILEGES ON DATABASE demo TO chizhang;\npostgres=&gt; \\connect demo \ndemo=&gt; \\dt \n\n-------- updated 19.1.10\n\n# in postico, let user chizhang connect to database demo. \n\\dt lists the tables in currently connected db\n\n\n3. create and drop table\n\nwith GUI postico - create table\nIn postico,\nCREATE TABLE fav_sports3 (\n\n   name char(20),\n   age integer,\n   sport char(20),\n   gender char(20)\n);\nColumns must be consistent with the csv file.\n\n\nOther operations related to table\nto drop table,\nDROP TABLE tablename\nchange table name,\nALTER TABLE table_name RENAME TO new_name;\n\n\n\n4. import csv (or gz)\n\nIssue with importing csv privilege\nIn postico, I need to be a superuser to COPY data from csv. The database demo is owned by user chizhang, which is not a superuser.\n\nSolution 1: Use command line\npostgres=# \\connect demo\ndemo=# \\copy fav_sports3 FROM '/Users/andrea/Documents/PhdProjects/Project-Paper2/Database/trialdata.csv' DELIMITER ',' CSV HEADER;\nNote the difference \\copy. But my code are all written using COPY as SQL. This method doesn’t work for a lot of files. At least I don’t know a fast way.\n\n\nSolution 2: change user privilege\npostgres=# ALTER USER chizhang WITH SUPERUSER;\n\\du\nNow user chizhang has the superuser privilege so can copy csv files using the ready SQL scripts.\nCOPY fav_sports3 FROM '/Users/andrea/Documents/PhdProjects/Project-Paper2/Database/trialdata.csv' DELIMITER ',' CSV HEADER;\nAfter finished, change back.\npostgres=# ALTER USER chizhang WITH NOSUPERUSER;\n\n\nSolution 3: import via postico\nBut need to create table first.\nthen in Postico, do the usual SQL stuff.\nSELECT * FROM fav_sports;\n\n\n\nimport from gz\nThis is an example on how to import from gzip.\n\\copy ADMISSIONS FROM PROGRAM 'gzip -dc ADMISSIONS.csv.gz' DELIMITER ',' CSV HEADER NULL ''\nCOPY FROM PROGRAM\n\ngzip\na lossless data compression, the result usually has suffix .gz. Usage: gzip [OPTION]... [FILE]...\n-d : decompress\n-c: standard output, keep original files unchanged\n\n\n\n\n5. Data type\nWhen creating a table, it is necessary to specify the data types.\nDROP TABLE IF EXISTS ADMISSIONS CASCADE;  -- and all other objects that depends on it\nCREATE TABLE ADMISSIONS\n(\n  ROW_ID INT NOT NULL,   -- not null constraint enforces a column must not accept NULL values\n  SUBJECT_ID INT NOT NULL,\n  HADM_ID INT NOT NULL,\n  ADMITTIME TIMESTAMP(0) NOT NULL,   -- 0 is precision\n  DISCHTIME TIMESTAMP(0) NOT NULL,\n  DEATHTIME TIMESTAMP(0),\n  ADMISSION_TYPE VARCHAR(50) NOT NULL,    -- character varying = variable-length with limit, stores up to n characters\n  ADMISSION_LOCATION VARCHAR(50) NOT NULL,\n  DISCHARGE_LOCATION VARCHAR(50) NOT NULL,\n  INSURANCE VARCHAR(255) NOT NULL,\n  LANGUAGE VARCHAR(10),\n  RELIGION VARCHAR(50),\n  MARITAL_STATUS VARCHAR(50),\n  ETHNICITY VARCHAR(200) NOT NULL,\n  EDREGTIME TIMESTAMP(0),\n  EDOUTTIME TIMESTAMP(0),\n  DIAGNOSIS VARCHAR(255),\n  HOSPITAL_EXPIRE_FLAG SMALLINT,  -- numeric type, +-32768\n  HAS_CHARTEVENTS_DATA SMALLINT NOT NULL,\n  CONSTRAINT adm_rowid_pk PRIMARY KEY (ROW_ID),  -- == unique not null\n  CONSTRAINT adm_hadm_unique UNIQUE (HADM_ID)  -- unique constraint for all rows\n) ;\nDate and time needs special attention.\nSET datestyle = dmy;\nCOPY ADMISSIONS FROM '/Users/andrea/Desktop/Database/DataDemo/ADMISSIONS.csv' DELIMITER ',' CSV HEADER NULL '';\n\nSELECT * FROM ADMISSIONS;\n‘du’"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-tidyeval.html",
    "href": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-tidyeval.html",
    "title": "Data Apothecary's Notes",
    "section": "",
    "text": "Tidy evaluation, one of the metaprogramming techniques\n\nA special type of non-standard evaluation (metaprogramming)\nThe dataframe itself becomes a temporary workspace (meaning that can access column using col instead of df$col)\n\ndata masking\nwrite my_variable not df$my_variable\n\n\ntidy selection\nchoose variables easily based on position, name, type, such as start_with('x')\n\n\nvectorisation\na function is vectorised when it returns a vector as long as the input, and when applying the function on a scalar is the same as doing it on the vector.\nfn(x[[i]]) == fn(x)[[i]]"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/R cpp notes.html",
    "href": "dev/oldnotes_programming/r_advanced_r_book/R cpp notes.html",
    "title": "R cpp notes",
    "section": "",
    "text": "run an argument many times, then print out the summary\n\n\n\ncheck storage mode: storage.mode(x)\nconvert an integer to a double by adding .0 after the number. can also use casting to change data types:\nevalCpp('(double)(40+2)')\n\n\n\nneed to define the type of input and return object\ndouble functionName(double x, double y){\n    \n    double res = sqrt(x*x + y*y); // body of the function\n  return res;\n}\n\n\n\nRprintf() prints formatted output. inside the message, can only use double quotes\n\n\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n//[[Rcpp::export]]\nint timesTwo(inx){\n  return 2*x ;\n}\nLoad into R\nlibrary(Rcpp)\nsourceCpp('code.cpp')\n\n# then call it like any other R function \ntimesTwo(21)\n\n\nif(condition){\n//\n}else{\n//\n}\n\n\n\nfor(init; condition; increment){\n    body\n}\ntypically (note i starts from 0)\nfor (init i = 0; i&lt;n; i++){\n\n}"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/R cpp notes.html#scripting",
    "href": "dev/oldnotes_programming/r_advanced_r_book/R cpp notes.html#scripting",
    "title": "R cpp notes",
    "section": "",
    "text": "#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n//[[Rcpp::export]]\nint timesTwo(inx){\n  return 2*x ;\n}\nLoad into R\nlibrary(Rcpp)\nsourceCpp('code.cpp')\n\n# then call it like any other R function \ntimesTwo(21)\n\n\nif(condition){\n//\n}else{\n//\n}\n\n\n\nfor(init; condition; increment){\n    body\n}\ntypically (note i starts from 0)\nfor (init i = 0; i&lt;n; i++){\n\n}"
  },
  {
    "objectID": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt2-fp.html",
    "href": "dev/oldnotes_programming/r_advanced_r_book/advanced-R-chpt2-fp.html",
    "title": "Functional",
    "section": "",
    "text": "Functional programming in R\n\nFunctional programming is complementatry to OOP\n\nfunctional language have first-class functions. functions behave like other data structure; assign to variables, store in lists, pass as argument to other functions, create inside functions, return as the result of a function.\nrequires pure functions: same input, same output. This means functions such as runif(), Sys.time() are not pure functions.\n\n\nFunctional\nA function that takes a function as an input, and returns a vector as output.\n\nMap\nI already know how to use it\n\n\nReduce, accumulate\nreduce is like f(f(f(x))).\nlst &lt;- map(1:4, ~ sample(1:10, 15, replace = T))\nlst\n# find the values that occur in every element \nz &lt;- lst[[1]]\nz &lt;- intersect(z, lst[[2]])\nz &lt;- intersect(z, lst[[3]])\nz &lt;- intersect(z, lst[[4]])\nz\n# can use: \nreduce(lst, intersect)\n\n# accumulate shows all the intermediate results \naccumulate(lst, intersect)\n\n\n\nFunction operator\nA function that takes a function as input, and returns a function as output.\nCan be thought of as a wrapper. I think it can provide alternative ways to deal with error, instead of tryCatch().\nUseful way to debug when using map suite: purrr::safely(function) and purrr::transpose(list)\nx &lt;- list(c(0.1, 0.1, 0.2),\n          c(0.3, 0.4, 0.5), \n          c(1, 0.2, 0.2), \n          'error')\n\n# use a FUNCTIONAL\nmap_dbl(x, sum)  # error will prevent any results from printing \n\n# use safely(), a FUNCTION OPERATOR with a functional \n# map(x, safe_sum). alternatively, \nout2 &lt;- map(x, safely(sum))\nmap(out2, pluck(1))   # results\nmap(out2, pluck(2))   # errors\n\n\n# inconvenient way of printing: use purrr::transpose \n# equivalent to transpose(out2)\nout3 &lt;- transpose(map(x, safely(sum)))\nout3$result\nout3$error\nWith purrr::possibly(function), it replaces error into a value\nl &lt;- list('a', 10, 100)\nmap_dbl(l, possibly(log, NA))  # need 'otherwise' argument"
  },
  {
    "objectID": "dev/oldnotes_programming/unix/Bash scripting.html",
    "href": "dev/oldnotes_programming/unix/Bash scripting.html",
    "title": "Bash scripting",
    "section": "",
    "text": "the content of headers.sh is head -n 1 seasonal/*.csv\nRun by\nbash headers.sh\nwith pipe: cut -d , -f 1 seasonal/*.csv | grep -v Date | sort | uniq is the all-dates.sh\nbash all-dates.sh &gt; dates.out   # save in the output \n\n\n$@ : all the command-line parameters given to the script\neg. sort $@ | uniq is the unique-lines.sh. then the following will process 2 files, replacing $@.\nbash unique-lines.sh seasonal/summer.csv seasonal/autumn.csv\n\n\n\n# Print the first and last data records of each file.\n# no semi colon\n\nfor filename in $@\ndo\n    head -n 2 $filename | tail -n 1\n    tail -n 1 $filename\ndone\n\n\n\nAn example script: script1.sh\n#!/bin/bash\n# a simple bash script \n\necho Hello World! \nLine 1: shebang\nfollowed by path to the interpreter to run. no space, always first line.\nLine 2: comment\nLine 3: command\n\n\nandrea$ ./script1.sh\n-bash: ./script1.sh: Permission denied  # no space in between \n# dot is the current dir, otherwise have to specify $PATH. \n# equivalent to \nandrea$ bash script1.sh\n\nandrea$ ls -l script1.sh\n-rw-r--r--@ 1 andrea  staff  55 Apr 24 13:23 script1.sh\n\n# allow owner to write and modify, everyone to execute\nandrea$ chmod 755 script1.sh    # change access permission\nandrea$ ls -l script1.sh\n-rwxr-xr-x@ 1 andrea  staff  55 Apr 24 13:23 script1.sh\n\n\n\nrefering to / reading a variable: put a $ before variable name\nsetting a variable, no $. For example, andrea$ HOSTNAME will give host name\nSeems like I can only use $1, $2 but not $A, or $arg1. This is because these are the special variables.\n$0 # name of Bash script \n$1-9 # first 9 arguments in Bash script \n$@ # all arguments \n$USER # username \n$HOSTNAME # hostname of the machine the script is running on \nexample see script2.sh\n\n\nsee script3.sh.\n\n\n\nsingle quotes does not allow substition of variables, it treats characters literally: when use a $ it is printed exactly like it is.\nvar1='hello world'\n\nvar2='see $var1' \necho $var2  # will give see $var1\nvar3=\"see $var1\" \necho $var3 # will give see hello world\n\n\n\nvar=$(ls | wc -l)  # count how many elements\necho There are $var entries in this directory  \n\n\n\nsee scripts 4, 4-2.\n\n\n\n\n(script5.sh) use the command read and save the user response into the variable varname\n\n\n\n(script6.sh)\nlet and expr.\n\n\n\n\nif [&lt;some test&gt;]  # if_[_cmd_]  the space matters!\nthen\n    &lt;commands&gt;\nfi\nwhen checking,\ntest 001 = 1  # false, string comparison\necho $?\n\ntest 001 -eq 1  # true, numerical comparison\necho $?\nNote that $? returns the last run process (exit status).\n0 means 0 error (true), other values indicate some unusual condition (e.g. 1 means false in the above situation)\n\n\nif [&lt;some test&gt;]  \nthen\n    &lt;commands&gt;\nelif [&lt;some test&gt;]  \nthen \n    &lt;different commands&gt;\nelse \n    &lt;other commands&gt;\nfi\n\n\n\nwhile [&lt;some test&gt;]  \ndo\n    &lt;commands&gt;\ndone\n\n\n\nvery similar to while.\nuntil [&lt;some test&gt;]  \ndo\n    &lt;commands&gt;\ndone\n\n\n\nfunction_name(){\n    &lt;commands&gt;\n}\n\n# or \n\nfunction function_name {\n    &lt;commands&gt;\n}"
  },
  {
    "objectID": "dev/oldnotes_programming/unix/Bash scripting.html#another-tutorial",
    "href": "dev/oldnotes_programming/unix/Bash scripting.html#another-tutorial",
    "title": "Bash scripting",
    "section": "",
    "text": "An example script: script1.sh\n#!/bin/bash\n# a simple bash script \n\necho Hello World! \nLine 1: shebang\nfollowed by path to the interpreter to run. no space, always first line.\nLine 2: comment\nLine 3: command\n\n\nandrea$ ./script1.sh\n-bash: ./script1.sh: Permission denied  # no space in between \n# dot is the current dir, otherwise have to specify $PATH. \n# equivalent to \nandrea$ bash script1.sh\n\nandrea$ ls -l script1.sh\n-rw-r--r--@ 1 andrea  staff  55 Apr 24 13:23 script1.sh\n\n# allow owner to write and modify, everyone to execute\nandrea$ chmod 755 script1.sh    # change access permission\nandrea$ ls -l script1.sh\n-rwxr-xr-x@ 1 andrea  staff  55 Apr 24 13:23 script1.sh\n\n\n\nrefering to / reading a variable: put a $ before variable name\nsetting a variable, no $. For example, andrea$ HOSTNAME will give host name\nSeems like I can only use $1, $2 but not $A, or $arg1. This is because these are the special variables.\n$0 # name of Bash script \n$1-9 # first 9 arguments in Bash script \n$@ # all arguments \n$USER # username \n$HOSTNAME # hostname of the machine the script is running on \nexample see script2.sh\n\n\nsee script3.sh.\n\n\n\nsingle quotes does not allow substition of variables, it treats characters literally: when use a $ it is printed exactly like it is.\nvar1='hello world'\n\nvar2='see $var1' \necho $var2  # will give see $var1\nvar3=\"see $var1\" \necho $var3 # will give see hello world\n\n\n\nvar=$(ls | wc -l)  # count how many elements\necho There are $var entries in this directory  \n\n\n\nsee scripts 4, 4-2.\n\n\n\n\n(script5.sh) use the command read and save the user response into the variable varname\n\n\n\n(script6.sh)\nlet and expr."
  },
  {
    "objectID": "dev/oldnotes_programming/unix/Bash scripting.html#if-statements-loops-functions",
    "href": "dev/oldnotes_programming/unix/Bash scripting.html#if-statements-loops-functions",
    "title": "Bash scripting",
    "section": "",
    "text": "if [&lt;some test&gt;]  # if_[_cmd_]  the space matters!\nthen\n    &lt;commands&gt;\nfi\nwhen checking,\ntest 001 = 1  # false, string comparison\necho $?\n\ntest 001 -eq 1  # true, numerical comparison\necho $?\nNote that $? returns the last run process (exit status).\n0 means 0 error (true), other values indicate some unusual condition (e.g. 1 means false in the above situation)\n\n\nif [&lt;some test&gt;]  \nthen\n    &lt;commands&gt;\nelif [&lt;some test&gt;]  \nthen \n    &lt;different commands&gt;\nelse \n    &lt;other commands&gt;\nfi\n\n\n\nwhile [&lt;some test&gt;]  \ndo\n    &lt;commands&gt;\ndone\n\n\n\nvery similar to while.\nuntil [&lt;some test&gt;]  \ndo\n    &lt;commands&gt;\ndone\n\n\n\nfunction_name(){\n    &lt;commands&gt;\n}\n\n# or \n\nfunction function_name {\n    &lt;commands&gt;\n}"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/project_template.html",
    "href": "dev/oldnotes_programming/rstudio_extension/project_template.html",
    "title": "Parts needed",
    "section": "",
    "text": "template function\ntemplate metadata\n\n\n\nOnly need one function\n\n\n\nthis file defines what’s shown on the project wizard\n\n\nBinding: which function to generate the project\nTitle: title in thhe project wizard\nOpenFiles:"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/project_template.html#template-function",
    "href": "dev/oldnotes_programming/rstudio_extension/project_template.html#template-function",
    "title": "Parts needed",
    "section": "",
    "text": "Only need one function"
  },
  {
    "objectID": "dev/oldnotes_programming/rstudio_extension/project_template.html#template-metadata-.dcf",
    "href": "dev/oldnotes_programming/rstudio_extension/project_template.html#template-metadata-.dcf",
    "title": "Parts needed",
    "section": "",
    "text": "this file defines what’s shown on the project wizard\n\n\nBinding: which function to generate the project\nTitle: title in thhe project wizard\nOpenFiles:"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git for Version Control.html",
    "href": "dev/oldnotes_programming/git/Git for Version Control.html",
    "title": "Git for Version Control",
    "section": "",
    "text": "workflow http://marklodato.github.io/visual-git-guide/index-en.html\nhttps://swcarpentry.github.io/git-novice/02-setup/index.html\ngit --version check version\nmkdir planets\ncd planets\ngit init\ngit config user.name 'Vlad Dracula' \nls -a  # list all \nls -la # list long format\n\ncat .git/config  # shows name Vlad Dracula\ngit config core.editor 'nano -w'  # set up editor, can also use atom\ngit status # should be nothing there\n\n# -- inside .git folder to change user name etc\ncd .git\ncat config\ngit config user.name 'name1'\n\n\nremember to set the global username and email! otherwise your contributions are not counted.\n$ git config --global user.name 'andrea'\n$ git config --global user.email 'andreachizhang@yahoo.com'\nnano mars.txt # create some content\ncat mars.txt # display \ngit status # check status again\n\ngit add mars.txt\ngit status # check status again\ngit commit -m 'satr notes on Mars as a base'  # message\ngit log  # display log \n\n# change someting in mars, then check status again\n\ngit diff # check difference \n\ngit add mars.txt\ngit commit -m 'add concerns about effects of Mars moon'\n\ngit add mars.txt\ngit diff  # compares working area with staging area, disappears after git add, before commit\ngit diff --staged  # comparing staging area with repositories\ngit diff HEAD # working area vs repositories \nif I have a separate file called mars2.txt, and I do git diff, nothing will happen until I do git add.\nIt’s possible that changes in both files are displayed at the same time, can be seen using git status and git diff, and git add . will update both.\nIf I happen to have deleted something accidently\ngit checkout 8dcecd668a9ae311aae0512ebe97f501595fe16b mars.txt\ngit checkout master\ntouch a.input b.input\nls\nnano .gitignore # and put a.input\ngit status # a.input will be ignored \nssh doesn’t need user name and pass, html needs\npwd  # should be planets repo (local), /Users/andrea/Desktop/planets\ngit remote -v # check if any server is connected\ngit remote add origin https://github.com/yymmhaha/planets.git  # origin is the name for https + blahblah, easier for later so we don't need to type again\ngit push origin master  # sometimes it asks for user and pass \n\ngit log --oneline\ngit diff c7c2eea..f88029b mars2.txt\ncollaborate, clone other’s repo to my desktop but another folder\ngit clone https://github.com/tinavisnovska/planets.git ~/Desktop/tina-planets\ngit remote -v  # now I should see my collaborator's name \n# change something then, add and commit \ngit push origin master\n\n# ------- pull to my own \ncd ~/Desktop/planets\ngit pull origin master\natom --wait mars.txt  # use atom to open \n\n\n\ncd Documents/Programming/R/Project-git\ngit init\ngit remote add origin https://github.com/yymmhaha/Paper.git  # unneeded if already \ngit pull origin master --allow-unrelated-histories\ngit rm \"Worklog - Thoughts and undone's.md\"  # remove something - has to use git before!! \ngit commit -m 'deleted thoughts and undones since unnecessary'\n\n\nProgress andrea$ git add 'Part 4- intervention-practicals.md'\nProgress andrea$ git commit -m 'added AB testing'\nProgress andrea$ git push\nwhen there is a conflict asking me to enter commit message to explain merge:\n# in vim\ni\n# write merge message \nesc\n:wq\n# then enter\nto be more specific, ESC and colon : makes the cursor go to bottom. w is write, q is quit.\n\n\n\nbefore committing, can use .gitignore to specify files unneeded. Do it before add anything. Add these lines in the file to ignore the hidden directories.\n.*\n!/.gitignore\nunstaged, use\n$ git checkout -- &lt;file&gt;\n$ git checkout '*.DS_Store'\nto discard changes in working directory.\nor\n$ git reset HEAD\n\n\n\n$ wget --no-check-certificate --content-disposition https://github.com/MIT-LCP/mimic-code/blob/master/concepts/sepsis/angus.sql\nor\n$ curl -LJO https://github.com/MIT-LCP/mimic-code/blob/master/concepts/sepsis/angus.sql\n\n\n\nWhat is a pull request?\nIt lets you tell others about changes you’ve pushed.\nFork\nEdit\nthen pull request\n\n\n\ngit checkout is switching between branches in a repo you have, for example if you’re on master branch and want to go to develop_branch, git checkout develop_branch.\ngit clone is fetching repo you don’t have\n\n\n\nFor the project, I have re-set up the github account on the pink computer. Then, go to the parent directory (‘Work’) and do\ngit clone ### then add the .git file that appears \nAfter any changes made in the pink computer, push as usual.\nIn this computer, git pull. This will make sure the changes are synced.\n\n\n(on the main computer) first try git pull\nIt will say there are some files that are untracked, and modified locally.\nModified locally: force over write (i.e. ignore all local changes) by\ngit reset --hard\ngit pull\nUntracked:\ngit clean -i\nc # for clean\nIn the end pull again."
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git for Version Control.html#quick-guide-for-me",
    "href": "dev/oldnotes_programming/git/Git for Version Control.html#quick-guide-for-me",
    "title": "Git for Version Control",
    "section": "",
    "text": "cd Documents/Programming/R/Project-git\ngit init\ngit remote add origin https://github.com/yymmhaha/Paper.git  # unneeded if already \ngit pull origin master --allow-unrelated-histories\ngit rm \"Worklog - Thoughts and undone's.md\"  # remove something - has to use git before!! \ngit commit -m 'deleted thoughts and undones since unnecessary'\n\n\nProgress andrea$ git add 'Part 4- intervention-practicals.md'\nProgress andrea$ git commit -m 'added AB testing'\nProgress andrea$ git push\nwhen there is a conflict asking me to enter commit message to explain merge:\n# in vim\ni\n# write merge message \nesc\n:wq\n# then enter\nto be more specific, ESC and colon : makes the cursor go to bottom. w is write, q is quit."
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git for Version Control.html#ignore",
    "href": "dev/oldnotes_programming/git/Git for Version Control.html#ignore",
    "title": "Git for Version Control",
    "section": "",
    "text": "before committing, can use .gitignore to specify files unneeded. Do it before add anything. Add these lines in the file to ignore the hidden directories.\n.*\n!/.gitignore\nunstaged, use\n$ git checkout -- &lt;file&gt;\n$ git checkout '*.DS_Store'\nto discard changes in working directory.\nor\n$ git reset HEAD"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git for Version Control.html#download-online-files-from-github",
    "href": "dev/oldnotes_programming/git/Git for Version Control.html#download-online-files-from-github",
    "title": "Git for Version Control",
    "section": "",
    "text": "$ wget --no-check-certificate --content-disposition https://github.com/MIT-LCP/mimic-code/blob/master/concepts/sepsis/angus.sql\nor\n$ curl -LJO https://github.com/MIT-LCP/mimic-code/blob/master/concepts/sepsis/angus.sql"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git for Version Control.html#fork-and-pull-request",
    "href": "dev/oldnotes_programming/git/Git for Version Control.html#fork-and-pull-request",
    "title": "Git for Version Control",
    "section": "",
    "text": "What is a pull request?\nIt lets you tell others about changes you’ve pushed.\nFork\nEdit\nthen pull request"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git for Version Control.html#checkout",
    "href": "dev/oldnotes_programming/git/Git for Version Control.html#checkout",
    "title": "Git for Version Control",
    "section": "",
    "text": "git checkout is switching between branches in a repo you have, for example if you’re on master branch and want to go to develop_branch, git checkout develop_branch.\ngit clone is fetching repo you don’t have"
  },
  {
    "objectID": "dev/oldnotes_programming/git/Git for Version Control.html#using-another-computer",
    "href": "dev/oldnotes_programming/git/Git for Version Control.html#using-another-computer",
    "title": "Git for Version Control",
    "section": "",
    "text": "For the project, I have re-set up the github account on the pink computer. Then, go to the parent directory (‘Work’) and do\ngit clone ### then add the .git file that appears \nAfter any changes made in the pink computer, push as usual.\nIn this computer, git pull. This will make sure the changes are synced.\n\n\n(on the main computer) first try git pull\nIt will say there are some files that are untracked, and modified locally.\nModified locally: force over write (i.e. ignore all local changes) by\ngit reset --hard\ngit pull\nUntracked:\ngit clean -i\nc # for clean\nIn the end pull again."
  },
  {
    "objectID": "models/mixed_models.html#overview",
    "href": "models/mixed_models.html#overview",
    "title": "Mixed models for repeted measurements",
    "section": "",
    "text": "MMRM has one distinct feature compared to other linear mixed models: subject-specific random effects are considered as residual effects (part of error correlation matrix)."
  },
  {
    "objectID": "programming/misc_software_papers.html",
    "href": "programming/misc_software_papers.html",
    "title": "Software papers",
    "section": "",
    "text": "Before writing a software paper, one needs to create a worthy software."
  },
  {
    "objectID": "programming/misc_software_papers.html#select-a-journal",
    "href": "programming/misc_software_papers.html#select-a-journal",
    "title": "Software papers",
    "section": "Select a journal",
    "text": "Select a journal\n\nR journal\nJournal of Open Source Software\n\n\nR journal\nTo create a template, use rjtools::create_article(). This compiles both a pdf and html.\nThe article should have the following folders\n\ndata\nfigures\nscripts\nmotivation-letter\n\n\nTroubleshooting\nWhen the error is related to missing tex package:\n\nin terminal, tlmgr install texpkgname"
  },
  {
    "objectID": "programming/r_pkg_dependency.html",
    "href": "programming/r_pkg_dependency.html",
    "title": "Dependencies",
    "section": "",
    "text": "Resources:"
  },
  {
    "objectID": "programming/r_pkg_dependency.html#overview-namespace-and-import",
    "href": "programming/r_pkg_dependency.html#overview-namespace-and-import",
    "title": "Dependencies",
    "section": "Overview: NAMESPACE and import",
    "text": "Overview: NAMESPACE and import\n\nError: could not find function xyz\nWhere the function xyz is from a dependency package, such as ggplot2.\n\ncould not find function “ggplot”\n\nThis happens when dependency package such as ggplot2 is not imported. When checking NAMESPACE, you see that this package is not in the import list.\nTo tackle this, locate the function where the error occurs, add the Roxygen2 tags as appropriate:\n\n#' @importFrom aaapkg aaa_fun\n#' @import bbbpkg\n#' @export\nmy_function &lt;- function(x,y){\n  ...\n}\n\nThen document (either devtools::document() or Build -&gt; Document button). Check NAMESPACE again, see if the package is imported."
  },
  {
    "objectID": "programming/r_pkg_dependency.html#packages-in-imports-and-depends",
    "href": "programming/r_pkg_dependency.html#packages-in-imports-and-depends",
    "title": "Dependencies",
    "section": "Packages in Imports and Depends",
    "text": "Packages in Imports and Depends\nInside DESCRIPTION, there are three categories of dependencies: imports, suggests and depends. For now focus on imports and depends.\nGenerally, three ways to call a function in a dependency:\n\nuse deppkg::function();\nuse Roxygen2 tag @importFrom deppkg function, then call this particular function directly;\nuse Roxygen2 tag @import deppkg and call anything directly.\n\n\nImports\nPackages are required to make functions run, but are NOT loaded when loading your own package.\nWhen a pkg is listed in DESCRIPTION under Imports: recommended to call with deppkg::function() syntax.\n\nmy_function &lt;- function(x,y){\n  z &lt;- dplyr::select(...)\n}\n\nAvoid importing anything with @import tag, so that it is easier to understand what function is not local.\n\n\nDepends\nVery similar to Imports, but loaded when your package is loaded.\nHowever, even after putting pkg (such as ggplot2) under Depends, you still need to call the functions in a correct way (the three options above)."
  },
  {
    "objectID": "programming/r_pkg_dependency.html#common-error-warning-and-notes",
    "href": "programming/r_pkg_dependency.html#common-error-warning-and-notes",
    "title": "Dependencies",
    "section": "Common error, warning and notes",
    "text": "Common error, warning and notes\n\nError: could not find function xyz\nWhere the function xyz is from a dependency package, such as ggplot2.\n\ncould not find function “ggplot”\n\nThis happens when dependency package such as ggplot2 is not imported. When checking NAMESPACE, you see that this package is not in the import list.\nTo tackle this, locate the function where the error occurs, add the Roxygen2 tags as appropriate:\n\n#' @importFrom aaapkg aaa_fun\n#' @import bbbpkg\n#' @export\nmy_function &lt;- function(x,y){\n  ...\n}\n\nThen document (either devtools::document() or Build -&gt; Document button). Check NAMESPACE again, see if the package is imported.\n\n\nNote: all declared imports should be used\n\nNamespaces in Imports field not imported from: ‘data.table’ ‘gt’ ‘gtExtras’ ‘magrittr’ ‘rlang’ All declared Imports should be used.\n\nThis might happen when these external packages are not used. I can be more explicit when I actually use these packages somewhere. See 11.4.1 of the book for more information."
  },
  {
    "objectID": "inference/intro_causal.html",
    "href": "inference/intro_causal.html",
    "title": "Intro to Causal Effects",
    "section": "",
    "text": "Smoking example\n\ndoes smoking cause lung cancer?\ndoes lung cancer cause people to smoke?\nis there a third factor that causes both smoking and lung cancer?\n\n\n\n\n\n\n\n\n\nData generation\nCorrect causal model\nCorrect causal effect\n\n\n\n\nCollider\nY ~ X\n1\n\n\nConfounder\nY ~ X; Z\n0.5\n\n\nMediator\nDirect effect: Y ~ X; Z. Total effect: Y ~ X\nDirect: 0; total: 1\n\n\nM-Bias\nY ~ X\n1"
  },
  {
    "objectID": "inference/rwd_rwe.html#use-cases-for-rwd",
    "href": "inference/rwd_rwe.html#use-cases-for-rwd",
    "title": "Real-world Data, Real-world Evidence",
    "section": "Use cases for RWD",
    "text": "Use cases for RWD\nA few examples\n\ncharacerise health conditions, interventions, care pathways and patient outcomes\npatient-reported outcomes, quality of life\nestimate economic burden\nestimate test accuracy or reproducibility of biomarker test results\n\nRather than using RWD to replace RCT, there are a few ways to improve RCT in smaller populations. See Wieseler 2023\n\nRWD in oncology\nChallenging to incorporate RWD in regulatory evidence, treatment decisions and efficacy results are dependent on clinical characteristics that are not normally observed in RWD sources:\n\ndisease staging\nperformance status\nmutation tests\n…"
  },
  {
    "objectID": "inference/rwd_rwe.html#methods",
    "href": "inference/rwd_rwe.html#methods",
    "title": "Real-world Data, Real-world Evidence",
    "section": "Methods",
    "text": "Methods\nPropensity score matching (PSM) to reproduce the effects of randomization\nHow RWD is used\n\ncombined with non-randomized single-arm trial: as external RWD control\ntarget trial emulation from RWD"
  },
  {
    "objectID": "inference/propensity_score.html",
    "href": "inference/propensity_score.html",
    "title": "Propensity score",
    "section": "",
    "text": "Propensity score\n(Rosenbaum and Rubin, 1983)\nFit a logistic regression:\nPredict the values (probability), they are the propensity scores.\nPS can also be estimated using other methods that produce probabilities, not just logistic regression: random forest, lasso logistic regression etc."
  },
  {
    "objectID": "inference/overview.html#estimands",
    "href": "inference/overview.html#estimands",
    "title": "Overview: causal inference",
    "section": "Estimands",
    "text": "Estimands\nGreifer, N., & Stuart, E. A. (2021). Choosing the estimand when matching or weighting in observational studies. arXiv preprint arXiv:2106.10577.\n\n\n\n\n\nATE: average treatment effect in the population\n\\(E[Y(1) - Y(0)]\\)\nATT: average treatment effect among the treated\n\\(E[Y(1) - Y(0) | Z = 1]\\)\nATC: average treatment effect among the controls\n\\(E[Y(1) - Y(0) | Z = 0]\\)\nATM: average treatment effect among the matched"
  },
  {
    "objectID": "inference/overview.html#effects",
    "href": "inference/overview.html#effects",
    "title": "Overview: causal inference",
    "section": "",
    "text": "\\(Y^{a=1}, Y^{a=0}\\) are the potential outcomes under treatment 1 and 0. They are random variables. Treatment A has causal effect if \\(Y^{a=1} \\neq Y^{a=0}\\).\nFor individual \\(i\\), \\(Y_i^{a=1}, Y_i^{a=0}\\) are deterministic.\nIn reality we do not observe both potential outcomes for an individual, since we only have ONE outcome. We observe \\(Y\\) and \\(A\\). For a population, average treatment effect (ATE) can be estimate"
  },
  {
    "objectID": "inference/overview.html#graphical-reppresentation",
    "href": "inference/overview.html#graphical-reppresentation",
    "title": "Overview: causal inference",
    "section": "Graphical reppresentation",
    "text": "Graphical reppresentation\n\n\n\n\n\n\n\n\nData generation\nCorrect causal model\nCorrect causal effect\n\n\n\n\nCollider\nY ~ X\n1\n\n\nConfounder\nY ~ X; Z\n0.5\n\n\nMediator\nDirect effect: Y ~ X; Z. Total effect: Y ~ X\nDirect: 0; total: 1\n\n\nM-Bias\nY ~ X\n1"
  },
  {
    "objectID": "inference/case_study_liggetid_1.html",
    "href": "inference/case_study_liggetid_1.html",
    "title": "Case study: length of hospital stay",
    "section": "",
    "text": "The data liggetid was collected at the Geriatric Department at Ullevål Sykehus. This dataset has been used for teaching at University of Oslo, MF9130E. The course material can be accessed here.\nWe will focus on the following variables:\nlos &lt;- readRDS('data/los.rds')\nhead(los, 3)\n\n  admission_year age    sex admission_from stroke los\n1           1987  81 female           home      0  13\n2           1987  96 female           home      0  17\n3           1987  79 female           home      0   6\nSome variables need to be recoded and put into factor to display nicely.\nShow the code\n# library(dplyr)\nlibrary(ggplot2)\n\n# remove NA\nlos &lt;- dplyr::filter(los, !is.na(sex) & !is.na(stroke) & !is.na(admission_from))\n\n# code admission from with text\n# unique(los$admission_from)\nlos$admission_from &lt;- factor(los$admission_from, \n                            levels = c('home', 'div_surgery', \n                                       'div_medicine', 'div_other', \n                                       'other_hospital', 'nursing_home'), \n                            labels = c('home', 'div_surgery', \n                                       'div_medicine', 'div_other', \n                                       'other_hospital', 'nursing_home'))\n\n# code admission year with text\nlos$admission_year &lt;- factor(los$admission_year,\n                            levels = c(1981:1987),\n                            labels = as.character(1981:1987))\n\nlos$stroke &lt;- factor(los$stroke, \n                          levels = c(0, 1), \n                          labels = c('no','yes'))"
  },
  {
    "objectID": "dev/internal_notes/proposal_ab_los.html",
    "href": "dev/internal_notes/proposal_ab_los.html",
    "title": "Analysis + Simulation",
    "section": "",
    "text": "Investigate antibiotics effect on length of hospital stay\nAll AB combined together\n\nAnalysis + Simulation\n\nWithout causal (can use liggetid data to develop)\n\nsurvival curve comparison\ncox model adjusting age, sex\nmixed model, with department as levels\nmissing data imputation\n\n\n\nCausal\n\nPS matching\nPS weighting\n\ncompare different techniques impact on the estimates\nsensitivity analysis\nLiterarature\nhttps://www.fda.gov/drugs/regulatory-science-action/exploiting-real-world-data-optimize-use-antibiotics\nhttps://www.nature.com/articles/s41598-021-86853-4\nhttps://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-022-02027-w"
  },
  {
    "objectID": "inference/mixed_models.html",
    "href": "inference/mixed_models.html",
    "title": "Mixed models for repeted measurements",
    "section": "",
    "text": "Resources:\n\nmmrm package vignette\nMixed models with R\nGLMM FAQ\n\n\nMixed modeling in R\n\nLinear mixed models: nlme::lme, lme4::mner, brms::brm\ngeneralized linear mixed models (GLMM): lme4::glmer, glmmTMB; brms::brm for Bayesian\nnonlinear mixed models: nlme::nlme, lme4::nlmer; brms::brm\n\n\n\n\n\n\n\n\n\nequation\nformula\nmeaning\n\n\n\n\n\\(B_0 + B_1X_i + e_i\\)\n\nno random effect\n\n\n\\((B_0+b_{g,0} + B_1X_i + e_i)\\)\nx+(1|group)\nrandom group intercept\n\n\n\n(x|group)\nrandom slopt of x within group, with correlated intercept\n\n\n\n(1+x|group)\n\n\n\n\n(0+x | group)\nrandom slop of x within group, no variation in intercept\n\n\n\n(-1+x | group)\n\n\n\n\n(1| group) + (0+x|group)\nuncorrelated random intercept and random slopt within group\n\n\n\n\n\nRandom intercept\nEach group has different intercelpt, but the slope is the same\n\\[gpa = (b_0 + effect_{student} + b_{occasion} * occasion + e)\\]\n\n\nMMRM\nMMRM has one distinct feature compared to other linear mixed models: subject-specific random effects are considered as residual effects (part of error correlation matrix).\n\nMethodology\nBasic linear mixed-effects model for a single level of grouping\n\\[\ny_i = X_i \\beta + Z_i b_i + \\epsilon_i, i = 1, ..., n\n\\] \\[\nb_i \\sim N(0, \\Psi), \\epsilon \\sim N(0, \\sigma^2 I)\n\\]\n\n\\(\\beta\\) is p-dim vector of fixed effects\n\\(b\\) is q-dim vecor of random patient specific effects\n\\(X_i\\) of size \\(n_i \\times p\\) and \\(Z_i\\) of size \\(n_i \\times q\\) are regressor matrices relating observations to the fixed effects and random effects.\n\\(\\epsilon_i\\) is \\(n_i\\)-dimensional within-subject error"
  },
  {
    "objectID": "inference/propensity_score.html#matching",
    "href": "inference/propensity_score.html#matching",
    "title": "Propensity score",
    "section": "Matching",
    "text": "Matching\nMatchIt package\nPS matching is ONE of the many matching techniques that uses PS as the difference."
  },
  {
    "objectID": "inference/propensity_score.html#weighting",
    "href": "inference/propensity_score.html#weighting",
    "title": "Propensity score",
    "section": "Weighting",
    "text": "Weighting\nUsed in weighting\nATE (population) \\(w_{ATE} = \\frac{Z_i}{p_i} + \\frac{1-Z_i}{1-p_i}\\)\nATT \\(w_{ATT} = \\frac{p_i Z_i}{p_i} + \\frac{p_i(1-Z_i)}{1-p_i}\\)\nATC \\(w_{ATC} = \\frac{(1 - p_i) Z_i}{p_i} + \\frac{(1 - p_i)(1-Z_i)}{1-p_i}\\)"
  },
  {
    "objectID": "inference/survival.html",
    "href": "inference/survival.html",
    "title": "Survival",
    "section": "",
    "text": "Links\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html\nhttps://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/\nhttps://www.danieldsjoberg.com/ggsurvfit/\nhttps://www.coursera.org/learn/survival-analysis-r-public-health\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html\nhttps://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/\njmpost: combines survival analysis, mixed effect model https://genentech.github.io/jmpost/main/"
  },
  {
    "objectID": "inference/overview.html#graphical-representation",
    "href": "inference/overview.html#graphical-representation",
    "title": "Overview: causal inference",
    "section": "Graphical representation",
    "text": "Graphical representation\n\n\n\n\n\n\n\n\nData generation\nCorrect causal model\nCorrect causal effect\n\n\n\n\nCollider\nY ~ X\n1\n\n\nConfounder\nY ~ X; Z\n0.5\n\n\nMediator\nDirect effect: Y ~ X; Z. Total effect: Y ~ X\nDirect: 0; total: 1\n\n\nM-Bias\nY ~ X\n1"
  },
  {
    "objectID": "inference/overview.html#selection-bias",
    "href": "inference/overview.html#selection-bias",
    "title": "Overview: causal inference",
    "section": "Selection bias",
    "text": "Selection bias\nThis bias is the result of selecting a common effect of 2 other variables (collider): a treatment, an outcome.\n\nnon-response, missing data\nself-selection, volunteer bias\nselection affected by treatment before study started\n\nA form of lack of exchangeability between the treated and untreated.\nCorrect for selection bias: IP weighting"
  },
  {
    "objectID": "inference/index.html#inference",
    "href": "inference/index.html#inference",
    "title": "Inference and models",
    "section": "Inference",
    "text": "Inference\nNotes on causal inference and other related topics.\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nOverview: causal inference\n\n\nCollider, confounder, mediator and M-bias \n\n\n\n\nPropensity score\n\n\nPropensity score, matching, weighting \n\n\n\n\nTarget trial emulation\n\n\nTTE \n\n\n\n\nG-Computation\n\n\nG-Computation \n\n\n\n\nMissing data and imputation\n\n\nOverview of multiple imputation \n\n\n\n\nMultiple imputation in R\n\n\nMICE, regression, PMM \n\n\n\n\nIntervals\n\n\nConfidence, credible and prediction intervals \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "inference/index.html#models",
    "href": "inference/index.html#models",
    "title": "Inference and models",
    "section": "Models",
    "text": "Models\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nMixed models for repeted measurements\n\n\n\n\n\n\n\nSurvival\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "inference/index.html#case-studies",
    "href": "inference/index.html#case-studies",
    "title": "Inference and models",
    "section": "Case studies",
    "text": "Case studies\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nCase study: length of hospital stay\n\n\nPart 1: EDA \n\n\n\n\nCase study: CTN\n\n\nData challenge R/Medicine 2024 \n\n\n\n\nCase study: CTN-51\n\n\nFocusing on CTN 51 data \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "inference/g_computation.html",
    "href": "inference/g_computation.html",
    "title": "G-Computation",
    "section": "",
    "text": "G-computation\n\nFit a model for y ~ x + z, where z is all covariates\nCreate a duplicate for each level of x\nSet the value of x to a single value for each cloned dataset: x = 1 for one, x = 0 for the other\npredict\ncalculate estimate, mean(x_1) - mean(x_0)\n\nAdvantages\n\nflexible\nprecise (compared to propensity-score based methods)\nbasis of other important models (e.g. TMLE)"
  },
  {
    "objectID": "inference/case_study_liggetid_1.html#visualization",
    "href": "inference/case_study_liggetid_1.html#visualization",
    "title": "Case study: length of hospital stay",
    "section": "Visualization",
    "text": "Visualization\n\nLOS vs age and sex\n\n\nShow the code\nplt_scat2 &lt;- ggplot(data = los, \n                    mapping = aes(x = age, y = los, shape = sex, color = sex))\nplt_scat2 &lt;- plt_scat2 + geom_point(size = 2, alpha = 0.7)\n# customize\nplt_scat2 &lt;- plt_scat2 + labs(\n  x = 'Age', \n  y = 'Length of hosptial stay (days)', \n  title = 'Length of stay versus age'\n)\nplt_scat2 &lt;- plt_scat2 + theme_bw() # make white background\n# change text size\nplt_scat2 &lt;- plt_scat2 + theme(\n  axis.text = element_text(size = 12),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15)\n)\n# change color\nplt_scat2 &lt;- plt_scat2 + scale_color_brewer(palette = 'Set1')\nplt_scat2\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nlibrary(ggridges)\nplt_ridge &lt;- ggplot(data = los, \n                    mapping = aes(x = los, y = admission_year, fill = sex))\nplt_ridge &lt;- plt_ridge + geom_density_ridges(alpha = 0.6) \nplt_ridge &lt;- plt_ridge + theme_ridges()\nplt_ridge &lt;- plt_ridge + labs(\n  x = 'Length of hosptial stay (days)', \n  y = 'Admission year', \n  title = 'Length of stay in each year, for each gender'\n)\n# change color\nplt_ridge &lt;- plt_ridge + scale_fill_brewer(palette = 'Set1')\nplt_ridge\n\n\nPicking joint bandwidth of 32.7\n\n\n\n\n\n\n\n\n\n\n\nLOS vs year of admission\n\n\nShow the code\nplt_box &lt;- ggplot(data = los, \n                  mapping = aes(x = admission_year, y = los, fill = sex))\nplt_box &lt;- plt_box + geom_boxplot(outlier.size = 1)\n# plt_box &lt;- plt_box + facet_wrap( ~ sex)\nplt_box &lt;- plt_box + coord_flip()\n\n# customize\nplt_box &lt;- plt_box + theme_bw() # make white background\nplt_box &lt;- plt_box + labs(\n  x = 'Admission year', \n  y = 'Length of hosptial stay (days)', \n  title = 'Length of stay in each year, both men and women'\n)\nplt_box &lt;- plt_box + theme(\n  axis.text = element_text(size = 12),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15), \n  strip.text = element_text(size = 12)\n)\n\nplt_box &lt;- plt_box + scale_fill_brewer(palette = 'Set1')\nplt_box \n\n\n\n\n\n\n\n\n\n\n\nLOS vs types of admission\n\n\nShow the code\nplt_box2 &lt;- ggplot(data = los, \n                   mapping = aes(x = admission_year, y = los, fill = sex))\nplt_box2 &lt;- plt_box2 + geom_boxplot(outlier.size = 0.8)\nplt_box2 &lt;- plt_box2 + facet_wrap( ~ admission_from)\n\n\n# customize\nplt_box2 &lt;- plt_box2 + theme_bw() # make white background\nplt_box2 &lt;- plt_box2 + labs(\n  x = 'Admission year', \n  y = 'Length of hosptial stay (days)', \n  title = 'Length of stay in each year, each type of admission'\n)\nplt_box2 &lt;- plt_box2 + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15), \n  strip.text = element_text(size = 12), \n  axis.text.x = element_text(angle = 45) # more readable\n)\n\nplt_box2 &lt;- plt_box2 + scale_fill_brewer(palette = 'Set1')\nplt_box2"
  },
  {
    "objectID": "inference/index.html#settings",
    "href": "inference/index.html#settings",
    "title": "Inference and models",
    "section": "",
    "text": "Different settings to apply methods.\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nReal-world Data, Real-world Evidence\n\n\nRWD, RWE \n\n\n\n\nGenomics in Drug Discovery\n\n\nUse of machine learning techniques \n\n\n\n\nAntibiotics\n\n\nBackground of antimicrobial drugs and resistance \n\n\n\n\nRWD EHR Vendor Engagement\n\n\nOverview of vendor engagement \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "inference/overview_ci.html",
    "href": "inference/overview_ci.html",
    "title": "Overview: causal inference",
    "section": "",
    "text": "\\(Y^{a=1}, Y^{a=0}\\) are the potential outcomes under treatment 1 and 0. They are random variables. Treatment A has causal effect if \\(Y^{a=1} \\neq Y^{a=0}\\).\nFor individual \\(i\\), \\(Y_i^{a=1}, Y_i^{a=0}\\) are deterministic.\nIn reality we do not observe both potential outcomes for an individual, since we only have ONE outcome. We observe \\(Y\\) and \\(A\\). For a population, average treatment effect (ATE) can be estimate"
  },
  {
    "objectID": "inference/overview_ci.html#effects",
    "href": "inference/overview_ci.html#effects",
    "title": "Overview: causal inference",
    "section": "",
    "text": "\\(Y^{a=1}, Y^{a=0}\\) are the potential outcomes under treatment 1 and 0. They are random variables. Treatment A has causal effect if \\(Y^{a=1} \\neq Y^{a=0}\\).\nFor individual \\(i\\), \\(Y_i^{a=1}, Y_i^{a=0}\\) are deterministic.\nIn reality we do not observe both potential outcomes for an individual, since we only have ONE outcome. We observe \\(Y\\) and \\(A\\). For a population, average treatment effect (ATE) can be estimate"
  },
  {
    "objectID": "inference/overview_ci.html#estimands",
    "href": "inference/overview_ci.html#estimands",
    "title": "Overview: causal inference",
    "section": "Estimands",
    "text": "Estimands\nGreifer, N., & Stuart, E. A. (2021). Choosing the estimand when matching or weighting in observational studies. arXiv preprint arXiv:2106.10577.\n\n\n\n\n\nATE: average treatment effect in the population\n\\(E[Y(1) - Y(0)]\\)\nATT: average treatment effect among the treated\n\\(E[Y(1) - Y(0) | Z = 1]\\)\nATC: average treatment effect among the controls\n\\(E[Y(1) - Y(0) | Z = 0]\\)\nATM: average treatment effect among the matched"
  },
  {
    "objectID": "inference/overview_ci.html#graphical-representation",
    "href": "inference/overview_ci.html#graphical-representation",
    "title": "Overview: causal inference",
    "section": "Graphical representation",
    "text": "Graphical representation\n\n\n\n\n\n\n\n\nData generation\nCorrect causal model\nCorrect causal effect\n\n\n\n\nCollider\nY ~ X\n1\n\n\nConfounder\nY ~ X; Z\n0.5\n\n\nMediator\nDirect effect: Y ~ X; Z. Total effect: Y ~ X\nDirect: 0; total: 1\n\n\nM-Bias\nY ~ X\n1"
  },
  {
    "objectID": "inference/overview_ci.html#selection-bias",
    "href": "inference/overview_ci.html#selection-bias",
    "title": "Overview: causal inference",
    "section": "Selection bias",
    "text": "Selection bias\nThis bias is the result of selecting a common effect of 2 other variables (collider): a treatment, an outcome.\n\nnon-response, missing data\nself-selection, volunteer bias\nselection affected by treatment before study started\n\nA form of lack of exchangeability between the treated and untreated.\nCorrect for selection bias: IP weighting"
  },
  {
    "objectID": "inference/genomics.html",
    "href": "inference/genomics.html",
    "title": "Genomics in Drug Discovery",
    "section": "",
    "text": "Why do we need precision medicine? Late-stage failures cost the most, and small improvements in failure rates at early stage yield largest savings - use better targets.\nAs of 2016, 10-15% targets have genetic data; increased to 50%, expect 13-15% cost reduction.\nNot all genes are targets, and not all targets are genes.\nFTO (fat mass and obesity gene). Can search FTO in clinical trial gov website.\nOverall nearly 60% are pursued by multiple companies, 26% by more than 5 companies: once a drug is made a target, redundancy is high. But before a target is proven, high diversity and novelty."
  },
  {
    "objectID": "inference/genomics.html#machine-learning",
    "href": "inference/genomics.html#machine-learning",
    "title": "Genomics in Drug Discovery",
    "section": "Machine learning",
    "text": "Machine learning\nGenomics data alone are insufficient for therapeutic development. How they interact with other types of data such as compounds, proteins, EHR, images, texts etc need to be investigated.\nTarget discovery: identify the molecure that can be targeted by a drug to produce a therapeutic effect, such as inhibition, to block the disease process.\nTherapeutic discovery: design potent therapeutic agents to modulate the target and block disease pathway. ML can be used to predict drug response in cell lines. Drug combination screening.\nDuring clinical studies, ML can help characterize patient groups and identify eligible patients from gene expression data and EHRs.\nDuring post-market studies, mining EHR and other RWD to provid additional evidence, such as patients’ drug response given different patient characteristics.\n\nSupervised learning\nRegression and classification, e.g. \n\ndrug sensitivity prediction\ngene expression signitures that predict clinical trial success\n\n\n\nUnsupervised learning\nClustering, e.g. \n\nfeature reduction in single-cell data to identify cell types\ncell types and biomarkers from single-cell RNA data\n\nExample: drug sensitivity predictive model. Identify biomarkers and build drug sensitivity predictive models using preclinical data, then apply to patients in early-stage clinical trials. Once validated, the model can be used for patient stratification and disease indicaation selection to support clinical development of a drug.\n(Example from (Vamathevan et al. 2019))"
  },
  {
    "objectID": "inference/missing_data.html",
    "href": "inference/missing_data.html",
    "title": "Missing data",
    "section": "",
    "text": "Missing data can occur in various situations:\n\nunit non-response: individuals decline to participate\nitem non-response: some questions or measurements left out by participating individuals\nloss of follow-up: drop outs\n\nFor unit non-response and loss of follow-up, weighting the data could help. Need to know the response rate, characteristics of the non-responders and how the respondent differ.\nSolutions (to item non-response)\n\ncomplete case analysis (listwise deletion)\nsingle imputation\n\nmean imputation\nconditional imputation: make a prediction, plus minus some noise\n\nmultiple imputation: repeat single imputation M times, produce M complete datasets, use Rubin’s rules to pool the estimates.\n\n\n\nThree categories of missing mechanism. Example: disease status, level of exposure, age. Exposure is missing for some subjects.\n\nMCAR: any two individuals have the same probability of having missing value for exposure. This is unlikely.\nMAR: any two individuaals with the same disease status and age have the same chance of having the exposure missing\nMNAR: chance of having missing exposure depends on the value of exposure\n\nData could also have special missing structures: multi-level, longitudinal and repeated measurements. In RCT a specific technique reference based imputation can be applied."
  },
  {
    "objectID": "dev/oldnotes_stat/unsupervised.html",
    "href": "dev/oldnotes_stat/unsupervised.html",
    "title": "Genomics data (microarray)",
    "section": "",
    "text": "A tool to detect expression of thousands of genes at the same time\nGene chip / DNA chip: each spot has a known DNA sequence or gene\nDNA molecules act as probes to detect gene expression (transcriptome) / mRNA transcripts\ncDNA (complementary DNA) from tissue 1 are labeled red, from tissue 2 are green\nred spot means gene is expressed in tissue 1\nyellow means gene is expressed in both tissues\ngene: a region of DNA coding for mRNA encoding the amino acidiee sequence\ngene expression: process by which genetic information in DNA is transcribed into mRNA\n\n\n\nderived from cancerous growths of humans or animals\nImmortalised in lab, proliferate indefinitely within an in vitro environment\nanalysing tumor cell lines allows us to identify targets that are expressed in cancer cells but not in normal cells\n\n\n\ngene x sample (or cell line)\neach entry is the expression level of a gene in a sample.\nexpression level: a gene generate more (or less) transcripts. this is a relative measurement of the number of transcripts\ncommonly taken log. Positive, increased level of expression; negative, decreased.\ncan compare GEM from healthy and cancer patients to find diagnostic biomarkers"
  },
  {
    "objectID": "dev/oldnotes_stat/unsupervised.html#k-means",
    "href": "dev/oldnotes_stat/unsupervised.html#k-means",
    "title": "Genomics data (microarray)",
    "section": "K-means",
    "text": "K-means\nNeed to pre-specify number of clusters K\nA good clustering achieves within-cluster variation as small as possible\nWCV (Ck), measures how much data within a cluster differ from each other\n\ntypically Euclidean distance\n\nNeed to minimize the TOTAL WCV for all clusters\nalgorithm\n\nrandomly assign a number from 1 to K to each observation, as initial cluster assignment\niterate until assignment stops changing:\n\nfor each of the K clusters: compute cluster centroid\nAssign each observation to the cluster whose centroid is closest (defined by Euclidean distance)"
  },
  {
    "objectID": "dev/oldnotes_stat/unsupervised.html#hierarchical-clustering",
    "href": "dev/oldnotes_stat/unsupervised.html#hierarchical-clustering",
    "title": "Genomics data (microarray)",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nDoes not require number of clusters K (still need to choose afterwards)\nBottom-up / agglomerative: build a hierarchy from bottom up, distance-based\nLinkage (dissimilarity between pairs of clusters)\n\ncentroid is common in genomics\nsingle (closest), complete (farthest two points), average\n\nDistance metrics\n\nEuclidean\ncorrelation based\nother distances that make sense in context\n\nPractical issues\n\nScaling of variables is important\nnumber of clusters\nchoice of dissimilarity metrics and linkage\nwhich features to use"
  },
  {
    "objectID": "inference/rwd_rwe.html#training-material",
    "href": "inference/rwd_rwe.html#training-material",
    "title": "Real-world Data, Real-world Evidence",
    "section": "Training material",
    "text": "Training material\nOnline courses\n\nReal-world evidence 1: Routinely collected data for clinical research by University of Basel\nReal-world evidence 2: Pragmatic trials - study designs for real-world decision making by University of Basel\n\nKeywords\n\nroutinely collected data for randomized trials (RCD-RCT)\nmeta-research\npragmatic trial\ngeneralizability, applicability and external validity of RCT\noptimal study design to support decision-making\nReal-world evidence in Pharmacoepidemiology by LSHTM\n\nKeywords\n\nsources of error, bias and confounding\nquantitative bias analysis (QBA)\nmissing data\nconfounding by indication\n\n\nReferences\nFang 2019\nSheffield 2020\n(Jemielita et al. 2021)\n\n\nJemielita, Thomas, Linnea Widman, Claire Fox, Stina Salomonsson, Kai Li Liaw, and Andreas Pettersson. 2021. “Replication of Oncology Randomized Trial Results using Swedish Registry Real World-Data: A Feasibility Study.” Clinical Pharmacology and Therapeutics 110 (6): 1613–21. https://doi.org/10.1002/cpt.2424."
  },
  {
    "objectID": "inference/missing_data.html#overview",
    "href": "inference/missing_data.html#overview",
    "title": "Missing data",
    "section": "",
    "text": "Missing data can occur in various situations:\n\nunit non-response: individuals decline to participate\nitem non-response: some questions or measurements left out by participating individuals\nloss of follow-up: drop outs\n\nFor unit non-response and loss of follow-up, weighting the data could help. Need to know the response rate, characteristics of the non-responders and how the respondent differ.\nSolutions (to item non-response)\n\ncomplete case analysis (listwise deletion)\nsingle imputation\n\nmean imputation\nconditional imputation: make a prediction, plus minus some noise\n\nmultiple imputation: repeat single imputation M times, produce M complete datasets, use Rubin’s rules to pool the estimates.\n\n\n\nThree categories of missing mechanism. Example: disease status, level of exposure, age. Exposure is missing for some subjects.\n\nMCAR: any two individuals have the same probability of having missing value for exposure. This is unlikely.\nMAR: any two individuaals with the same disease status and age have the same chance of having the exposure missing\nMNAR: chance of having missing exposure depends on the value of exposure\n\nData could also have special missing structures: multi-level, longitudinal and repeated measurements. In RCT a specific technique reference based imputation can be applied."
  },
  {
    "objectID": "dev/side_projects/ahus_proposals.html",
    "href": "dev/side_projects/ahus_proposals.html",
    "title": "Paper 1",
    "section": "",
    "text": "Paper 1\nQuality assurance of AB use\nTarget journal: BMJ quality and safety https://qualitysafety.bmj.com\n\nInitial analysis\nAB skjema + Mv processer\nCol E-H (doctor hypothesis)\nSo far keep all types, produce wide format data with time stamps\nMerge AB categories\nLocation, time of the day\npay attention to\n\ntype\ntime consistency\nactual use (compare with process table time stamps)\nfirst Tx for infection\n\n\n\n\nKeywords (Paper 2)\nsubgroup analysis\njoint model\ncausal inference (estimand framework)\nmixed model\nRandom effect (at an individual level)?\n\n\nJoint modeling\nCombines probability distribution from linear mixed effect with random effect (longitudinal), and survival cox model (computes hazard ratio).\nhttps://pubmed.ncbi.nlm.nih.gov/34674542/"
  },
  {
    "objectID": "inference/missing_data.html#consideration",
    "href": "inference/missing_data.html#consideration",
    "title": "Missing data",
    "section": "Consideration",
    "text": "Consideration\nNeed to account for the missinng data process, preserve the relations in the data and uncertainty in the relations.\n\nMAR assumption whether plausible. (FCS can handle both MAR anad MNAR)\nform of imputation model: structure and error distribution\npredictors, as many relevant as possible, including interactions\nthe order in which to impute\nset up starting imputation and number of iteration\ndecide number of imputed datasets"
  },
  {
    "objectID": "inference/antibiotics.html",
    "href": "inference/antibiotics.html",
    "title": "Antibiotics",
    "section": "",
    "text": "Drug-resistant diseases kill 700k people (Plackett2020) yearly, however fewer new antibiotic drugs are reaching the market.\nDevelopment cost: 1.5 billion USD (for one drug, based on 2017 study) while revenue is 45 million USD.\n\nnot enough demand: physicians prescribe less, treatment cycle is short compared to chronic diseases\nprice is low\ndevelopment is difficult\n\nhttps://www.nature.com/articles/s41429-023-00629-8"
  },
  {
    "objectID": "inference/antibiotics.html#economics-and-production",
    "href": "inference/antibiotics.html#economics-and-production",
    "title": "Antibiotics",
    "section": "",
    "text": "Drug-resistant diseases kill 700k people (Plackett2020) yearly, however fewer new antibiotic drugs are reaching the market.\nDevelopment cost: 1.5 billion USD (for one drug, based on 2017 study) while revenue is 45 million USD.\n\nnot enough demand: physicians prescribe less, treatment cycle is short compared to chronic diseases\nprice is low\ndevelopment is difficult\n\nhttps://www.nature.com/articles/s41429-023-00629-8"
  },
  {
    "objectID": "inference/antibiotics.html#antibiotics-stewardship",
    "href": "inference/antibiotics.html#antibiotics-stewardship",
    "title": "Antibiotics",
    "section": "Antibiotics stewardship",
    "text": "Antibiotics stewardship\nCoursera course: Antibiotics stewardship\nAntimicrobials are the 2nd most frequently prescribed claass of pharmaceuticals. As much as 50% of antibiotic use is inappropriate.\nFive D’s: (if it is the right) drug, dose, delivery, deescalation, duration.\nImpact of inappropriate AB use: poor patient outcomes (adverse reactions, organ toxicity, AB resistance, increased mortality); excess costs (drug acquisition cost, complication management, prolonged hospital stays, costs associated with AB resistance)\n\nPrinciples\nProphylactic to prevent infection, preemptive to abort infection, empiric to provide initial control in absence of knowledge of its etiology, definitive to cure infectiou of a knownn etiology or its antimicrobial susceptibility\nEmpiric use is very common - e.g. community and hospital acquired pneumonia, sepsis.\n\n\nPKPD\nPharmacokinetics (PK): what the body does to the drug - absorption, distribution, metabolism, elimination\nPharmacodynamics (PD): what the drug does to the body / target organism - measured drug concentration and antimicrobial effect (e.g. adverse effect, safety)\nMIC: minimum inhibitory concentration, from no visible growth to 99.9% bacteria kill (MBC)\nPharmacodynamic measures\n\nCmax, AB peak concentration\nCmin, AB trough concentration\nCmax / MIC, AUC / MIC, T&gt;MIC (on concentration time curve)\n\nConcentration-dependent AB classes: large, infrequent doses\n\naminoglycosides (e.g. gentamicin, tobramycin, aminkacin for life-threatening nosocomial infections)\nfluoroquinolonse (e.g. norfloxacin, ciprofloxacin, levofloxacin)\npolymyxin\n\nTime-dependent AB classes (beta-lactam): optimize the duration\n\npenicillins\ncephalosporins\ncarbapenems\nmacrolides"
  },
  {
    "objectID": "inference/antibiotics.html#research-on-antibiotics-use",
    "href": "inference/antibiotics.html#research-on-antibiotics-use",
    "title": "Antibiotics",
    "section": "Research on antibiotics use",
    "text": "Research on antibiotics use\n(relevant to my research)\nhttps://jamanetwork.com/journals/jamanetworkopen/fullarticle/2814214\nhttps://jamanetwork.com/journals/jamanetworkopen/fullarticle/2814216\nhttps://pubmed.ncbi.nlm.nih.gov/37760690/"
  },
  {
    "objectID": "inference/imputation_2.html",
    "href": "inference/imputation_2.html",
    "title": "Multiple imputation in R",
    "section": "",
    "text": "Here I use the small dataset nhanes included in mice package. It has 25 rows, and three out of four variables have missings.\nThe original NHANES data is a large national level survey, some are publicly available via R package nhanes.\nlibrary(mice)\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n# load example dataset from mice\nhead(nhanes)\n\n  age  bmi hyp chl\n1   1   NA  NA  NA\n2   2 22.7   1 187\n3   1   NA   1 187\n4   3   NA  NA  NA\n5   1 20.4   1 113\n6   3   NA  NA 184\n\nsummary(nhanes)\n\n      age            bmi             hyp             chl       \n Min.   :1.00   Min.   :20.40   Min.   :1.000   Min.   :113.0  \n 1st Qu.:1.00   1st Qu.:22.65   1st Qu.:1.000   1st Qu.:185.0  \n Median :2.00   Median :26.75   Median :1.000   Median :187.0  \n Mean   :1.76   Mean   :26.56   Mean   :1.235   Mean   :191.4  \n 3rd Qu.:2.00   3rd Qu.:28.93   3rd Qu.:1.000   3rd Qu.:212.0  \n Max.   :3.00   Max.   :35.30   Max.   :2.000   Max.   :284.0  \n                NA's   :9       NA's   :8       NA's   :10\nExamine missing pattern with md.pattern(data).\n# 27 missing in total\n# by col: 8 for hyp, 9 for bmi, 10 for chl\n# by row: n missing numbers\n\nmd.pattern(nhanes)\n\n\n\n\n\n\n\n\n   age hyp bmi chl   \n13   1   1   1   1  0\n3    1   1   1   0  1\n1    1   1   0   1  1\n1    1   0   0   1  2\n7    1   0   0   0  3\n     0   8   9  10 27"
  },
  {
    "objectID": "inference/imputation_2.html#imputation-without-model",
    "href": "inference/imputation_2.html#imputation-without-model",
    "title": "Multiple imputation in R",
    "section": "Imputation without model",
    "text": "Imputation without model\n\nImpute with mean\n\n# only run once, since it's just the mean\nimp &lt;- mice(data = nhanes, \n            method = 'mean', \n            m = 1, \n            maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimp # this is not the imputed value\n\nClass: mids\nNumber of multiple imputations:  1 \nImputation methods:\n   age    bmi    hyp    chl \n    \"\" \"mean\" \"mean\" \"mean\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n# check imputed values for bmi: same as mean(nhanes$bmi, na.rm=T)\nimp$imp$bmi\n\n         1\n1  26.5625\n3  26.5625\n4  26.5625\n6  26.5625\n10 26.5625\n11 26.5625\n12 26.5625\n16 26.5625\n21 26.5625\n\n# impute the dataset\nnhanes_imp &lt;- complete(imp)\nnhanes_imp\n\n   age     bmi      hyp   chl\n1    1 26.5625 1.235294 191.4\n2    2 22.7000 1.000000 187.0\n3    1 26.5625 1.000000 187.0\n4    3 26.5625 1.235294 191.4\n5    1 20.4000 1.000000 113.0\n6    3 26.5625 1.235294 184.0\n7    1 22.5000 1.000000 118.0\n8    1 30.1000 1.000000 187.0\n9    2 22.0000 1.000000 238.0\n10   2 26.5625 1.235294 191.4\n11   1 26.5625 1.235294 191.4\n12   2 26.5625 1.235294 191.4\n13   3 21.7000 1.000000 206.0\n14   2 28.7000 2.000000 204.0\n15   1 29.6000 1.000000 191.4\n16   1 26.5625 1.235294 191.4\n17   3 27.2000 2.000000 284.0\n18   2 26.3000 2.000000 199.0\n19   1 35.3000 1.000000 218.0\n20   3 25.5000 2.000000 191.4\n21   1 26.5625 1.235294 191.4\n22   1 33.2000 1.000000 229.0\n23   1 27.5000 1.000000 131.0\n24   3 24.9000 1.000000 191.4\n25   2 27.4000 1.000000 186.0\n\n\n\n\nImpute by sampling\n\nimps &lt;- mice(data = nhanes, \n            method = 'sample', \n            m = 1, \n            maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimps$imp$bmi\n\n      1\n1  27.4\n3  22.0\n4  22.5\n6  22.7\n10 20.4\n11 33.2\n12 22.7\n16 29.6\n21 25.5"
  },
  {
    "objectID": "inference/imputation_2.html#imputation-with-regression",
    "href": "inference/imputation_2.html#imputation-with-regression",
    "title": "Multiple imputation in R",
    "section": "Imputation with regression",
    "text": "Imputation with regression\nRegression methods (continuous, normal outcome) are implemented in mice with methods starting with norm.\n\nLinear regression without parameter uncertainty, mice.impute.norm.nob\nLinear regression through prediction, mice.impute.norm.predict\nBayesian linear regression, mice.impute.norm\nLinear regression bootstrap, mice.impute.norm.boot\n\n\nExample: Regression without parameter uncertainty\nWe can generate two imputed datasets by setting m=2.\nThere is a certain level of randomness, so would be a good idea to set seed.\n\nset.seed(1)\nimpr0 &lt;- mice(nhanes, method = 'norm.nob', m=2, maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  1   2  bmi  hyp  chl\n\nimpr0\n\nClass: mids\nNumber of multiple imputations:  2 \nImputation methods:\n       age        bmi        hyp        chl \n        \"\" \"norm.nob\" \"norm.nob\" \"norm.nob\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\nnhanes_impr0 &lt;- complete(impr0) # by default, returns the first imputation\nnhanes_impr0\n\n   age      bmi       hyp      chl\n1    1 35.53430 1.2503841 256.6153\n2    2 22.70000 1.0000000 187.0000\n3    1 27.31412 1.0000000 187.0000\n4    3 25.31243 2.3880837 267.1435\n5    1 20.40000 1.0000000 113.0000\n6    3 17.94547 1.5855064 184.0000\n7    1 22.50000 1.0000000 118.0000\n8    1 30.10000 1.0000000 187.0000\n9    2 22.00000 1.0000000 238.0000\n10   2 26.99782 1.0810473 206.9927\n11   1 32.71511 0.7819353 213.7222\n12   2 27.65399 0.7904680 209.6716\n13   3 21.70000 1.0000000 206.0000\n14   2 28.70000 2.0000000 204.0000\n15   1 29.60000 1.0000000 252.1596\n16   1 27.47980 0.6071353 145.9557\n17   3 27.20000 2.0000000 284.0000\n18   2 26.30000 2.0000000 199.0000\n19   1 35.30000 1.0000000 218.0000\n20   3 25.50000 2.0000000 245.7884\n21   1 35.12809 0.5807116 232.4652\n22   1 33.20000 1.0000000 229.0000\n23   1 27.50000 1.0000000 131.0000\n24   3 24.90000 1.0000000 268.3929\n25   2 27.40000 1.0000000 186.0000\n\n\nWhen we have two imputed datasets, we can check the values for each of the variables. For example, extract bmi variable from the imputed data imp,\n\n# two imputed datasets (m=2)\nimpr0$imp$bmi\n\n          1        2\n1  35.53430 32.26078\n3  27.31412 22.55473\n4  25.31243 14.90410\n6  17.94547 22.59196\n10 26.99782 25.08534\n11 32.71511 27.71485\n12 27.65399 25.76286\n16 27.47980 30.34985\n21 35.12809 29.89142\n\n\nWe can also specify which imputed dataset to use as our complete data. Set index to 0 (action = 0) returns the original dataset with missing values.\nHere we check which of the imputed data is being used as the completed dataset. First take a note of the row IDs (based on bmi, for example). Then we generate completed dataset.\n\nif no action argument is set, then it returns the first imputation by default\naction=0 corresponds to the original data with missing values\n\n\n# check which imputed data is used for the final result, take note of row id\nid_missing &lt;- which(is.na(nhanes$bmi))\nid_missing\n\n[1]  1  3  4  6 10 11 12 16 21\n\nnhanes_impr0_action0 &lt;- complete(impr0, action = 0) \nnhanes_impr0_action0[id_missing, ] # original data with missing bmi\n\n   age bmi hyp chl\n1    1  NA  NA  NA\n3    1  NA   1 187\n4    3  NA  NA  NA\n6    3  NA  NA 184\n10   2  NA  NA  NA\n11   1  NA  NA  NA\n12   2  NA  NA  NA\n16   1  NA  NA  NA\n21   1  NA  NA  NA\n\nnhanes_impr0_action1 &lt;- complete(impr0, action = 1) \nnhanes_impr0_action1[id_missing, ] # using first imputation\n\n   age      bmi       hyp      chl\n1    1 35.53430 1.2503841 256.6153\n3    1 27.31412 1.0000000 187.0000\n4    3 25.31243 2.3880837 267.1435\n6    3 17.94547 1.5855064 184.0000\n10   2 26.99782 1.0810473 206.9927\n11   1 32.71511 0.7819353 213.7222\n12   2 27.65399 0.7904680 209.6716\n16   1 27.47980 0.6071353 145.9557\n21   1 35.12809 0.5807116 232.4652\n\nnhanes_impr0_action2 &lt;- complete(impr0, action = 2) \nnhanes_impr0_action2[id_missing, ] # using second imputation\n\n   age      bmi       hyp      chl\n1    1 32.26078 0.4616324 228.0022\n3    1 22.55473 1.0000000 187.0000\n4    3 14.90410 1.4558818 212.7958\n6    3 22.59196 1.7664882 184.0000\n10   2 25.08534 1.2940549 201.5872\n11   1 27.71485 0.9410698 169.2427\n12   2 25.76286 1.3570093 168.5961\n16   1 30.34985 0.6878971 163.7262\n21   1 29.89142 1.0452062 212.9144\n\n\n\n\nOther imputation by linear regression\nOther various of imputaton via linear regression can be implemented simply by changing the method argument.\n\nLinear regression through prediction, mice.impute.norm.predict\nBayesian linear regression, mice.impute.norm\nLinear regression bootstrap, mice.impute.norm.boot\n\n\nimpr &lt;- mice(nhanes, method = 'norm.predict', m=1, maxit=1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimpr$imp$bmi\n\n          1\n1  28.33396\n3  28.33396\n4  22.75613\n6  21.17519\n10 27.19573\n11 29.12443\n12 26.26576\n16 30.28688\n21 28.33396\n\n\nBayesian linear regression\n\nimpb &lt;- mice(nhanes, method = 'norm', m=1, maxit=1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimpb$imp$bmi\n\n          1\n1  33.82959\n3  28.98754\n4  20.88810\n6  19.11391\n10 27.32990\n11 29.44117\n12 22.68062\n16 32.13267\n21 22.03164\n\n# nhanes_impb &lt;- complete(impb)\n\nBootstrap\n\nimpbt &lt;- mice(nhanes, method = 'norm.boot', m=1, maxit=1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\nimpbt$imp$bmi\n\n          1\n1  24.19248\n3  28.77464\n4  22.42321\n6  23.47542\n10 21.95529\n11 23.12703\n12 25.84230\n16 27.68216\n21 26.43770"
  },
  {
    "objectID": "inference/imputation_2.html#predictive-mean-matching-pmm",
    "href": "inference/imputation_2.html#predictive-mean-matching-pmm",
    "title": "Multiple imputation in R",
    "section": "Predictive Mean Matching PMM",
    "text": "Predictive Mean Matching PMM\nThe idea behind PMM is as follow.\n\nwith complete data, estimate a linear regression of Y (some missing) on Z (no missing), results in coefficients \\(\\beta\\).\ndraw \\(\\beta^*\\) from the posterior predictive distribution of \\(\\beta\\) (multivariate normal with mean b and covariance matrix of b).\ngenerate predicted values for \\(Y_{hat}\\) (complete cases) and \\(Y_{star}\\) (missing)\nfor each \\(Y_{star}\\), identify a few cases (7,12) whose predicted values \\(Y_{hat}\\) are close to the predicted \\(Y_{star}\\) (10 in the illustration below)\nrandomly draw one value from the observed \\(Y\\) from the doner cases (6, 11).\n\n\n\n\n\n\nAssumption for PMM: distribution of missing is the same as obsereved data of the candidates that produce the closest values to the predicted value by the missing entry.\nPMM is robust to transformation, less vulnerable to model misspecification.\nImplementation in mice:\n\nPredictive mean matching, mice.impute.pmm\nWeighted predictive mean matching, mice.impute.midastouch\nMultivariate predictive mean matching, mice.impute.mpmm\n\n\nimp_pmm &lt;- mice(nhanes, method = 'pmm', m=1, maxit=10)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n  6   1  bmi  hyp  chl\n  7   1  bmi  hyp  chl\n  8   1  bmi  hyp  chl\n  9   1  bmi  hyp  chl\n  10   1  bmi  hyp  chl\n\nimp_pmm\n\nClass: mids\nNumber of multiple imputations:  1 \nImputation methods:\n  age   bmi   hyp   chl \n   \"\" \"pmm\" \"pmm\" \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n# imputations for bmi\nimp_pmm$imp$bmi\n\n      1\n1  35.3\n3  27.2\n4  27.4\n6  22.5\n10 26.3\n11 22.5\n12 26.3\n16 33.2\n21 35.3\n\n\n\nimp_pmms &lt;- mice(nhanes, method = 'midastouch', m=1, maxit=10)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n  6   1  bmi  hyp  chl\n  7   1  bmi  hyp  chl\n  8   1  bmi  hyp  chl\n  9   1  bmi  hyp  chl\n  10   1  bmi  hyp  chl\n\nimp_pmm\n\nClass: mids\nNumber of multiple imputations:  1 \nImputation methods:\n  age   bmi   hyp   chl \n   \"\" \"pmm\" \"pmm\" \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\nimp_pmms$imp$bmi\n\n      1\n1  22.5\n3  30.1\n4  27.2\n6  27.4\n10 27.4\n11 30.1\n12 28.7\n16 22.5\n21 27.2"
  },
  {
    "objectID": "inference/imputation_1_overview.html",
    "href": "inference/imputation_1_overview.html",
    "title": "Missing data and imputation",
    "section": "",
    "text": "Missing data can occur in various situations:\n\nunit non-response: individuals decline to participate\nitem non-response: some questions or measurements left out by participating individuals\nloss of follow-up: drop outs\n\nFor unit non-response and loss of follow-up, weighting the data could help. Need to know the response rate, characteristics of the non-responders and how the respondent differ.\nSolutions (to item non-response)\n\ncomplete case analysis (listwise deletion)\nsingle imputation\n\nmean imputation\nconditional imputation: make a prediction, plus minus some noise\n\nmultiple imputation: repeat single imputation M times, produce M complete datasets, use Rubin’s rules to pool the estimates.\n\n\n\nThree categories of missing mechanism. Example: disease status, level of exposure, age. Exposure is missing for some subjects.\n\nMCAR: any two individuals have the same probability of having missing value for exposure. This is unlikely.\nMAR: any two individuaals with the same disease status and age have the same chance of having the exposure missing\nMNAR: chance of having missing exposure depends on the value of exposure\n\nData could also have special missing structures: multi-level, longitudinal and repeated measurements. In RCT a specific technique reference based imputation can be applied."
  },
  {
    "objectID": "inference/imputation_1_overview.html#overview",
    "href": "inference/imputation_1_overview.html#overview",
    "title": "Missing data and imputation",
    "section": "",
    "text": "Missing data can occur in various situations:\n\nunit non-response: individuals decline to participate\nitem non-response: some questions or measurements left out by participating individuals\nloss of follow-up: drop outs\n\nFor unit non-response and loss of follow-up, weighting the data could help. Need to know the response rate, characteristics of the non-responders and how the respondent differ.\nSolutions (to item non-response)\n\ncomplete case analysis (listwise deletion)\nsingle imputation\n\nmean imputation\nconditional imputation: make a prediction, plus minus some noise\n\nmultiple imputation: repeat single imputation M times, produce M complete datasets, use Rubin’s rules to pool the estimates.\n\n\n\nThree categories of missing mechanism. Example: disease status, level of exposure, age. Exposure is missing for some subjects.\n\nMCAR: any two individuals have the same probability of having missing value for exposure. This is unlikely.\nMAR: any two individuaals with the same disease status and age have the same chance of having the exposure missing\nMNAR: chance of having missing exposure depends on the value of exposure\n\nData could also have special missing structures: multi-level, longitudinal and repeated measurements. In RCT a specific technique reference based imputation can be applied."
  },
  {
    "objectID": "inference/imputation_1_overview.html#consideration",
    "href": "inference/imputation_1_overview.html#consideration",
    "title": "Missing data and imputation",
    "section": "Consideration",
    "text": "Consideration\nNeed to account for the missinng data process, preserve the relations in the data and uncertainty in the relations.\n\nMAR assumption whether plausible. (FCS can handle both MAR anad MNAR)\nform of imputation model: structure and error distribution\npredictors, as many relevant as possible, including interactions\nthe order in which to impute\nset up starting imputation and number of iteration\ndecide number of imputed datasets"
  },
  {
    "objectID": "inference/intervals.html",
    "href": "inference/intervals.html",
    "title": "Intervals",
    "section": "",
    "text": "Key difference: confidence and credible intervals: about the (unknown) parameter; prediction interval: about individual (unseen) observations."
  },
  {
    "objectID": "inference/intervals.html#prediction-intereval",
    "href": "inference/intervals.html#prediction-intereval",
    "title": "Intervals",
    "section": "Prediction intereval",
    "text": "Prediction intereval\nIn a regression model, you might want to know both confidence and prediction intervals.\n\nCI for mean value of \\(y\\) when \\(x =0\\), the mean response (e.g. growth of GDP), this is a parameter, an average\nPI for \\(y\\) when \\(x=0\\), this is an individual observation.\n\n\nIn simple linear regression\nStandard deviation for linear predictor \\(\\alpha + \\beta x\\) is\n\\(\\hat{\\sigma}_{linpred} = \\hat{\\sigma} \\sqrt{\\frac{1}{n} + \\frac{(x_{new} - \\bar{x})^2}{\\sum(x_i - \\bar{x})}}\\)\nConfidence interval\n\\(\\hat{y_{new}} \\pm t_{1-\\frac{\\alpha}{2}, n-2} \\times \\sqrt{\\hat{\\sigma}^2 (\\frac{1}{n} + \\frac{(x_{new} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2})}\\)\nStandard deviation for the predicted value \\(\\alpha + \\beta x + \\epsilon\\) is\n\\(\\hat{\\sigma}_{prediction} = \\hat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(x_{new} - \\bar{x})^2}{\\sum(x_i - \\bar{x})}}\\)\nPrediction interval (frequentist)\n\\(\\hat{y_{new}} \\pm t_{1-\\frac{\\alpha}{2}, n-2} \\times \\sqrt{\\hat{\\sigma}^2 (1 + \\frac{1}{n} + \\frac{(x_{new} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2})}\\)"
  },
  {
    "objectID": "inference/intervals.html#predictive-distribution",
    "href": "inference/intervals.html#predictive-distribution",
    "title": "Confidence, credible and prediction intervals",
    "section": "Predictive distribution",
    "text": "Predictive distribution\nPrior and posterior predictive distribution\nDifference between posterior distribution and posterior predictive distribution PPD\n\nposterior dist \\(p(\\theta|x) = c \\times p(x|\\theta)p(\\theta)\\), depends on the parame*ter \\(\\theta\\)\nPPD does not depend on \\(\\theta\\) as it is integrated out, for unobserved \\(x^*\\),\n\n\\(p(x^*|x) = \\int_{\\Theta} c \\times p(x^*, \\theta|x) d\\theta = \\int_{\\Theta} c \\times p(x^*|\\theta)p(\\theta|x) d\\theta\\)"
  },
  {
    "objectID": "inference/intervals.html#confidence-vs-credible-interval",
    "href": "inference/intervals.html#confidence-vs-credible-interval",
    "title": "Intervals",
    "section": "Confidence vs credible interval",
    "text": "Confidence vs credible interval\nConfidence interval (frequentist), about the unknown but fixed parameter. That means, the parameter is not treated as a random variable so does not have a probability distribution. CI is random because it is based on your sample.\nCredible interval (Bayesian), associated with posterior distribution of the parameter. The parameter is treated as a random variable hence has a probability distribution."
  },
  {
    "objectID": "inference/intervals.html#posterior-predictive-distribution",
    "href": "inference/intervals.html#posterior-predictive-distribution",
    "title": "Intervals",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\nDifference between posterior distribution and posterior predictive distribution PPD\n\nposterior dist \\(p(\\theta|x) = c \\times p(x|\\theta)p(\\theta)\\), depends on the parameter \\(\\theta\\)\nPPD does not depend on \\(\\theta\\) as it is integrated out, for unobserved \\(x^*\\),\n\n\\(p(x^*|x) = \\int_{\\Theta} c \\times p(x^*, \\theta|x) d\\theta = \\int_{\\Theta} c \\times p(x^*|\\theta)p(\\theta|x) d\\theta\\)\nPD is part of PPD formulation.\n\nPD explains the unknown parameter (treated as a random variable), conditional on the evidence observed (data).\nPPD is the distribution for the future predicted data based on the data you have already seen."
  },
  {
    "objectID": "inference/intervals.html#predict-mortality-using-2000-2019-data",
    "href": "inference/intervals.html#predict-mortality-using-2000-2019-data",
    "title": "Intervals",
    "section": "Predict mortality using 2000-2019 data",
    "text": "Predict mortality using 2000-2019 data\n\n# take pre 2019 data\ndt &lt;- d[year &lt;= 2019, .(year, deaths_vs_pop_per_100k)]\n\n# prediction \ndnew &lt;- data.frame(year = c(2020, 2021, 2022, 2023))\n\n### Linear regression with lm\n\nm_linear &lt;- lm(deaths_vs_pop_per_100k ~ year, \n               data = dt)\n\n# summary(m_linear)\n\n# produce two intervals\npred_freq_ci &lt;- predict(m_linear, newdata = dnew, interval = 'confidence')\npred_freq_pi &lt;- predict(m_linear, newdata = dnew, interval = 'prediction')\n\n\nVerify from formula\n\n# verify with formula\nn &lt;- nrow(dt)\n\n# option 1\nfitted_val &lt;- m_linear$fitted.values\nmse &lt;- sum((dt$deaths_vs_pop_per_100k - fitted_val)^2)/(n-2)\nmse\n\n[1] 4.383279\n\n# option 2\nsum((m_linear$residuals)^2)/(n-2)\n\n[1] 4.383279\n\n# option 3\nsummary(m_linear)$sigma^2\n\n[1] 4.383279\n\n# option 4\ndvmisc::get_mse(m_linear)\n\n[1] 4.383279\n\n# t-val\ntval &lt;- qt(p=0.975, df=n-2)\nmean_x &lt;- mean(dt$year)\n\n# sum(xi - xbar)^2\nssx &lt;- sum((dt$year - mean_x)^2)\n\nsd_confint &lt;- sqrt(mse * (1/20+ ((dnew$year - mean_x)^2)/ssx))\nsd_predint &lt;- sqrt(mse * (1 + 1/20+ ((dnew$year - mean_x)^2)/ssx))\n\n\n# point prediction\nb0 &lt;- coef(m_linear)[1]\nb &lt;- coef(m_linear)[2]\nprednew &lt;- b0 + b*dnew$year\nprednew\n\n[1] 85.68773 82.50765 79.32757 76.14749\n\n# prediction interval\npredint_linear &lt;- data.frame(fit = prednew, \n                             lwr = prednew - tval*sd_predint, \n                             upr = prednew + tval*sd_predint)\n\npredint_linear\n\n       fit      lwr      upr\n1 85.68773 80.83777 90.53770\n2 82.50765 77.59214 87.42316\n3 79.32757 74.34154 84.31360\n4 76.14749 71.08617 81.20880\n\n# confidence interval\nconfint_linear &lt;- data.frame(fit = prednew, \n                             lwr = prednew - tval*sd_confint, \n                             upr = prednew + tval*sd_confint)\n\nconfint_linear\n\n       fit      lwr      upr\n1 85.68773 83.64447 87.73100\n2 82.50765 80.31334 84.70196\n3 79.32757 76.97954 81.67560\n4 76.14749 73.64355 78.65142\n\n\n\n\nLinear regression with rstanarm\n\nm_bayes &lt;- rstanarm::stan_glm(\n  deaths_vs_pop_per_100k ~ year, \n  data = dt, \n  family = gaussian,\n  iter = 2000,\n  chains = 8,\n  refresh = 0\n)\n\nm_bayes \n\nstan_glm\n family:       gaussian [identity]\n formula:      deaths_vs_pop_per_100k ~ year\n observations: 20\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 6506.5  175.7\nyear          -3.2    0.1\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.2    0.4   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nsims &lt;- as.matrix(m_bayes)\nhead(sims)\n\n          parameters\niterations (Intercept)      year    sigma\n      [1,]    6227.938 -3.040142 3.106669\n      [2,]    6569.432 -3.210131 1.821779\n      [3,]    6750.044 -3.299792 2.419353\n      [4,]    6559.827 -3.204924 3.050782\n      [5,]    6306.464 -3.079356 1.727595\n      [6,]    6458.871 -3.155199 1.824202\n\nmedian &lt;- apply(sims, 2, median)\nmedian\n\n(Intercept)        year       sigma \n6506.530315   -3.178598    2.190397 \n\nmad_sd &lt;- apply(sims, 2, mad) \n# median absolute deviation (similar to sd)\nmad_sd\n\n(Intercept)        year       sigma \n175.6739842   0.0874817   0.3800231 \n\n\n\nCredible interval\n\n# credible interval about the fit\ncred &lt;- rstanarm::posterior_interval(m_bayes, prob = 0.95)\ncred\n\n                   2.5%       97.5%\n(Intercept) 6142.839155 6874.270468\nyear          -3.361534   -2.998133\nsigma          1.611480    3.201237\n\n# equivalent to\nsims &lt;- as.matrix(m_bayes)\napply(sims, 2, quantile, probs = c(0.025, 0.5, 0.975))\n\n       parameters\n        (Intercept)      year    sigma\n  2.5%     6142.839 -3.361534 1.611480\n  50%      6506.530 -3.178598 2.190397\n  97.5%    6874.270 -2.998133 3.201237\n\n\n\n\nPoint prediction\n\n# point predict\n# uses median from the posterior sim\ny_point_pred &lt;- predict(m_bayes, newdata = dnew)\ny_point_pred\n\n       1        2        3        4 \n85.69776 82.51901 79.34027 76.16153 \n\na_hat &lt;- coef(m_bayes)[1] # median\nb_hat &lt;- coef(m_bayes)[2] # median\na_hat + b_hat*dnew$year\n\n[1] 85.76176 82.58316 79.40456 76.22596\n\n\n\n# linear predictor with uncertainty via a's and b's\ny_linpred &lt;- rstanarm::posterior_linpred(m_bayes, newdata = dnew)\nhead(y_linpred)\n\n          \niterations        1        2        3        4\n      [1,] 86.85001 83.80987 80.76972 77.72958\n      [2,] 84.96646 81.75633 78.54619 75.33606\n      [3,] 84.46453 81.16473 77.86494 74.56515\n      [4,] 85.88082 82.67590 79.47097 76.26605\n      [5,] 86.16433 83.08497 80.00562 76.92626\n      [6,] 85.36923 82.21403 79.05883 75.90363\n\n# focus on one year: 2023\nhist(y_linpred[, 4])\n\n\n\nhead(y_linpred[, 4])\n\n[1] 77.72958 75.33606 74.56515 76.26605 76.92626 75.90363\n\ny_linpred_byhand &lt;- sims[,1] + dnew$year[4] * sims[,2]\ny_linpred_byhand[1:6]\n\n[1] 77.72958 75.33606 74.56515 76.26605 76.92626 75.90363\n\n\n\n# posterior predictive dist (with sigma)\ny_postpred &lt;- rstanarm::posterior_predict(m_bayes, newdata = dnew)\nhead(y_postpred)\n\n            1        2        3        4\n[1,] 84.26813 84.62143 86.88816 80.15932\n[2,] 84.77948 79.19925 79.47003 74.50026\n[3,] 84.28265 82.71001 71.98839 70.65769\n[4,] 86.88798 82.69186 84.09118 79.60984\n[5,] 87.15609 82.98913 76.53739 76.95372\n[6,] 87.23112 82.82839 80.20166 74.13253\n\n# by hand\n# focus on one year: 2023\nn_sim &lt;- nrow(sims)\ny_postpred_byhand &lt;- y_linpred_byhand + rnorm(n_sim, 0, sims[,3])\nhist(y_postpred_byhand)\n\n\n\nhist(y_postpred[,4])\n\n\n\ny_postpred[,4] |&gt; summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  64.27   74.44   76.10   76.13   77.83   88.49 \n\ny_postpred_byhand |&gt; summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  63.13   74.43   76.14   76.15   77.84   88.06 \n\n\n### Prediction\n\npred_all &lt;- rstanarm::posterior_predict(m_bayes, d)\n\n\npred_all_quantile &lt;- apply(pred_all, 2, quantile, \n                    probs = c(0.025, 0.5, 0.975)) |&gt; \n  t()  |&gt; \n  as.data.frame()\ntail(pred_all_quantile)\n\n       2.5%      50%    97.5%\n19 87.21596 92.02281 96.74796\n20 84.02340 88.91247 93.65092\n21 80.59699 85.69280 90.81009\n22 77.41676 82.45745 87.62077\n23 74.31205 79.28714 84.50176\n24 70.96315 76.13478 81.39219\n\n\n\n\nShow the code\npd &lt;- copy(d)\nhead(pd)\n\n\n   year deaths_n     age pop_jan1_n deaths_vs_pop_per_100k\n1: 2000     5427 000_059    3613275               150.1962\n2: 2001     5289 000_059    3636329               145.4489\n3: 2002     5303 000_059    3656613               145.0249\n4: 2003     5144 000_059    3679479               139.8024\n5: 2004     5069 000_059    3694134               137.2175\n6: 2005     4957 000_059    3705027               133.7912\n\n\nShow the code\npd &lt;- cbind(pd, pred_all_quantile)\n\nsetnames(pd, old = '2.5%', new = 'exp_death_per100k_025')\nsetnames(pd, old = '50%', new = 'exp_death_per100k_50')\nsetnames(pd, old = '97.5%', new = 'exp_death_per100k_975')\n\n# also compute the expected death overall\n\npd[, exp_death_025 := exp_death_per100k_025 * pop_jan1_n/100000]\npd[, exp_death_50 := exp_death_per100k_50 * pop_jan1_n/100000]\npd[, exp_death_975 := exp_death_per100k_975 * pop_jan1_n/100000]\n\npd[, alert := fcase(\n  deaths_vs_pop_per_100k &gt; exp_death_per100k_975, \"Higher than expected\",\n  default = \"Expected\"\n)]\n\npd[, type := fcase(\n  year &lt;= 2019, paste0(\"Baseline (2000-2019)\"),\n  default = \"Pandemic years (2020-2023)\"\n)]\n\n\n# make plot\nq &lt;- ggplot(pd, aes(x = year))\nq &lt;- q + geom_ribbon(mapping = aes(ymin = exp_death_per100k_025, \n                                   ymax = exp_death_per100k_975), \n                     alpha = 0.3)\nq &lt;- q + geom_line(mapping = aes(y = exp_death_per100k_50, lty = type), linewidth = 1)\nq &lt;- q + geom_point(mapping = aes(y = deaths_vs_pop_per_100k, color = alert), size = 3)\nq &lt;- q + geom_vline(xintercept = 2019.5, color = \"red\", lty = 2)\nq &lt;- q + expand_limits(y=0)\nq &lt;- q + scale_y_continuous(\"Number of death per 100k\", expand = expansion(mult = c(0, 0.1)))\nq &lt;- q + scale_x_continuous(\"Year\", breaks = seq(2000, 2023, 2))\nq &lt;- q + scale_linetype_discrete(NULL)\nq &lt;- q + scale_color_brewer(NULL, palette = \"Set1\", direction = -1)\nq &lt;- q + theme_bw()\nq &lt;- q + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15), \n  axis.text.x = element_text(angle = 45, vjust = 0.5) \n)\nq &lt;- q + theme(legend.position = \"bottom\", legend.direction = \"horizontal\")\nq &lt;- q + theme(legend.box = \"horizontal\", legend.margin = margin(2, 2, 2, 2))\nq"
  },
  {
    "objectID": "inference/intervals.html#explore-the-data",
    "href": "inference/intervals.html#explore-the-data",
    "title": "Intervals",
    "section": "Explore the data",
    "text": "Explore the data\n\n\nShow the code\nq1 &lt;- ggplot(d, aes(x = year, y = deaths_n, group))\nq1 &lt;- q1 + geom_line()\nq1 &lt;- q1 + geom_point(size = 2)\nq1 &lt;- q1 + geom_vline(xintercept = 2019.5, color = \"red\", lty = 2)\nq1 &lt;- q1 + theme_bw()\nq1 &lt;- q1 + scale_x_continuous(breaks = seq(2000, 2023, 2))\nq1 &lt;- q1 + labs(\n  x = 'Year', \n  y = 'Number of deaths', \n  title = 'Number of death in Norway \\n2000 - 2023'\n)\nq1 &lt;- q1 + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 12), \n  axis.text.x = element_text(angle = 45, vjust = 0.5) \n)\n# q1\n\n# per 100k\nq2 &lt;- ggplot(d, aes(x = year, y = deaths_vs_pop_per_100k))\nq2 &lt;- q2 + geom_line()\nq2 &lt;- q2 + geom_point(size = 2)\nq2 &lt;- q2 + geom_vline(xintercept = 2019.5, color = \"red\", lty = 2)\nq2 &lt;- q2 + theme_bw()\nq2 &lt;- q2 + scale_x_continuous(breaks = seq(2000, 2023, 2))\nq2 &lt;- q2 + labs(\n  x = 'Year', \n  y = 'Number of deaths', \n  title = 'Number of death in Norway \\n(per 100k population) 2000 - 2023'\n)\nq2 &lt;- q2 + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 12), \n  axis.text.x = element_text(angle = 45, vjust = 0.5) \n)\n# plot side by side\nq1 + q2"
  },
  {
    "objectID": "inference/intervals.html#model-mortality-using-2000-2019-data",
    "href": "inference/intervals.html#model-mortality-using-2000-2019-data",
    "title": "Intervals",
    "section": "Model mortality using 2000-2019 data",
    "text": "Model mortality using 2000-2019 data\n\n# take pre 2019 data\ndt &lt;- d[year &lt;= 2019, .(year, deaths_vs_pop_per_100k)]\n\n# prediction \ndnew &lt;- data.frame(year = c(2020, 2021, 2022, 2023))\n\n\nLinear regression with lm\n\nm_linear &lt;- lm(deaths_vs_pop_per_100k ~ year, \n               data = dt)\n\n# summary(m_linear)\n\n# produce two intervals\npred_freq_pi &lt;- predict(m_linear, newdata = dnew, interval = 'prediction')\npred_freq_ci &lt;- predict(m_linear, newdata = dnew, interval = 'confidence')\n\n\nVerify from formula\n\n# verify with formula\nn &lt;- nrow(dt)\n\n# option 1\nfitted_val &lt;- m_linear$fitted.values\nmse &lt;- sum((dt$deaths_vs_pop_per_100k - fitted_val)^2)/(n-2)\nmse\n\n[1] 4.383279\n\n# option 2\nsum((m_linear$residuals)^2)/(n-2)\n\n[1] 4.383279\n\n# option 3\nsummary(m_linear)$sigma^2\n\n[1] 4.383279\n\n# option 4\ndvmisc::get_mse(m_linear)\n\n[1] 4.383279\n\n# t-val\ntval &lt;- qt(p=0.975, df=n-2)\nmean_x &lt;- mean(dt$year)\n\n# sum(xi - xbar)^2\nssx &lt;- sum((dt$year - mean_x)^2)\n\nsd_confint &lt;- sqrt(mse * (1/20+ ((dnew$year - mean_x)^2)/ssx))\nsd_predint &lt;- sqrt(mse * (1 + 1/20+ ((dnew$year - mean_x)^2)/ssx))\n\n\n# point prediction\nb0 &lt;- coef(m_linear)[1]\nb &lt;- coef(m_linear)[2]\nprednew &lt;- b0 + b*dnew$year\nprednew\n\n[1] 85.68773 82.50765 79.32757 76.14749\n\n# prediction interval\npredint_linear &lt;- data.frame(fit = prednew, \n                             lwr = prednew - tval*sd_predint, \n                             upr = prednew + tval*sd_predint)\n\npredint_linear\n\n       fit      lwr      upr\n1 85.68773 80.83777 90.53770\n2 82.50765 77.59214 87.42316\n3 79.32757 74.34154 84.31360\n4 76.14749 71.08617 81.20880\n\npred_freq_pi # compare with result from lm\n\n       fit      lwr      upr\n1 85.68773 80.83777 90.53770\n2 82.50765 77.59214 87.42316\n3 79.32757 74.34154 84.31360\n4 76.14749 71.08617 81.20880\n\n# confidence interval\nconfint_linear &lt;- data.frame(fit = prednew, \n                             lwr = prednew - tval*sd_confint, \n                             upr = prednew + tval*sd_confint)\n\nconfint_linear\n\n       fit      lwr      upr\n1 85.68773 83.64447 87.73100\n2 82.50765 80.31334 84.70196\n3 79.32757 76.97954 81.67560\n4 76.14749 73.64355 78.65142\n\npred_freq_ci # compare with result from lm\n\n       fit      lwr      upr\n1 85.68773 83.64447 87.73100\n2 82.50765 80.31334 84.70196\n3 79.32757 76.97954 81.67560\n4 76.14749 73.64355 78.65142\n\n\n\n\n\nLinear regression with rstanarm\n\nm_bayes &lt;- rstanarm::stan_glm(\n  deaths_vs_pop_per_100k ~ year, \n  data = dt, \n  family = gaussian,\n  iter = 2000,\n  chains = 8,\n  refresh = 0\n)\n\nm_bayes \n\nstan_glm\n family:       gaussian [identity]\n formula:      deaths_vs_pop_per_100k ~ year\n observations: 20\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 6510.0  169.5\nyear          -3.2    0.1\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.2    0.4   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nsims &lt;- as.matrix(m_bayes)\nmedian &lt;- apply(sims, 2, median)\nmedian\nmad_sd &lt;- apply(sims, 2, mad) \n# median absolute deviation (similar to sd)\nmad_sd\n\n\nCredible interval\n\n# credible interval about the fit\ncred &lt;- rstanarm::posterior_interval(m_bayes, prob = 0.95)\ncred\n\n                   2.5%       97.5%\n(Intercept) 6147.110403 6852.327354\nyear          -3.350749   -2.999700\nsigma          1.614450    3.220209\n\n# equivalent to\nsims &lt;- as.matrix(m_bayes)\napply(sims, 2, quantile, probs = c(0.025, 0.5, 0.975))\n\n       parameters\n        (Intercept)      year    sigma\n  2.5%     6147.110 -3.350749 1.614450\n  50%      6510.045 -3.180305 2.194318\n  97.5%    6852.327 -2.999700 3.220209\n\n\n\n\n\nPoint prediction\n\n# point predict\n# uses median from the posterior sim\ny_point_pred &lt;- predict(m_bayes, newdata = dnew)\ny_point_pred\n\n       1        2        3        4 \n85.69091 82.51177 79.33263 76.15349 \n\na_hat &lt;- coef(m_bayes)[1] # median\nb_hat &lt;- coef(m_bayes)[2] # median\na_hat + b_hat*dnew$year\n\n[1] 85.82895 82.64865 79.46834 76.28804\n\n\n\n\nUncertainty of linear predictor\nThe uncertainty of linear predictor, \\(a + bx\\) is propagated through the uncertainty in \\(a\\) and \\(b\\), respectively. For now the error term is not included.\nrstanarm::posterior_linpred is equivalent to using each pairs of \\(a, b\\) from the posterior distribution to compute the point predictions.\n\n# linear predictor with uncertainty via a's and b's\ny_linpred &lt;- rstanarm::posterior_linpred(m_bayes, newdata = dnew)\nhead(y_linpred)\n\n          \niterations        1        2        3        4\n      [1,] 86.76040 83.68034 80.60028 77.52022\n      [2,] 86.33213 83.24274 80.15334 77.06394\n      [3,] 86.24299 83.12216 80.00133 76.88050\n      [4,] 85.26344 82.07579 78.88813 75.70048\n      [5,] 85.49074 82.23835 78.98596 75.73357\n      [6,] 85.28163 82.01827 78.75492 75.49156\n\n# focus on one year: 2023\nhist(y_linpred[, 4], main = 'Predictions for 2023')\n\n\n\n\n\n\n\nhead(y_linpred[, 4])\n\n[1] 77.52022 77.06394 76.88050 75.70048 75.73357 75.49156\n\ny_linpred_byhand &lt;- sims[,1] + dnew$year[4] * sims[,2]\ny_linpred_byhand[1:6]\n\n[1] 77.52022 77.06394 76.88050 75.70048 75.73357 75.49156\n\n\n\n\nPosterior predictive distribution\nWith PPD, include the additional uncertainty in \\(\\sigma\\). This is equivalent to using the linear predictor from above, plus a random draw from rnorm(1, 0, sigma).\nDue to randomness in the error, we can not get the exact same results. But they should be close enough.\n\n# posterior predictive dist (with sigma)\ny_postpred &lt;- rstanarm::posterior_predict(m_bayes, newdata = dnew)\nhead(y_postpred)\n\n            1        2        3        4\n[1,] 85.38304 80.76660 80.77002 78.29299\n[2,] 83.58124 83.70593 83.34616 76.07834\n[3,] 86.75044 85.02622 78.82639 76.81196\n[4,] 86.80538 78.61904 81.45173 78.71479\n[5,] 83.38782 82.76790 79.21554 75.24868\n[6,] 90.54799 80.21181 83.45230 75.14096\n\n# by hand\n# focus on one year: 2023\nn_sim &lt;- nrow(sims)\ny_postpred_byhand &lt;- y_linpred_byhand + rnorm(n_sim, 0, sims[,3])\npar(mfrow = c(1,2))\nhist(y_postpred_byhand, main = 'PPD for 2023 (linpred + error)')\nhist(y_postpred[,4], main = 'PPD for 2023 (post_predict)')\n\n\n\n\n\n\n\ny_postpred[,4] |&gt; summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  66.36   74.44   76.12   76.15   77.85   87.16 \n\ny_postpred_byhand |&gt; summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  65.10   74.51   76.23   76.21   77.89   87.72"
  },
  {
    "objectID": "inference/intervals.html#prediction-for-2020-2023-mortality",
    "href": "inference/intervals.html#prediction-for-2020-2023-mortality",
    "title": "Intervals",
    "section": "Prediction for 2020-2023 mortality",
    "text": "Prediction for 2020-2023 mortality\nHere we choose to use rstanarm::posterior_predict().\n\npred_all &lt;- rstanarm::posterior_predict(m_bayes, d)\n\npred_all_quantile &lt;- apply(pred_all, 2, quantile, \n                    probs = c(0.025, 0.5, 0.975)) |&gt; \n  t()  |&gt; \n  as.data.frame()\npred_all_quantile[21:24,]\n\n       2.5%      50%    97.5%\n21 80.76780 85.74303 90.58276\n22 77.46361 82.48300 87.47500\n23 74.24409 79.29768 84.40856\n24 70.99240 76.20945 81.30611\n\n\nWe can compare with the frequentist prediction interval, and we see that they are very close.\n\npredint_linear\n\n       fit      lwr      upr\n1 85.68773 80.83777 90.53770\n2 82.50765 77.59214 87.42316\n3 79.32757 74.34154 84.31360\n4 76.14749 71.08617 81.20880"
  },
  {
    "objectID": "inference/intervals.html#visualize-mortality-prediction",
    "href": "inference/intervals.html#visualize-mortality-prediction",
    "title": "Intervals",
    "section": "Visualize mortality prediction",
    "text": "Visualize mortality prediction\n\n\nShow the code\npd &lt;- copy(d)\n# head(pd)\npd &lt;- cbind(pd, pred_all_quantile)\n\nsetnames(pd, old = '2.5%', new = 'exp_death_per100k_025')\nsetnames(pd, old = '50%', new = 'exp_death_per100k_50')\nsetnames(pd, old = '97.5%', new = 'exp_death_per100k_975')\n\n# also compute the expected death overall\n\npd[, exp_death_025 := exp_death_per100k_025 * pop_jan1_n/100000]\npd[, exp_death_50 := exp_death_per100k_50 * pop_jan1_n/100000]\npd[, exp_death_975 := exp_death_per100k_975 * pop_jan1_n/100000]\n\npd[, alert := fcase(\n  deaths_vs_pop_per_100k &gt; exp_death_per100k_975, \"Higher than expected\",\n  default = \"Expected\"\n)]\n\npd[, type := fcase(\n  year &lt;= 2019, paste0(\"Baseline (2000-2019)\"),\n  default = \"Pandemic years (2020-2023)\"\n)]\n\n\n# make plot\nq &lt;- ggplot(pd, aes(x = year))\nq &lt;- q + geom_ribbon(mapping = aes(ymin = exp_death_per100k_025, \n                                   ymax = exp_death_per100k_975), \n                     alpha = 0.3)\nq &lt;- q + geom_line(mapping = aes(y = exp_death_per100k_50, lty = type), linewidth = 1)\nq &lt;- q + geom_point(mapping = aes(y = deaths_vs_pop_per_100k, color = alert), size = 3)\nq &lt;- q + geom_vline(xintercept = 2019.5, color = \"red\", lty = 2)\nq &lt;- q + expand_limits(y=0)\nq &lt;- q + scale_y_continuous(\"Number of death per 100k\", expand = expansion(mult = c(0, 0.1)))\nq &lt;- q + scale_x_continuous(\"Year\", breaks = seq(2000, 2023, 2))\nq &lt;- q + scale_linetype_discrete(NULL)\nq &lt;- q + scale_color_brewer(NULL, palette = \"Set1\", direction = -1)\nq &lt;- q + theme_bw()\nq &lt;- q + theme(\n  axis.text = element_text(size = 11),\n  axis.title = element_text(size = 12), \n  plot.title = element_text(size = 15), \n  axis.text.x = element_text(angle = 45, vjust = 0.5) \n)\nq &lt;- q + theme(legend.position = \"bottom\", legend.direction = \"horizontal\")\nq &lt;- q + theme(legend.box = \"horizontal\", legend.margin = margin(2, 2, 2, 2))\nq"
  },
  {
    "objectID": "inference/rwd_vendor_engagement.html",
    "href": "inference/rwd_vendor_engagement.html",
    "title": "RWD EHR Vendor Engagement",
    "section": "",
    "text": "Vendor engagement (VE) in the context of Real-world data and Electronic Health Records (EHR/EMR) is the collaboration between various healthcare organizations (such as hospitals, clinics and medical practices) and vendors that provide EHR systems. VE is essential for successful EHR implementation and ongoing use in healthcare organizations. This partnership ensures that EHR systems meet the needs of healthcare providers and improves patient care.\nA few EHR vendors:\n\nEpicCare\nOracle Health EHR\nathenahealth\neClinicalWorks"
  },
  {
    "objectID": "inference/rwd_vendor_engagement.html#what-is-vendor-engagement",
    "href": "inference/rwd_vendor_engagement.html#what-is-vendor-engagement",
    "title": "RWD EHR Vendor Engagement",
    "section": "",
    "text": "Vendor engagement (VE) in the context of Real-world data and Electronic Health Records (EHR/EMR) is the collaboration between various healthcare organizations (such as hospitals, clinics and medical practices) and vendors that provide EHR systems. VE is essential for successful EHR implementation and ongoing use in healthcare organizations. This partnership ensures that EHR systems meet the needs of healthcare providers and improves patient care.\nA few EHR vendors:\n\nEpicCare\nOracle Health EHR\nathenahealth\neClinicalWorks"
  },
  {
    "objectID": "inference/rwd_vendor_engagement.html#who-is-involed",
    "href": "inference/rwd_vendor_engagement.html#who-is-involed",
    "title": "RWD EHR Vendor Engagement",
    "section": "Who is involed",
    "text": "Who is involed"
  },
  {
    "objectID": "inference/rwd_vendor_engagement.html#how-is-it-implemented",
    "href": "inference/rwd_vendor_engagement.html#how-is-it-implemented",
    "title": "RWD EHR Vendor Engagement",
    "section": "How is it implemented",
    "text": "How is it implemented\n\nSelection process\nHealthcare orgaisations engage with EHR vendors to select EHR systems that best fits their needs. This involves evaluating the products, features, pricing and support services.\n\n\nImplementation\nThis process innvolves data migration, software installation, customization to meet specific workflow requirements, and staff training. The customization could be to support specialized clinical workflows, integrate with other softwaree solutions or implementing additional features and modules.\n\n\nAfter implementation\nVE continues after the EHR system is implemented. Healthcare organizations rely on vendors for ongoing technical support, software updates and maintenance to ensure that the systems function smoothly and remains compliant with regulatory requirements.\nFeedback and collaboration between healthcare organizations and the vendor is crucial. Feedbacks on usability, functionality and performance of the EHR system can inform future updates and improvements for the vendor.\n\n\nRegulation and compliance\nRegulatory compliance in the context of electronic health records (EHRs) typically includes adherence to laws and regulations aimed at protecting patient privacy, ensuring data security, and promoting the interoperability of health information. Some of the key regulations that healthcare organizations and EHR vendors need to comply with include:\nHealth Insurance Portability and Accountability Act (HIPAA): HIPAA sets the standard for protecting sensitive patient data. It includes the Privacy Rule, which governs the use and disclosure of protected health information (PHI), and the Security Rule, which outlines security standards for protecting electronic PHI (ePHI). HIPAA also includes the Breach Notification Rule, which requires covered entities to notify individuals affected by breaches of their PHI.\nHITECH Act: The Health Information Technology for Economic and Clinical Health (HITECH) Act promotes the adoption and meaningful use of health information technology, including EHRs. It introduced incentives for healthcare providers to adopt EHRs and strengthened HIPAA’s privacy and security requirements, including increased penalties for non-compliance.\nMeaningful Use (now Promoting Interoperability) Program: The Centers for Medicare & Medicaid Services (CMS) previously administered the Meaningful Use program, which incentivized eligible healthcare providers to adopt and demonstrate meaningful use of certified EHR technology. This program has transitioned to the Promoting Interoperability program, which focuses on promoting the exchange of health information and improving interoperability.\nFDA Regulations: The U.S. Food and Drug Administration (FDA) regulates certain types of EHR software that meet the definition of a medical device. EHR vendors must comply with FDA regulations, particularly if their software includes features that are considered medical devices."
  },
  {
    "objectID": "inference/case_study_ctn.html",
    "href": "inference/case_study_ctn.html",
    "title": "Case study: CTN",
    "section": "",
    "text": "Task overview see here\nQuestions of interest (examples)\nArticle describing the process: Odom 2023"
  },
  {
    "objectID": "inference/rwd_vendor_engagement.html#implemention",
    "href": "inference/rwd_vendor_engagement.html#implemention",
    "title": "RWD EHR Vendor Engagement",
    "section": "Implemention",
    "text": "Implemention\n\nSelection process\nHealthcare organisations engage with EHR vendors to select EHR systems that best fits their needs. This involves evaluating the products, features, pricing and support services.\n\n\nImplementation\nThis process involves data migration, software installation, customization to meet specific workflow requirements, and staff training. The customization could be to support specialized clinical workflows, integrate with other software solutions or implementing additional features and modules.\n\n\nAfter implementation\nVE continues after the EHR system is implemented. Healthcare organizations rely on vendors for ongoing technical support, software updates and maintenance to ensure that the systems function smoothly and remains compliant with regulatory requirements.\nFeedback and collaboration between healthcare organizations and the vendor is crucial. Feedbacks on usability, functionality and performance of the EHR system can inform future updates and improvements for the vendor.\n\n\nRegulation and compliance\nRegulatory compliance in the context of electronic health records (EHRs) typically includes adherence to laws and regulations aimed at protecting patient privacy, ensuring data security, and promoting the interoperability of health information. Some of the key regulations that healthcare organizations and EHR vendors need to comply with include:\nHealth Insurance Portability and Accountability Act (HIPAA): HIPAA sets the standard for protecting sensitive patient data. It includes the Privacy Rule, which governs the use and disclosure of protected health information (PHI), and the Security Rule, which outlines security standards for protecting electronic PHI (ePHI). HIPAA also includes the Breach Notification Rule, which requires covered entities to notify individuals affected by breaches of their PHI.\nHITECH Act: The Health Information Technology for Economic and Clinical Health (HITECH) Act promotes the adoption and meaningful use of health information technology, including EHRs. It introduced incentives for healthcare providers to adopt EHRs and strengthened HIPAA’s privacy and security requirements, including increased penalties for non-compliance.\nMeaningful Use (now Promoting Interoperability) Program: The Centers for Medicare & Medicaid Services (CMS) previously administered the Meaningful Use program, which incentivized eligible healthcare providers to adopt and demonstrate meaningful use of certified EHR technology. This program has transitioned to the Promoting Interoperability program, which focuses on promoting the exchange of health information and improving interoperability.\nFDA Regulations: The U.S. Food and Drug Administration (FDA) regulates certain types of EHR software that meet the definition of a medical device. EHR vendors must comply with FDA regulations, particularly if their software includes features that are considered medical devices."
  },
  {
    "objectID": "inference/case_study_ctn.html#ctn-data",
    "href": "inference/case_study_ctn.html#ctn-data",
    "title": "Case study: CTN",
    "section": "CTN data",
    "text": "CTN data\n\nStudy protocol\nThree studies\n\nCTN 30(Protocol): prescription opiate abuse treatment study. Randomized, outpatient study, whether adding additional drug counseling improves outcome. Two phases.\n\nTx0: Bup/Nx (buprenorphine/naloxone) + SMM (standard medical management)\nTx1: Bup/Nx + EMM (enhanced medical management)\nphase 1 n=653, phase 2 n=360\nprimary finding: patients are likely to reduce opioid use during Bup/Nx treatment, however unsuccessful outcome is highly likely even after 12 weeks if tapering off treatment.\n\nCTN 27(Protocol, Saxon 2013): Bup/Nx vs MET (methadone) on liver function. Randomized, open-label, multi-center, phase 4 study.\nCTN 51 (Lee 2018): Bup/Nx vs XR-NTX naltrexone, effectiveness of pharmacotheerapeutic aids to recovery who completed detoxification and achieved short term abstinence. n=570\n\nXR-NTX (n=283): induction hurdle, higher rate of 24-week relapse, fewer successfully initiated XR-NTX (204 of 283) than Bup/Nx (270 of 287)\nfor those inducted successfully, relapse rate is similar\nrelapse: 4 consecutive weeks of any non-study opioid use (UDS or report), or 7 consecutive days of self-reported use.\n\n\nIf want to know which subject belongs to which study, use CreateProtocolHistory() or everybody data to have project ID.\nOpioids: Oxymorphone, Opium, Fentanyl, Hydromorphone, Codeine, Suboxone, Tramadol, Morphine, Buprenorphine, Hydrocodone, Opioid, Methadone, Oxycodone, Heroin\n\n\nTimes\n\nday 0: consent signed\nday of randomization (dr)\nday of receiving first dose of study drug (dd)\ninduction delay: dr - dd (in terms of days, not weeks)\nany day before dd is pre-treatment, even if participants are assigned to a treatment arm\n\npre-tx (baseline) / treatment period is defined by the first non-zero dose for the patient.\n\n\n\n\nEndpoints (CTNote)\nAs of CTNote, there are three types of outcomes\n\nabstinence\nuse reduction\nrelapse\n\n\n\nRisk factors\ndatasets info\n\n\nData details\n\nctn0094data\nctn0094extra\n\n\nlibrary(public.ctn0094data)\nlibrary(public.ctn0094extra)\n\n# visit_imp &lt;- public.ctn0094extra::derived_visitImputed\n# eth &lt;- public.ctn0094extra::derived_raceEthnicity\n\n#visit_imp\n#eth\n\n\ninduct_delay &lt;- public.ctn0094extra::derived_inductDelay\ninduct_delay\n\n# A tibble: 2,492 × 3\n     who treatment            inductDelay\n   &lt;int&gt; &lt;fct&gt;                      &lt;dbl&gt;\n 1     2 Outpatient BUP + EMM           0\n 2     3 Inpatient BUP                 NA\n 3     4 Inpatient NR-NTX               0\n 4     6 Outpatient BUP + SMM           0\n 5     7 Inpatient NR-NTX               1\n 6     9 Inpatient NR-NTX              NA\n 7    10 Outpatient BUP                 0\n 8    11 Methadone                      0\n 9    12 Outpatient BUP                 0\n10    13 Outpatient BUP                 0\n# ℹ 2,482 more rows\n\n\nWeekly pattern data. Week 1 starts the day after randomisation or signed consent, depending whether patients are randomized or not. Negative time stamps suggest prior randomization (or consent).\n\npattern_o &lt;- public.ctn0094extra::derived_weeklyOpioidPattern\npattern_o\n\n# A tibble: 3,560 × 8\n     who startWeek randWeek1 randWeek2 endWeek Baseline Phase_1          Phase_2\n   &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;  \n 1     1        -4        NA        NA      15 _____    ooooooooooooooo  \"\"     \n 2     2        -5         0        NA      14 ____++   ----oo-o-o-o+o   \"\"     \n 3     3        -6         0        NA      23 _______  o-ooo-ooooooooo… \"\"     \n 4     4        -4         0        NA      24 ____-    ---------------… \"\"     \n 5     5        -4        NA        NA      15 _____    ooooooooooooooo  \"\"     \n 6     6        -6         0        NA      14 ____+_+  -ooooooooooooo   \"\"     \n 7     7        -5         0        NA      24 _____+   ----ooooooooooo… \"\"     \n 8     8        -4        NA        NA      25 _____    ooooooooooooooo… \"\"     \n 9     9        -6         0        NA      22 ____+__  ooooooooooooooo… \"\"     \n10    10        -7         0        NA      22 _____+_- --o--*++o-+++++… \"\"     \n# ℹ 3,550 more rows\n\n\n\npattern_t &lt;- public.ctn0094extra::derived_weeklyTLFBPattern\npattern_t\n\n# A tibble: 3,560 × 8\n     who startWeek randWeek1 randWeek2 endWeek Baseline Phase_1          Phase_2\n   &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;  \n 1     1        -4        NA        NA      14 _____    oooooooooooooo   \"\"     \n 2     2        -5         0        NA      14 ++++++   -----------*+-   \"\"     \n 3     3        -6         0        NA      23 ++++*--  ----*----------… \"\"     \n 4     4        -4         0        NA      24 +++--    -------------*-… \"\"     \n 5     5        -4        NA        NA      14 _____    oooooooooooooo   \"\"     \n 6     6        -6         0        NA      14 +++++++  --------------   \"\"     \n 7     7        -5         0        NA      24 _+++--   ---------------… \"\"     \n 8     8        -4        NA        NA      24 _____    ooooooooooooooo… \"\"     \n 9     9        -6         0        NA      22 +++**--  ---------------… \"\"     \n10    10        -7         0        NA      22 --*+++++ ---------++-+-*… \"\"     \n# ℹ 3,550 more rows\n\n\nOutcomes (relapse)\n\nout &lt;- CTNote::outcomesCTN0094\nout$ctn0094_relapse_event |&gt; table() # relapse\n\n\n   0    1 \n 619 2941"
  },
  {
    "objectID": "inference/case_study_ctn_internal.html",
    "href": "inference/case_study_ctn_internal.html",
    "title": "Case study CTN (internal notes)",
    "section": "",
    "text": "What do I wish to get out of this analysis\n\na new dataset that is complex enough to test out methodology, potentially causal inference, since this is a RCT dataset\nlearn more about the novel ways to present data\ntry out AI for generating ideas\nwrite about it in at least 2 blogposts\n\nMy practical aim is to do ONE analysis only, and present the result in an innovative way.\n\n\nVisualization would be my priority, and presenting non-standard data is a key. This suggests that adding interactivity would be relevant here.\nLimited relevant knowledge means that my analysis wouldn’t standout."
  },
  {
    "objectID": "inference/case_study_ctn_internal.html#analysis-plan",
    "href": "inference/case_study_ctn_internal.html#analysis-plan",
    "title": "Case study CTN (internal notes)",
    "section": "",
    "text": "check"
  },
  {
    "objectID": "inference/case_study_ctn_internal.html#motivation",
    "href": "inference/case_study_ctn_internal.html#motivation",
    "title": "Case study CTN (internal notes)",
    "section": "",
    "text": "What do I wish to get out of this analysis\n\na new dataset that is complex enough to test out methodology, potentially causal inference, since this is a RCT dataset\nlearn more about the novel ways to present data\ntry out AI for generating ideas\nwrite about it in at least 2 blogposts\n\nMy practical aim is to do ONE analysis only, and present the result in an innovative way.\n\n\nVisualization would be my priority, and presenting non-standard data is a key. This suggests that adding interactivity would be relevant here.\nLimited relevant knowledge means that my analysis wouldn’t standout."
  },
  {
    "objectID": "inference/case_study_ctn_internal.html#plan",
    "href": "inference/case_study_ctn_internal.html#plan",
    "title": "Case study CTN (internal notes)",
    "section": "Plan",
    "text": "Plan\nThe deadline for completion is 2024.5.1, however I can only spare 7 full days to work on it.\n\nResearch stage: narrow down the scope\nOutcome: abstinence, use reduction, relapse\nRisk factors\n\n\nAnalysis stage: do one analysis\nPossibly focus on CTN51 study, examine endpoints including relapse (since it is also documented in a paper).\n\n\nVisualization: interactivity\nPossibly not standard visualization (KM plot)"
  },
  {
    "objectID": "inference/case_study_ctn_internal.html#log",
    "href": "inference/case_study_ctn_internal.html#log",
    "title": "Case study CTN (internal notes)",
    "section": "Log",
    "text": "Log\nStage I\n\n3.25 Initialize project, understand symbolic drug use data\n3.26 Read through vignette for the data derivation. Read data description for 3 data sources, try to understand the data generating process.\n3.27 Ask chatgpt for some background information on opioid use disorder. Merge datasets into wide format, then ask chatgpt to give some analysis suggestion.\n\nStage II\n\n3.28 Narrowed down scope, decide to create a visual tool for summarizing ICE in RCT. Read papers, identify important types of ICE and find relevant information.\n\nDecide to use lollipop plot for individual patient vis, possibly enhanced by augmented text, add interactivity to the plot using ggiraph\nadd some kind of grouping to multiple patients"
  },
  {
    "objectID": "inference/case_study_ctn_internal.html#background-information",
    "href": "inference/case_study_ctn_internal.html#background-information",
    "title": "Case study CTN (internal notes)",
    "section": "Background information",
    "text": "Background information\nOpioid epidemic in the US: driven by overprescription of opioid pain medications, proliferation of illicit opioids such as heroin, and emergence of highly potent synthetic opioids (e.g. fentanyl)\nTreatment for OUD involves a combination of pharmacotherapy (e.g. methadone, buprenorphine), behavioral therapies, support services. Medication-assisted treatment (MAT) is effective.\nChallenges: limited availability of treatment providers and resources, socialeconomic factors\nRisk factors for relapse\n\nbiological\n\npredisposition, family history of substance abuse\nco-occuring mental health disorders: depression, anxiety, post-traumatic stress disorder PTSD\nphysical health issues: chronic pain or other\n\npsycological\n\ncraving and withdrawl symptoms during detoxification\npoor coping skills, inability to deal with stress, negative emotions\nlow self-esteem\n\nsocial\n\npeer pressure, acquaintances who use opioid\nfamily dynamics, dysfunctional family relationships, lack of support\nsocial isolation\n\nenvironmental\n\navailability of opioids (prescription or illicit)\nstressful environments, poverty\n\ntreatment related\n\ninadequate treatment, lack of comprehensive care\npremature discontinuation\nlack of engagement\n\n\nRisk factors for drop-out\n\nseverity of addiction\npsychological\nsocial supoort (lack of)\nstigma and shame\ntreatment accessibility and affordability"
  },
  {
    "objectID": "inference/case_study_ctn_internal.html#analysis-proposal",
    "href": "inference/case_study_ctn_internal.html#analysis-proposal",
    "title": "Case study CTN (internal notes)",
    "section": "Analysis proposal",
    "text": "Analysis proposal\nChatgpt has suggested a few ideas for analysis:\n\nregression\nsurvival analysis\npropensity score matching\nmachine learning (rf, svm, gb), combined with feature importance analysis\nclustering (k-means, hierarchical)"
  },
  {
    "objectID": "inference/estimands_ice.html",
    "href": "inference/estimands_ice.html",
    "title": "Estimands, intercurrent events",
    "section": "",
    "text": "Key question: how to present the data visually?\nWhat are considered as important, what are not?\nITT (intention to treat): include the data after rescue medicine. Includes dropouts\nPP (Per-protocol): exclude patients taking rescue medicine, only analyse the complete cases\nTrial estimands: trial treatment effect depends on how events occur after treatment initiation.\nTreatment effect for a given outcome.\nFive core attributes\n\npopulation\ntreatment conditions\nendpoint\nsummary measure\nstrategies to handle each type of intercurrent event\n\n\nIntercurrent event\nPost-baseline events (post randomisation in RCT) that affects the interpretation of outcome.\nTwo categories:\n\ntreatment-modifying events, affects the receipt of assigned treatment. E.g. early discontinuation, use of rescue, wrong dose, wrong type (placebo for example).\ntruncating events, e.g. death, amputation of limb when the limb is relevant for the research question.\n\nStrategies (multiple can be used for different ICE in the same study)\n\ntreatment policy strategy: treat as it is, ignore ICE\ncomposite strategy: modifies the endpoint value, defined by the investigator\nwhile-on-treatment (while alive): before intercurrent event data is used.\nhypothetical strategy\nprincipal stratum strategy: redefine the population"
  },
  {
    "objectID": "inference/case_study_ctn51.html",
    "href": "inference/case_study_ctn51.html",
    "title": "Case study: CTN-51",
    "section": "",
    "text": "CTN 51 (Lee 2018): Bup/Nx vs XR-NTX naltrexone, effectiveness of pharmacotheerapeutic aids to recovery who completed detoxification and achieved short term abstinence. n=570\n\nXR-NTX (n=283): induction hurdle, higher rate of 24-week relapse, fewer successfully initiated XR-NTX (204 of 283) than Bup/Nx (270 of 287)\nfor those inducted successfully, relapse rate is similar\nrelapse: 4 consecutive weeks of any non-study opioid use (UDS or report), or 7 consecutive days of self-reported use.\n\nReproduce some of the results, try to understand the events involved after initiation\nKey numbers\n\nrelapse is 4 consecutive weeks of non-study opioid use or 7 days of self-reported use\nITT (n=570), 24 weeks relapse greater for NTX\npre-protocol (n=474), similar relapse\nrelapse rate is between 57% to 65%\n\nThings to understand\n\ninitiation hurdle"
  },
  {
    "objectID": "inference/case_study_ctn51.html#study-information",
    "href": "inference/case_study_ctn51.html#study-information",
    "title": "Case study: CTN-51",
    "section": "",
    "text": "CTN 51 (Lee 2018): Bup/Nx vs XR-NTX naltrexone, effectiveness of pharmacotheerapeutic aids to recovery who completed detoxification and achieved short term abstinence. n=570\n\nXR-NTX (n=283): induction hurdle, higher rate of 24-week relapse, fewer successfully initiated XR-NTX (204 of 283) than Bup/Nx (270 of 287)\nfor those inducted successfully, relapse rate is similar\nrelapse: 4 consecutive weeks of any non-study opioid use (UDS or report), or 7 consecutive days of self-reported use.\n\nReproduce some of the results, try to understand the events involved after initiation\nKey numbers\n\nrelapse is 4 consecutive weeks of non-study opioid use or 7 days of self-reported use\nITT (n=570), 24 weeks relapse greater for NTX\npre-protocol (n=474), similar relapse\nrelapse rate is between 57% to 65%\n\nThings to understand\n\ninitiation hurdle"
  },
  {
    "objectID": "inference/case_study_ctn51.html#datasets-used",
    "href": "inference/case_study_ctn51.html#datasets-used",
    "title": "Case study: CTN-51",
    "section": "Datasets used",
    "text": "Datasets used\n\nlibrary(public.ctn0094data)\nlibrary(public.ctn0094extra)\nlibrary(CTNote)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(stringr)\n\nout &lt;- CTNote::outcomesCTN0094\n\n# take c51 \n\neverybody$project |&gt; table() # this includes all eligible\n\n\n  27   30   51 \n1920  868  772 \n\nout_c51 &lt;- left_join(everybody, out) |&gt; \n  filter(project == 51)\n\nJoining with `by = join_by(who)`\n\nout_c51 |&gt; head()\n\n# A tibble: 6 × 64\n    who project usePatternUDS             fiellin2006_abs kosten1993_isAbs\n  &lt;dbl&gt; &lt;fct&gt;   &lt;chr&gt;                               &lt;dbl&gt; &lt;lgl&gt;           \n1     3 51      o-ooo-ooooooooooooooooo                 2 FALSE           \n2     4 51      -------------------o-o-o               21 TRUE            \n3     7 51      ----oooooooooooooooooooo                4 TRUE            \n4     9 51      oooooooooooooooooooooo                  0 FALSE           \n5    18 51      ooooooooooooooooooooooooo               0 FALSE           \n6    20 51      ooooooooooooooooooooooooo               0 FALSE           \n# ℹ 59 more variables: krupitsky2011A_isAbs &lt;lgl&gt;, krupitsky2011B_abs &lt;dbl&gt;,\n#   ling1998_isAbs &lt;lgl&gt;, lofwall2018_isAbs &lt;lgl&gt;, mokri2016_abs_time &lt;dbl&gt;,\n#   mokri2016_abs_event &lt;dbl&gt;, schottenfeld2005_abs &lt;dbl&gt;,\n#   schottenfeld2008A_abs_time &lt;dbl&gt;, schottenfeld2008A_abs_event &lt;dbl&gt;,\n#   schottenfeld2008B_abs &lt;dbl&gt;, shufman1994_absN_time &lt;dbl&gt;,\n#   shufman1994_absN_event &lt;dbl&gt;, weissLingCTN0030_isAbs &lt;lgl&gt;,\n#   comer2006_red &lt;dbl&gt;, eissenberg1997_isAbs &lt;lgl&gt;, fiellin2006_red &lt;dbl&gt;, …\n\n\n\nTreatment and outcome related\nOutcome includes pattern and class\nRandomization\ntreatment: time is study day, amount is dose or 1 (injection)"
  },
  {
    "objectID": "programming/git_setup.html",
    "href": "programming/git_setup.html",
    "title": "Set up version control",
    "section": "",
    "text": "Recently I have switched to a new MacBook M2 machine. The most important task is to set up version control, and synchronize important GitHub repositories.\n\nInstall Git\nCheck if you have git\nwhich git\ngit --version\ngit config\nI had to install git. I decided to do it with Homebrew.\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nAfter a few minutes, configure homebrew.\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; /Users/chizhang/.zprofile\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nbrew help\nIf the last line runs, it suggests that homebrew is correctly installed. Now try to install git with homebrew.\nbrew install git\n\n\nConfigure git\nAfter download, try to run the configuration again.\ngit config\ngit --version\nSince this is a new computer, need to configure the user name and email again. I made the names exactly the same as my old computer. To get the configuration from the other computer, I used git config --list.\ngit config --global user.name 'MYNAME'\ngit config --global user.name 'MYEMAIL'\ngit config --global credential.helper osxkeychain\n\n\nNew SSH key\nI followed the following steps: Checking for existing SSH keys\nFirst check whether I have existing ssh keys.\nls -al ~/.ssh\nNone returned, then I need to create a new one.\nGenerating a new SSH key\nssh-keygen -t ed25519 -C \"MYEMAIL\"\nNeed to enter passphrase.\nAdd SSH key to the ssh-agent\neval \"$(ssh-agent -s)\"\nopen ~/.ssh/config\nIf it does not exist, create a new one.\ntouch ~/.ssh/config\nnano ~/.ssh/config\nWrite the following in the config file. Pay attention to typos!\nHost github.com\n  AddKeysToAgent yes\n  UseKeychain yes\n  IdentityFile ~/.ssh/id_ed25519\nAdd the private key to the ssh-agent.\nssh-add --apple-use-keychain ~/.ssh/id_ed25519\n\n\nAdd SSH key to GitHub Account\nCopy the public SSH key to my clipboard.\npbcopy &lt; ~/.ssh/id_ed25519.pub\nGo on GitHub, go to Settings -&gt; SSH and GPG keys -&gt; New SSH Key\nPaste the public key."
  },
  {
    "objectID": "inference/rwd_statistics_overview.html",
    "href": "inference/rwd_statistics_overview.html",
    "title": "RWD statistical considerations",
    "section": "",
    "text": "In addition to data quality, missing, …\nThe key is the bias\nSelection bias (p67)\nInformation bias (p68)\nExamples of confounders\nAspects to consider: PROTECT framework (Fang 2019)"
  },
  {
    "objectID": "inference/rwd_statistics_overview.html#motivation-why-rwd-requires-special-treatment",
    "href": "inference/rwd_statistics_overview.html#motivation-why-rwd-requires-special-treatment",
    "title": "RWD statistical considerations",
    "section": "",
    "text": "In addition to data quality, missing, …\nThe key is the bias\nSelection bias (p67)\nInformation bias (p68)\nExamples of confounders\nAspects to consider: PROTECT framework (Fang 2019)"
  },
  {
    "objectID": "inference/rwd_statistics_overview.html#methods",
    "href": "inference/rwd_statistics_overview.html#methods",
    "title": "RWD statistical considerations",
    "section": "Methods",
    "text": "Methods\n\nStratification methods\nmatching, propensity score (p71)\n\n\nSensitivity analysis\np72\n\n\nMissing data\nOthers: G-methods (generalized)"
  },
  {
    "objectID": "programming/test_slides.html",
    "href": "programming/test_slides.html",
    "title": "Quarto Presentations",
    "section": "",
    "text": "This presentation will show you examples of what you can do with Quarto and Reveal.js, including:\n\nPresenting code and LaTeX equations\nIncluding computations in slide output\nImage, video, and iframe backgrounds\nFancy transitions and animations\nPrinting to PDF"
  }
]